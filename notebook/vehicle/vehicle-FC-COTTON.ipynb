{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "NN on vehicle\n",
    "\n",
    "Fully connected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plot param\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (28.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, manipulate the input and output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, always assumme that the state represent by single dimension vector\n",
    "in other word it only has one feature.\n",
    "\n",
    "Meanwhile the action size should be check if there is non-determinism in which action the controller should choose (it will influence how the score/loss function will be calculated). For instance, when the state has two ND actions (0 and 1), the loss function of that sample should be zero if the predicted action is action[0] OR action[1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this vehicle case the input size is one (there is no ND on the actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniqueness of the actions would determine the size of the output layer neuron's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the controller file\n",
    "f = open('../COTONN/vehicle_small_bdd/controller.scs', \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in f:\n",
    "    if '#MATRIX:DATA\\n' in line:                \n",
    "        for line in f: # now you are at the lines you want\n",
    "            # skip the #BEGIN \n",
    "            # read the state-actions\n",
    "            lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'90667 5\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrain_dataset = []\n",
    "ltrain_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in lines:\n",
    "    ltrain_dataset.append(x.split(' ')[0].strip('\\n'))\n",
    "    ltrain_label.append(x.split(' ')[1].strip('\\n'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.asarray(ltrain_dataset)\n",
    "train_label = np.asarray(ltrain_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.astype(np.float32)\n",
    "train_label = train_label.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_label = len(set(train_label))\n",
    "num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcoll_label = sorted(list(set(train_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 20.0,\n",
       " 21.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 34.0,\n",
       " 35.0,\n",
       " 41.0,\n",
       " 42.0,\n",
       " 43.0,\n",
       " 47.0,\n",
       " 48.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcoll_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcoll_label.index(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  7., ..., 43., 43.,  5.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_dataset[:200], train_label[:200], 'r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('title')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_transformed = np.zeros_like(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_label.shape[0]):\n",
    "    train_label_transformed[i] = lcoll_label.index(train_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  4., ..., 15., 15.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   97.,    98.,   147., ..., 90665., 90666., 90667.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lcoll_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_hot = (np.arange(len(lcoll_label)) == train_label_transformed[:, None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158, 18)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_reform_dim = train_dataset[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_reform_dim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48158"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the upper limit of experiment\n",
    "upper_limit = train_dataset_reform_dim.shape[0] #40\n",
    "# upper_limit = 2*11*11*199\n",
    "upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   97.],\n",
       "       [   98.],\n",
       "       [  147.],\n",
       "       ...,\n",
       "       [90665.],\n",
       "       [90666.],\n",
       "       [90667.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slice the samples\n",
    "train_sliced = train_dataset_reform_dim[:upper_limit]\n",
    "train_sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   97],\n",
       "       [   98],\n",
       "       [  147],\n",
       "       ...,\n",
       "       [90665],\n",
       "       [90666],\n",
       "       [90667]], dtype=uint32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change it to init\n",
    "train_sliced_int = train_sliced.astype(np.uint32)\n",
    "train_sliced_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,  97],\n",
       "       [  0,   0,   0,  98],\n",
       "       [  0,   0,   0, 147],\n",
       "       ...,\n",
       "       [  0,   1,  98,  41],\n",
       "       [  0,   1,  98,  42],\n",
       "       [  0,   1,  98,  43]], dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsi8_unordered = train_sliced_int.view(np.uint8)\n",
    "tsi8 = np.flip(tsi8_unordered,1)\n",
    "tsi8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 97,   0,   0,   0],\n",
       "       [ 98,   0,   0,   0],\n",
       "       [147,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 41,  98,   1,   0],\n",
       "       [ 42,  98,   1,   0],\n",
       "       [ 43,  98,   1,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsi8_unordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converte to bit representation\n",
    "# toy_vehicle_input = np.unpackbits(tu_u8, axis=1)\n",
    "# toy_vehicle_input\n",
    "toy_vehicle_input_unreduced = np.unpackbits(tsi8).reshape(-1,32)\n",
    "toy_vehicle_input_unreduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158, 18)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_vehicle_input = toy_vehicle_input_unreduced[:, np.arange(14,32)]\n",
    "toy_vehicle_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48158"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = toy_vehicle_input.shape[0]\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_reform = toy_vehicle_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158, 18)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_reform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158, 18)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158, 18)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_reform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_dim = train_label[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_dim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48158"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 7.],\n",
       "       ...,\n",
       "       [43.],\n",
       "       [43.],\n",
       "       [ 5.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slice the samples\n",
    "train_label_sliced = train_label_dim[:upper_limit]\n",
    "train_label_sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 0],\n",
       "       [ 7],\n",
       "       ...,\n",
       "       [43],\n",
       "       [43],\n",
       "       [ 5]], dtype=uint32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change it to int\n",
    "train_label_sliced_int = train_label_sliced.astype(np.uint32)\n",
    "train_label_sliced_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 7,  0,  0,  0],\n",
       "       ...,\n",
       "       [43,  0,  0,  0],\n",
       "       [43,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0]], dtype=uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsi8_label_unordered = train_label_sliced_int.view(np.uint8)\n",
    "tsi8_label_unordered\n",
    "# tsi8_label = np.flip(tsi8_label_unordered,1)\n",
    "# tsi8_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  7],\n",
       "       ...,\n",
       "       [ 0,  0,  0, 43],\n",
       "       [ 0,  0,  0, 43],\n",
       "       [ 0,  0,  0,  5]], dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsi8_label = np.flip(tsi8_label_unordered,1)\n",
    "tsi8_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converte to bit representation\n",
    "# toy_vehicle_input = np.unpackbits(tu_u8, axis=1)\n",
    "# toy_vehicle_input\n",
    "toy_vehicle_label_unreduced = np.unpackbits(tsi8_label).reshape(-1,32)\n",
    "toy_vehicle_label_unreduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158, 6)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_vehicle_label = toy_vehicle_label_unreduced[:, np.arange(26,32)]\n",
    "toy_vehicle_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158, 6)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_vehicle_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48158, 18)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_hot[:upper_limit].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_label = toy_vehicle_label.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_label = train_label_hot[:upper_limit].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 layer\n",
    "# different number of output layer\n",
    "batch_size = upper_limit\n",
    "input_size = train_dataset_reform.shape[1]\n",
    "hidden_size = 128\n",
    "hidden_size2 = 128\n",
    "hidden_size3 = 128\n",
    "hidden_size4 = 128\n",
    "\n",
    "\"\"\"\n",
    "hidden_size5 = 128\n",
    "hidden_size6 = 128\n",
    "hidden_size7 = 128\n",
    "hidden_size8 = 128\n",
    "hidden_size9 = 128\n",
    "\"\"\"\n",
    "\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "# Input data. For the training data, we use a placeholder that will be fed\n",
    "# at run time with a training minibatch.\n",
    "tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, input_size))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_label))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Variables.\n",
    "weights1 = tf.Variable(tf.truncated_normal([input_size, hidden_size]))\n",
    "biases1 = tf.Variable(tf.truncated_normal([hidden_size]))\n",
    "\n",
    "weights2 = tf.Variable(tf.truncated_normal([hidden_size, hidden_size2]))\n",
    "# biases2 = tf.Variable(tf.zeros([num_label]))\n",
    "biases2 = tf.Variable(tf.truncated_normal([hidden_size2]))\n",
    "\n",
    "weights3 = tf.Variable(tf.truncated_normal([hidden_size2, hidden_size3]))\n",
    "biases3 = tf.Variable(tf.truncated_normal([hidden_size3]))\n",
    "# weights3 = tf.Variable(tf.truncated_normal([hidden_size2, num_label]))\n",
    "# biases3 = tf.Variable(tf.truncated_normal([num_label]))\n",
    "\n",
    "weights4 = tf.Variable(tf.truncated_normal([hidden_size3, hidden_size4]))\n",
    "biases4 = tf.Variable(tf.truncated_normal([hidden_size4]))\n",
    "\n",
    "weights5 = tf.Variable(tf.truncated_normal([hidden_size4, num_label]))\n",
    "biases5 = tf.Variable(tf.truncated_normal([num_label]))\n",
    "\n",
    "\"\"\"\n",
    "weights6 = tf.Variable(tf.truncated_normal([hidden_size5, hidden_size6]))\n",
    "biases6 = tf.Variable(tf.truncated_normal([hidden_size6]))\n",
    "\n",
    "weights7 = tf.Variable(tf.truncated_normal([hidden_size6, hidden_size7]))\n",
    "biases7 = tf.Variable(tf.truncated_normal([hidden_size7]))\n",
    "\n",
    "weights8 = tf.Variable(tf.truncated_normal([hidden_size7, hidden_size8]))\n",
    "biases8 = tf.Variable(tf.truncated_normal([hidden_size8]))\n",
    "\n",
    "weights9 = tf.Variable(tf.truncated_normal([hidden_size8, hidden_size9]))\n",
    "biases9 = tf.Variable(tf.truncated_normal([hidden_size9]))\n",
    "\n",
    "weights10 = tf.Variable(tf.truncated_normal([hidden_size9, num_label]))\n",
    "biases10 = tf.Variable(tf.truncated_normal([num_label]))\n",
    "\"\"\"\n",
    "\n",
    "# Training computation.\n",
    "logits1 = tf.matmul(tf_train_dataset, weights1) + biases1  \n",
    "\n",
    "relu_act_func_d = tf.nn.sigmoid(logits1)\n",
    "relu_act_func = tf.nn.dropout(relu_act_func_d, keep_prob)\n",
    "logits2 = tf.matmul(relu_act_func_d, weights2) + biases2\n",
    "\n",
    "relu_act_func2_d = tf.nn.sigmoid(logits2)\n",
    "relu_act_func2 = tf.nn.dropout(relu_act_func2_d, keep_prob)\n",
    "logits3 = tf.matmul(relu_act_func2_d, weights3) + biases3\n",
    "\n",
    "relu_act_func3_d = tf.nn.sigmoid(logits3)\n",
    "relu_act_func3 = tf.nn.dropout(relu_act_func3_d, keep_prob)\n",
    "logits4 = tf.matmul(relu_act_func3_d, weights4) + biases4\n",
    "\n",
    "relu_act_func4_d = tf.nn.sigmoid(logits4)\n",
    "relu_act_func4 = tf.nn.dropout(relu_act_func4_d, keep_prob)\n",
    "logits5 = tf.matmul(relu_act_func4_d, weights5) + biases5\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "relu_act_func5_d = tf.nn.relu(logits5)\n",
    "relu_act_func5 = tf.nn.dropout(relu_act_func5_d, keep_prob)\n",
    "logits6 = tf.matmul(relu_act_func5_d, weights6) + biases6\n",
    "\n",
    "relu_act_func6_d = tf.nn.relu(logits6)\n",
    "relu_act_func6 = tf.nn.dropout(relu_act_func6_d, keep_prob)\n",
    "logits7 = tf.matmul(relu_act_func6_d, weights7) + biases7\n",
    "\n",
    "relu_act_func7_d = tf.nn.relu(logits7)\n",
    "# dropout = tf.nn.dropout(relu_act_func7, keep_prob)\n",
    "# logits8 = tf.matmul(dropout, weights8) + biases8\n",
    "relu_act_func7 = tf.nn.dropout(relu_act_func7_d, keep_prob)\n",
    "logits8 = tf.matmul(relu_act_func7_d, weights8) + biases8 \n",
    "\n",
    "relu_act_func8_d = tf.nn.relu(logits8)\n",
    "# dropout = tf.nn.dropout(relu_act_func7, keep_prob)\n",
    "# logits8 = tf.matmul(dropout, weights8) + biases8\n",
    "relu_act_func8 = tf.nn.dropout(relu_act_func8_d, keep_prob)\n",
    "logits9 = tf.matmul(relu_act_func8_d, weights9) + biases9 \n",
    "\n",
    "relu_act_func9_d = tf.nn.relu(logits9)\n",
    "relu_act_func9 = tf.nn.dropout(relu_act_func9_d, keep_prob)\n",
    "# dropout = tf.nn.dropout(relu_act_func7, keep_prob)\n",
    "# logits8 = tf.matmul(dropout, weights8) + biases8\n",
    "logits10 = tf.matmul(relu_act_func9_d, weights10) + biases10 \n",
    "\"\"\"\n",
    "\n",
    "logits = logits5\n",
    "# loss = tf.reduce_mean(tf.losses.absolute_difference(labels=tf_train_labels, predictions=train_prediction))\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits3))\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "\n",
    "# Optimizer.\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(0.005).minimize(loss)\n",
    "\n",
    "# Predictions for the training, validation, and test data.\n",
    "# train_prediction = tf.nn.softmax(logits3)\n",
    "train_prediction = tf.round(tf.nn.sigmoid(logits))\n",
    "# writer = tf.summary.FileWriter('./vehicle', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_18:0' shape=(48158, 18) dtype=float32>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_19:0' shape=(48158, 6) dtype=float32>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration_one_epoch = np.floor(num_samples / batch_size)\n",
    "iteration_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multilabel(predictions, labels):\n",
    "  return (100.0*np.mean(np.round(predictions) == np.round(batch_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48158"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = num_samples//batch_size\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 1., 0.],\n",
       "       [0., 1., 0., ..., 0., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_bin = train_dataset_reform.astype(np.float32)\n",
    "train_dataset_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 0., 1., 0., 1., 1.],\n",
       "       [1., 0., 1., 0., 1., 1.],\n",
       "       [0., 0., 0., 1., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_bin = toy_vehicle_label.astype(np.float32)\n",
    "train_label_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Epoch      1 Step:      0 Loss:   3.7705 Training Acc:    44.26    44.26    44.26%\n",
      "Epoch      2 Step:      0 Loss:   2.7656 Training Acc:    47.48    47.48    47.48%\n",
      "Epoch      3 Step:      0 Loss:   2.0765 Training Acc:    53.54    53.54    53.54%\n",
      "Epoch      4 Step:      0 Loss:   1.6497 Training Acc:    56.44    56.44    56.44%\n",
      "Epoch      5 Step:      0 Loss:   1.3288 Training Acc:    57.28    57.28    57.28%\n",
      "Epoch      6 Step:      0 Loss:   1.0355 Training Acc:    57.95    57.95    57.95%\n",
      "Epoch      7 Step:      0 Loss:   0.8134 Training Acc:    59.71    59.71    59.71%\n",
      "Epoch      8 Step:      0 Loss:   0.7797 Training Acc:    61.90    61.90    61.90%\n",
      "Epoch      9 Step:      0 Loss:   0.8261 Training Acc:    61.01    61.01    61.01%\n",
      "Epoch     10 Step:      0 Loss:   0.8606 Training Acc:    59.50    59.50    59.50%\n",
      "Epoch     11 Step:      0 Loss:   0.8673 Training Acc:    58.26    58.26    58.26%\n",
      "Epoch     12 Step:      0 Loss:   0.8452 Training Acc:    57.95    57.95    57.95%\n",
      "Epoch     13 Step:      0 Loss:   0.8039 Training Acc:    58.58    58.58    58.58%\n",
      "Epoch     14 Step:      0 Loss:   0.7580 Training Acc:    59.62    59.62    59.62%\n",
      "Epoch     15 Step:      0 Loss:   0.7196 Training Acc:    60.77    60.77    60.77%\n",
      "Epoch     16 Step:      0 Loss:   0.6946 Training Acc:    61.81    61.81    61.81%\n",
      "Epoch     17 Step:      0 Loss:   0.6830 Training Acc:    62.18    62.18    62.18%\n",
      "Epoch     18 Step:      0 Loss:   0.6799 Training Acc:    62.10    62.10    62.10%\n",
      "Epoch     19 Step:      0 Loss:   0.6789 Training Acc:    61.87    61.87    61.87%\n",
      "Epoch     20 Step:      0 Loss:   0.6764 Training Acc:    61.77    61.77    61.77%\n",
      "Epoch     21 Step:      0 Loss:   0.6716 Training Acc:    61.78    61.78    61.78%\n",
      "Epoch     22 Step:      0 Loss:   0.6659 Training Acc:    61.74    61.74    61.74%\n",
      "Epoch     23 Step:      0 Loss:   0.6608 Training Acc:    61.55    61.55    61.55%\n",
      "Epoch     24 Step:      0 Loss:   0.6570 Training Acc:    61.20    61.20    61.20%\n",
      "Epoch     25 Step:      0 Loss:   0.6545 Training Acc:    60.72    60.72    60.72%\n",
      "Epoch     26 Step:      0 Loss:   0.6528 Training Acc:    60.48    60.48    60.48%\n",
      "Epoch     27 Step:      0 Loss:   0.6512 Training Acc:    60.50    60.50    60.50%\n",
      "Epoch     28 Step:      0 Loss:   0.6489 Training Acc:    60.73    60.73    60.73%\n",
      "Epoch     29 Step:      0 Loss:   0.6459 Training Acc:    61.28    61.28    61.28%\n",
      "Epoch     30 Step:      0 Loss:   0.6424 Training Acc:    62.01    62.01    62.01%\n",
      "Epoch     31 Step:      0 Loss:   0.6392 Training Acc:    62.75    62.75    62.75%\n",
      "Epoch     32 Step:      0 Loss:   0.6369 Training Acc:    63.42    63.42    63.42%\n",
      "Epoch     33 Step:      0 Loss:   0.6358 Training Acc:    63.82    63.82    63.82%\n",
      "Epoch     34 Step:      0 Loss:   0.6354 Training Acc:    64.08    64.08    64.08%\n",
      "Epoch     35 Step:      0 Loss:   0.6351 Training Acc:    64.24    64.24    64.24%\n",
      "Epoch     36 Step:      0 Loss:   0.6346 Training Acc:    64.35    64.35    64.35%\n",
      "Epoch     37 Step:      0 Loss:   0.6338 Training Acc:    64.41    64.41    64.41%\n",
      "Epoch     38 Step:      0 Loss:   0.6328 Training Acc:    64.47    64.47    64.47%\n",
      "Epoch     39 Step:      0 Loss:   0.6319 Training Acc:    64.55    64.55    64.55%\n",
      "Epoch     40 Step:      0 Loss:   0.6311 Training Acc:    64.62    64.62    64.62%\n",
      "Epoch     41 Step:      0 Loss:   0.6303 Training Acc:    64.69    64.69    64.69%\n",
      "Epoch     42 Step:      0 Loss:   0.6297 Training Acc:    64.78    64.78    64.78%\n",
      "Epoch     43 Step:      0 Loss:   0.6290 Training Acc:    64.73    64.73    64.73%\n",
      "Epoch     44 Step:      0 Loss:   0.6286 Training Acc:    64.69    64.69    64.69%\n",
      "Epoch     45 Step:      0 Loss:   0.6282 Training Acc:    64.61    64.61    64.61%\n",
      "Epoch     46 Step:      0 Loss:   0.6279 Training Acc:    64.61    64.61    64.61%\n",
      "Epoch     47 Step:      0 Loss:   0.6275 Training Acc:    64.65    64.65    64.65%\n",
      "Epoch     48 Step:      0 Loss:   0.6272 Training Acc:    64.77    64.77    64.77%\n",
      "Epoch     49 Step:      0 Loss:   0.6268 Training Acc:    64.90    64.90    64.90%\n",
      "Epoch     50 Step:      0 Loss:   0.6265 Training Acc:    64.97    64.97    64.97%\n",
      "Epoch     51 Step:      0 Loss:   0.6263 Training Acc:    65.02    65.02    65.02%\n",
      "Epoch     52 Step:      0 Loss:   0.6261 Training Acc:    65.05    65.05    65.05%\n",
      "Epoch     53 Step:      0 Loss:   0.6259 Training Acc:    65.10    65.10    65.10%\n",
      "Epoch     54 Step:      0 Loss:   0.6257 Training Acc:    65.16    65.16    65.16%\n",
      "Epoch     55 Step:      0 Loss:   0.6255 Training Acc:    65.26    65.26    65.26%\n",
      "Epoch     56 Step:      0 Loss:   0.6252 Training Acc:    65.37    65.37    65.37%\n",
      "Epoch     57 Step:      0 Loss:   0.6250 Training Acc:    65.45    65.45    65.45%\n",
      "Epoch     58 Step:      0 Loss:   0.6247 Training Acc:    65.55    65.55    65.55%\n",
      "Epoch     59 Step:      0 Loss:   0.6244 Training Acc:    65.60    65.60    65.60%\n",
      "Epoch     60 Step:      0 Loss:   0.6240 Training Acc:    65.66    65.66    65.66%\n",
      "Epoch     61 Step:      0 Loss:   0.6237 Training Acc:    65.70    65.70    65.70%\n",
      "Epoch     62 Step:      0 Loss:   0.6235 Training Acc:    65.71    65.71    65.71%\n",
      "Epoch     63 Step:      0 Loss:   0.6233 Training Acc:    65.72    65.72    65.72%\n",
      "Epoch     64 Step:      0 Loss:   0.6232 Training Acc:    65.73    65.73    65.73%\n",
      "Epoch     65 Step:      0 Loss:   0.6229 Training Acc:    65.77    65.77    65.77%\n",
      "Epoch     66 Step:      0 Loss:   0.6227 Training Acc:    65.79    65.79    65.79%\n",
      "Epoch     67 Step:      0 Loss:   0.6224 Training Acc:    65.81    65.81    65.81%\n",
      "Epoch     68 Step:      0 Loss:   0.6222 Training Acc:    65.82    65.82    65.82%\n",
      "Epoch     69 Step:      0 Loss:   0.6220 Training Acc:    65.81    65.81    65.81%\n",
      "Epoch     70 Step:      0 Loss:   0.6219 Training Acc:    65.81    65.81    65.81%\n",
      "Epoch     71 Step:      0 Loss:   0.6216 Training Acc:    65.77    65.77    65.77%\n",
      "Epoch     72 Step:      0 Loss:   0.6214 Training Acc:    65.77    65.77    65.77%\n",
      "Epoch     73 Step:      0 Loss:   0.6212 Training Acc:    65.79    65.79    65.79%\n",
      "Epoch     74 Step:      0 Loss:   0.6209 Training Acc:    65.81    65.81    65.81%\n",
      "Epoch     75 Step:      0 Loss:   0.6207 Training Acc:    65.87    65.87    65.87%\n",
      "Epoch     76 Step:      0 Loss:   0.6205 Training Acc:    65.92    65.92    65.92%\n",
      "Epoch     77 Step:      0 Loss:   0.6202 Training Acc:    65.92    65.92    65.92%\n",
      "Epoch     78 Step:      0 Loss:   0.6200 Training Acc:    65.92    65.92    65.92%\n",
      "Epoch     79 Step:      0 Loss:   0.6198 Training Acc:    65.92    65.92    65.92%\n",
      "Epoch     80 Step:      0 Loss:   0.6196 Training Acc:    65.98    65.98    65.98%\n",
      "Epoch     81 Step:      0 Loss:   0.6194 Training Acc:    66.02    66.02    66.02%\n",
      "Epoch     82 Step:      0 Loss:   0.6191 Training Acc:    66.01    66.01    66.01%\n",
      "Epoch     83 Step:      0 Loss:   0.6189 Training Acc:    65.99    65.99    65.99%\n",
      "Epoch     84 Step:      0 Loss:   0.6187 Training Acc:    66.01    66.01    66.01%\n",
      "Epoch     85 Step:      0 Loss:   0.6185 Training Acc:    66.05    66.05    66.05%\n",
      "Epoch     86 Step:      0 Loss:   0.6183 Training Acc:    66.06    66.06    66.06%\n",
      "Epoch     87 Step:      0 Loss:   0.6180 Training Acc:    66.06    66.06    66.06%\n",
      "Epoch     88 Step:      0 Loss:   0.6178 Training Acc:    66.08    66.08    66.08%\n",
      "Epoch     89 Step:      0 Loss:   0.6176 Training Acc:    66.09    66.09    66.09%\n",
      "Epoch     90 Step:      0 Loss:   0.6174 Training Acc:    66.12    66.12    66.12%\n",
      "Epoch     91 Step:      0 Loss:   0.6172 Training Acc:    66.12    66.12    66.12%\n",
      "Epoch     92 Step:      0 Loss:   0.6170 Training Acc:    66.13    66.13    66.13%\n",
      "Epoch     93 Step:      0 Loss:   0.6168 Training Acc:    66.13    66.13    66.13%\n",
      "Epoch     94 Step:      0 Loss:   0.6166 Training Acc:    66.15    66.15    66.15%\n",
      "Epoch     95 Step:      0 Loss:   0.6164 Training Acc:    66.16    66.16    66.16%\n",
      "Epoch     96 Step:      0 Loss:   0.6162 Training Acc:    66.15    66.15    66.15%\n",
      "Epoch     97 Step:      0 Loss:   0.6160 Training Acc:    66.16    66.16    66.16%\n",
      "Epoch     98 Step:      0 Loss:   0.6158 Training Acc:    66.17    66.17    66.17%\n",
      "Epoch     99 Step:      0 Loss:   0.6156 Training Acc:    66.18    66.18    66.18%\n",
      "Epoch    100 Step:      0 Loss:   0.6155 Training Acc:    66.18    66.18    66.18%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    101 Step:      0 Loss:   0.6153 Training Acc:    66.19    66.19    66.19%\n",
      "Epoch    102 Step:      0 Loss:   0.6151 Training Acc:    66.21    66.21    66.21%\n",
      "Epoch    103 Step:      0 Loss:   0.6149 Training Acc:    66.23    66.23    66.23%\n",
      "Epoch    104 Step:      0 Loss:   0.6148 Training Acc:    66.22    66.22    66.22%\n",
      "Epoch    105 Step:      0 Loss:   0.6146 Training Acc:    66.22    66.22    66.22%\n",
      "Epoch    106 Step:      0 Loss:   0.6145 Training Acc:    66.23    66.23    66.23%\n",
      "Epoch    107 Step:      0 Loss:   0.6143 Training Acc:    66.24    66.24    66.24%\n",
      "Epoch    108 Step:      0 Loss:   0.6142 Training Acc:    66.23    66.23    66.23%\n",
      "Epoch    109 Step:      0 Loss:   0.6140 Training Acc:    66.23    66.23    66.23%\n",
      "Epoch    110 Step:      0 Loss:   0.6139 Training Acc:    66.23    66.23    66.23%\n",
      "Epoch    111 Step:      0 Loss:   0.6137 Training Acc:    66.24    66.24    66.24%\n",
      "Epoch    112 Step:      0 Loss:   0.6136 Training Acc:    66.24    66.24    66.24%\n",
      "Epoch    113 Step:      0 Loss:   0.6134 Training Acc:    66.25    66.25    66.25%\n",
      "Epoch    114 Step:      0 Loss:   0.6133 Training Acc:    66.24    66.24    66.24%\n",
      "Epoch    115 Step:      0 Loss:   0.6132 Training Acc:    66.25    66.25    66.25%\n",
      "Epoch    116 Step:      0 Loss:   0.6131 Training Acc:    66.26    66.26    66.26%\n",
      "Epoch    117 Step:      0 Loss:   0.6129 Training Acc:    66.26    66.26    66.26%\n",
      "Epoch    118 Step:      0 Loss:   0.6128 Training Acc:    66.27    66.27    66.27%\n",
      "Epoch    119 Step:      0 Loss:   0.6127 Training Acc:    66.27    66.27    66.27%\n",
      "Epoch    120 Step:      0 Loss:   0.6126 Training Acc:    66.27    66.27    66.27%\n",
      "Epoch    121 Step:      0 Loss:   0.6124 Training Acc:    66.27    66.27    66.27%\n",
      "Epoch    122 Step:      0 Loss:   0.6123 Training Acc:    66.28    66.28    66.28%\n",
      "Epoch    123 Step:      0 Loss:   0.6122 Training Acc:    66.29    66.29    66.29%\n",
      "Epoch    124 Step:      0 Loss:   0.6121 Training Acc:    66.29    66.29    66.29%\n",
      "Epoch    125 Step:      0 Loss:   0.6120 Training Acc:    66.30    66.30    66.30%\n",
      "Epoch    126 Step:      0 Loss:   0.6118 Training Acc:    66.32    66.32    66.32%\n",
      "Epoch    127 Step:      0 Loss:   0.6117 Training Acc:    66.32    66.32    66.32%\n",
      "Epoch    128 Step:      0 Loss:   0.6116 Training Acc:    66.33    66.33    66.33%\n",
      "Epoch    129 Step:      0 Loss:   0.6115 Training Acc:    66.34    66.34    66.34%\n",
      "Epoch    130 Step:      0 Loss:   0.6114 Training Acc:    66.35    66.35    66.35%\n",
      "Epoch    131 Step:      0 Loss:   0.6113 Training Acc:    66.37    66.37    66.37%\n",
      "Epoch    132 Step:      0 Loss:   0.6112 Training Acc:    66.39    66.39    66.39%\n",
      "Epoch    133 Step:      0 Loss:   0.6111 Training Acc:    66.39    66.39    66.39%\n",
      "Epoch    134 Step:      0 Loss:   0.6109 Training Acc:    66.39    66.39    66.39%\n",
      "Epoch    135 Step:      0 Loss:   0.6108 Training Acc:    66.39    66.39    66.39%\n",
      "Epoch    136 Step:      0 Loss:   0.6107 Training Acc:    66.40    66.40    66.40%\n",
      "Epoch    137 Step:      0 Loss:   0.6106 Training Acc:    66.39    66.39    66.39%\n",
      "Epoch    138 Step:      0 Loss:   0.6105 Training Acc:    66.40    66.40    66.40%\n",
      "Epoch    139 Step:      0 Loss:   0.6104 Training Acc:    66.41    66.41    66.41%\n",
      "Epoch    140 Step:      0 Loss:   0.6103 Training Acc:    66.42    66.42    66.42%\n",
      "Epoch    141 Step:      0 Loss:   0.6102 Training Acc:    66.43    66.43    66.43%\n",
      "Epoch    142 Step:      0 Loss:   0.6101 Training Acc:    66.43    66.43    66.43%\n",
      "Epoch    143 Step:      0 Loss:   0.6099 Training Acc:    66.43    66.43    66.43%\n",
      "Epoch    144 Step:      0 Loss:   0.6098 Training Acc:    66.43    66.43    66.43%\n",
      "Epoch    145 Step:      0 Loss:   0.6097 Training Acc:    66.44    66.44    66.44%\n",
      "Epoch    146 Step:      0 Loss:   0.6096 Training Acc:    66.44    66.44    66.44%\n",
      "Epoch    147 Step:      0 Loss:   0.6095 Training Acc:    66.45    66.45    66.45%\n",
      "Epoch    148 Step:      0 Loss:   0.6094 Training Acc:    66.46    66.46    66.46%\n",
      "Epoch    149 Step:      0 Loss:   0.6093 Training Acc:    66.46    66.46    66.46%\n",
      "Epoch    150 Step:      0 Loss:   0.6091 Training Acc:    66.48    66.48    66.48%\n",
      "Epoch    151 Step:      0 Loss:   0.6090 Training Acc:    66.49    66.49    66.49%\n",
      "Epoch    152 Step:      0 Loss:   0.6089 Training Acc:    66.50    66.50    66.50%\n",
      "Epoch    153 Step:      0 Loss:   0.6088 Training Acc:    66.51    66.51    66.51%\n",
      "Epoch    154 Step:      0 Loss:   0.6087 Training Acc:    66.52    66.52    66.52%\n",
      "Epoch    155 Step:      0 Loss:   0.6085 Training Acc:    66.53    66.53    66.53%\n",
      "Epoch    156 Step:      0 Loss:   0.6084 Training Acc:    66.54    66.54    66.54%\n",
      "Epoch    157 Step:      0 Loss:   0.6083 Training Acc:    66.54    66.54    66.54%\n",
      "Epoch    158 Step:      0 Loss:   0.6082 Training Acc:    66.55    66.55    66.55%\n",
      "Epoch    159 Step:      0 Loss:   0.6080 Training Acc:    66.56    66.56    66.56%\n",
      "Epoch    160 Step:      0 Loss:   0.6079 Training Acc:    66.58    66.58    66.58%\n",
      "Epoch    161 Step:      0 Loss:   0.6078 Training Acc:    66.57    66.57    66.57%\n",
      "Epoch    162 Step:      0 Loss:   0.6077 Training Acc:    66.57    66.57    66.57%\n",
      "Epoch    163 Step:      0 Loss:   0.6075 Training Acc:    66.59    66.59    66.59%\n",
      "Epoch    164 Step:      0 Loss:   0.6074 Training Acc:    66.60    66.60    66.60%\n",
      "Epoch    165 Step:      0 Loss:   0.6073 Training Acc:    66.62    66.62    66.62%\n",
      "Epoch    166 Step:      0 Loss:   0.6071 Training Acc:    66.62    66.62    66.62%\n",
      "Epoch    167 Step:      0 Loss:   0.6070 Training Acc:    66.64    66.64    66.64%\n",
      "Epoch    168 Step:      0 Loss:   0.6068 Training Acc:    66.65    66.65    66.65%\n",
      "Epoch    169 Step:      0 Loss:   0.6067 Training Acc:    66.65    66.65    66.65%\n",
      "Epoch    170 Step:      0 Loss:   0.6065 Training Acc:    66.68    66.68    66.68%\n",
      "Epoch    171 Step:      0 Loss:   0.6064 Training Acc:    66.70    66.70    66.70%\n",
      "Epoch    172 Step:      0 Loss:   0.6062 Training Acc:    66.70    66.70    66.70%\n",
      "Epoch    173 Step:      0 Loss:   0.6061 Training Acc:    66.72    66.72    66.72%\n",
      "Epoch    174 Step:      0 Loss:   0.6060 Training Acc:    66.72    66.72    66.72%\n",
      "Epoch    175 Step:      0 Loss:   0.6058 Training Acc:    66.73    66.73    66.73%\n",
      "Epoch    176 Step:      0 Loss:   0.6057 Training Acc:    66.75    66.75    66.75%\n",
      "Epoch    177 Step:      0 Loss:   0.6055 Training Acc:    66.75    66.75    66.75%\n",
      "Epoch    178 Step:      0 Loss:   0.6053 Training Acc:    66.78    66.78    66.78%\n",
      "Epoch    179 Step:      0 Loss:   0.6052 Training Acc:    66.78    66.78    66.78%\n",
      "Epoch    180 Step:      0 Loss:   0.6050 Training Acc:    66.82    66.82    66.82%\n",
      "Epoch    181 Step:      0 Loss:   0.6049 Training Acc:    66.82    66.82    66.82%\n",
      "Epoch    182 Step:      0 Loss:   0.6047 Training Acc:    66.84    66.84    66.84%\n",
      "Epoch    183 Step:      0 Loss:   0.6046 Training Acc:    66.85    66.85    66.85%\n",
      "Epoch    184 Step:      0 Loss:   0.6044 Training Acc:    66.86    66.86    66.86%\n",
      "Epoch    185 Step:      0 Loss:   0.6042 Training Acc:    66.87    66.87    66.87%\n",
      "Epoch    186 Step:      0 Loss:   0.6041 Training Acc:    66.88    66.88    66.88%\n",
      "Epoch    187 Step:      0 Loss:   0.6039 Training Acc:    66.91    66.91    66.91%\n",
      "Epoch    188 Step:      0 Loss:   0.6037 Training Acc:    66.92    66.92    66.92%\n",
      "Epoch    189 Step:      0 Loss:   0.6036 Training Acc:    66.93    66.93    66.93%\n",
      "Epoch    190 Step:      0 Loss:   0.6034 Training Acc:    66.95    66.95    66.95%\n",
      "Epoch    191 Step:      0 Loss:   0.6032 Training Acc:    66.96    66.96    66.96%\n",
      "Epoch    192 Step:      0 Loss:   0.6030 Training Acc:    66.99    66.99    66.99%\n",
      "Epoch    193 Step:      0 Loss:   0.6028 Training Acc:    67.00    67.00    67.00%\n",
      "Epoch    194 Step:      0 Loss:   0.6027 Training Acc:    67.03    67.03    67.03%\n",
      "Epoch    195 Step:      0 Loss:   0.6025 Training Acc:    67.05    67.05    67.05%\n",
      "Epoch    196 Step:      0 Loss:   0.6023 Training Acc:    67.05    67.05    67.05%\n",
      "Epoch    197 Step:      0 Loss:   0.6021 Training Acc:    67.07    67.07    67.07%\n",
      "Epoch    198 Step:      0 Loss:   0.6019 Training Acc:    67.08    67.08    67.08%\n",
      "Epoch    199 Step:      0 Loss:   0.6017 Training Acc:    67.11    67.11    67.11%\n",
      "Epoch    200 Step:      0 Loss:   0.6016 Training Acc:    67.14    67.14    67.14%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    201 Step:      0 Loss:   0.6014 Training Acc:    67.15    67.15    67.15%\n",
      "Epoch    202 Step:      0 Loss:   0.6012 Training Acc:    67.19    67.19    67.19%\n",
      "Epoch    203 Step:      0 Loss:   0.6010 Training Acc:    67.19    67.19    67.19%\n",
      "Epoch    204 Step:      0 Loss:   0.6008 Training Acc:    67.22    67.22    67.22%\n",
      "Epoch    205 Step:      0 Loss:   0.6006 Training Acc:    67.25    67.25    67.25%\n",
      "Epoch    206 Step:      0 Loss:   0.6004 Training Acc:    67.26    67.26    67.26%\n",
      "Epoch    207 Step:      0 Loss:   0.6002 Training Acc:    67.28    67.28    67.28%\n",
      "Epoch    208 Step:      0 Loss:   0.6000 Training Acc:    67.29    67.29    67.29%\n",
      "Epoch    209 Step:      0 Loss:   0.5998 Training Acc:    67.30    67.30    67.30%\n",
      "Epoch    210 Step:      0 Loss:   0.5996 Training Acc:    67.32    67.32    67.32%\n",
      "Epoch    211 Step:      0 Loss:   0.5994 Training Acc:    67.34    67.34    67.34%\n",
      "Epoch    212 Step:      0 Loss:   0.5992 Training Acc:    67.36    67.36    67.36%\n",
      "Epoch    213 Step:      0 Loss:   0.5990 Training Acc:    67.37    67.37    67.37%\n",
      "Epoch    214 Step:      0 Loss:   0.5988 Training Acc:    67.38    67.38    67.38%\n",
      "Epoch    215 Step:      0 Loss:   0.5986 Training Acc:    67.40    67.40    67.40%\n",
      "Epoch    216 Step:      0 Loss:   0.5984 Training Acc:    67.43    67.43    67.43%\n",
      "Epoch    217 Step:      0 Loss:   0.5982 Training Acc:    67.46    67.46    67.46%\n",
      "Epoch    218 Step:      0 Loss:   0.5980 Training Acc:    67.49    67.49    67.49%\n",
      "Epoch    219 Step:      0 Loss:   0.5978 Training Acc:    67.50    67.50    67.50%\n",
      "Epoch    220 Step:      0 Loss:   0.5976 Training Acc:    67.53    67.53    67.53%\n",
      "Epoch    221 Step:      0 Loss:   0.5974 Training Acc:    67.52    67.52    67.52%\n",
      "Epoch    222 Step:      0 Loss:   0.5972 Training Acc:    67.57    67.57    67.57%\n",
      "Epoch    223 Step:      0 Loss:   0.5970 Training Acc:    67.56    67.56    67.56%\n",
      "Epoch    224 Step:      0 Loss:   0.5967 Training Acc:    67.60    67.60    67.60%\n",
      "Epoch    225 Step:      0 Loss:   0.5965 Training Acc:    67.63    67.63    67.63%\n",
      "Epoch    226 Step:      0 Loss:   0.5963 Training Acc:    67.62    67.62    67.62%\n",
      "Epoch    227 Step:      0 Loss:   0.5961 Training Acc:    67.68    67.68    67.68%\n",
      "Epoch    228 Step:      0 Loss:   0.5959 Training Acc:    67.68    67.68    67.68%\n",
      "Epoch    229 Step:      0 Loss:   0.5957 Training Acc:    67.69    67.69    67.69%\n",
      "Epoch    230 Step:      0 Loss:   0.5955 Training Acc:    67.71    67.71    67.71%\n",
      "Epoch    231 Step:      0 Loss:   0.5953 Training Acc:    67.74    67.74    67.74%\n",
      "Epoch    232 Step:      0 Loss:   0.5950 Training Acc:    67.74    67.74    67.74%\n",
      "Epoch    233 Step:      0 Loss:   0.5948 Training Acc:    67.77    67.77    67.77%\n",
      "Epoch    234 Step:      0 Loss:   0.5946 Training Acc:    67.78    67.78    67.78%\n",
      "Epoch    235 Step:      0 Loss:   0.5944 Training Acc:    67.78    67.78    67.78%\n",
      "Epoch    236 Step:      0 Loss:   0.5942 Training Acc:    67.79    67.79    67.79%\n",
      "Epoch    237 Step:      0 Loss:   0.5940 Training Acc:    67.81    67.81    67.81%\n",
      "Epoch    238 Step:      0 Loss:   0.5937 Training Acc:    67.83    67.83    67.83%\n",
      "Epoch    239 Step:      0 Loss:   0.5935 Training Acc:    67.87    67.87    67.87%\n",
      "Epoch    240 Step:      0 Loss:   0.5933 Training Acc:    67.88    67.88    67.88%\n",
      "Epoch    241 Step:      0 Loss:   0.5931 Training Acc:    67.89    67.89    67.89%\n",
      "Epoch    242 Step:      0 Loss:   0.5928 Training Acc:    67.92    67.92    67.92%\n",
      "Epoch    243 Step:      0 Loss:   0.5926 Training Acc:    67.91    67.91    67.91%\n",
      "Epoch    244 Step:      0 Loss:   0.5924 Training Acc:    67.92    67.92    67.92%\n",
      "Epoch    245 Step:      0 Loss:   0.5922 Training Acc:    67.94    67.94    67.94%\n",
      "Epoch    246 Step:      0 Loss:   0.5920 Training Acc:    67.94    67.94    67.94%\n",
      "Epoch    247 Step:      0 Loss:   0.5917 Training Acc:    67.96    67.96    67.96%\n",
      "Epoch    248 Step:      0 Loss:   0.5915 Training Acc:    67.96    67.96    67.96%\n",
      "Epoch    249 Step:      0 Loss:   0.5913 Training Acc:    67.98    67.98    67.98%\n",
      "Epoch    250 Step:      0 Loss:   0.5911 Training Acc:    67.97    67.97    67.97%\n",
      "Epoch    251 Step:      0 Loss:   0.5909 Training Acc:    68.00    68.00    68.00%\n",
      "Epoch    252 Step:      0 Loss:   0.5907 Training Acc:    67.98    67.98    67.98%\n",
      "Epoch    253 Step:      0 Loss:   0.5904 Training Acc:    68.02    68.02    68.02%\n",
      "Epoch    254 Step:      0 Loss:   0.5902 Training Acc:    68.05    68.05    68.05%\n",
      "Epoch    255 Step:      0 Loss:   0.5900 Training Acc:    68.03    68.03    68.03%\n",
      "Epoch    256 Step:      0 Loss:   0.5898 Training Acc:    68.08    68.08    68.08%\n",
      "Epoch    257 Step:      0 Loss:   0.5896 Training Acc:    68.07    68.07    68.07%\n",
      "Epoch    258 Step:      0 Loss:   0.5894 Training Acc:    68.06    68.06    68.06%\n",
      "Epoch    259 Step:      0 Loss:   0.5892 Training Acc:    68.09    68.09    68.09%\n",
      "Epoch    260 Step:      0 Loss:   0.5889 Training Acc:    68.10    68.10    68.10%\n",
      "Epoch    261 Step:      0 Loss:   0.5887 Training Acc:    68.11    68.11    68.11%\n",
      "Epoch    262 Step:      0 Loss:   0.5885 Training Acc:    68.15    68.15    68.15%\n",
      "Epoch    263 Step:      0 Loss:   0.5883 Training Acc:    68.14    68.14    68.14%\n",
      "Epoch    264 Step:      0 Loss:   0.5881 Training Acc:    68.17    68.17    68.17%\n",
      "Epoch    265 Step:      0 Loss:   0.5879 Training Acc:    68.18    68.18    68.18%\n",
      "Epoch    266 Step:      0 Loss:   0.5877 Training Acc:    68.17    68.17    68.17%\n",
      "Epoch    267 Step:      0 Loss:   0.5875 Training Acc:    68.20    68.20    68.20%\n",
      "Epoch    268 Step:      0 Loss:   0.5873 Training Acc:    68.19    68.19    68.19%\n",
      "Epoch    269 Step:      0 Loss:   0.5871 Training Acc:    68.22    68.22    68.22%\n",
      "Epoch    270 Step:      0 Loss:   0.5868 Training Acc:    68.22    68.22    68.22%\n",
      "Epoch    271 Step:      0 Loss:   0.5866 Training Acc:    68.25    68.25    68.25%\n",
      "Epoch    272 Step:      0 Loss:   0.5864 Training Acc:    68.26    68.26    68.26%\n",
      "Epoch    273 Step:      0 Loss:   0.5862 Training Acc:    68.26    68.26    68.26%\n",
      "Epoch    274 Step:      0 Loss:   0.5860 Training Acc:    68.28    68.28    68.28%\n",
      "Epoch    275 Step:      0 Loss:   0.5858 Training Acc:    68.28    68.28    68.28%\n",
      "Epoch    276 Step:      0 Loss:   0.5856 Training Acc:    68.30    68.30    68.30%\n",
      "Epoch    277 Step:      0 Loss:   0.5854 Training Acc:    68.32    68.32    68.32%\n",
      "Epoch    278 Step:      0 Loss:   0.5852 Training Acc:    68.32    68.32    68.32%\n",
      "Epoch    279 Step:      0 Loss:   0.5850 Training Acc:    68.34    68.34    68.34%\n",
      "Epoch    280 Step:      0 Loss:   0.5848 Training Acc:    68.33    68.33    68.33%\n",
      "Epoch    281 Step:      0 Loss:   0.5846 Training Acc:    68.35    68.35    68.35%\n",
      "Epoch    282 Step:      0 Loss:   0.5844 Training Acc:    68.35    68.35    68.35%\n",
      "Epoch    283 Step:      0 Loss:   0.5842 Training Acc:    68.38    68.38    68.38%\n",
      "Epoch    284 Step:      0 Loss:   0.5840 Training Acc:    68.40    68.40    68.40%\n",
      "Epoch    285 Step:      0 Loss:   0.5838 Training Acc:    68.39    68.39    68.39%\n",
      "Epoch    286 Step:      0 Loss:   0.5836 Training Acc:    68.40    68.40    68.40%\n",
      "Epoch    287 Step:      0 Loss:   0.5834 Training Acc:    68.40    68.40    68.40%\n",
      "Epoch    288 Step:      0 Loss:   0.5832 Training Acc:    68.40    68.40    68.40%\n",
      "Epoch    289 Step:      0 Loss:   0.5831 Training Acc:    68.38    68.38    68.38%\n",
      "Epoch    290 Step:      0 Loss:   0.5829 Training Acc:    68.41    68.41    68.41%\n",
      "Epoch    291 Step:      0 Loss:   0.5827 Training Acc:    68.39    68.39    68.39%\n",
      "Epoch    292 Step:      0 Loss:   0.5824 Training Acc:    68.45    68.45    68.45%\n",
      "Epoch    293 Step:      0 Loss:   0.5822 Training Acc:    68.46    68.46    68.46%\n",
      "Epoch    294 Step:      0 Loss:   0.5821 Training Acc:    68.41    68.41    68.41%\n",
      "Epoch    295 Step:      0 Loss:   0.5819 Training Acc:    68.45    68.45    68.45%\n",
      "Epoch    296 Step:      0 Loss:   0.5817 Training Acc:    68.43    68.43    68.43%\n",
      "Epoch    297 Step:      0 Loss:   0.5815 Training Acc:    68.45    68.45    68.45%\n",
      "Epoch    298 Step:      0 Loss:   0.5813 Training Acc:    68.49    68.49    68.49%\n",
      "Epoch    299 Step:      0 Loss:   0.5812 Training Acc:    68.46    68.46    68.46%\n",
      "Epoch    300 Step:      0 Loss:   0.5810 Training Acc:    68.47    68.47    68.47%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    301 Step:      0 Loss:   0.5808 Training Acc:    68.48    68.48    68.48%\n",
      "Epoch    302 Step:      0 Loss:   0.5806 Training Acc:    68.48    68.48    68.48%\n",
      "Epoch    303 Step:      0 Loss:   0.5804 Training Acc:    68.47    68.47    68.47%\n",
      "Epoch    304 Step:      0 Loss:   0.5803 Training Acc:    68.49    68.49    68.49%\n",
      "Epoch    305 Step:      0 Loss:   0.5801 Training Acc:    68.49    68.49    68.49%\n",
      "Epoch    306 Step:      0 Loss:   0.5799 Training Acc:    68.49    68.49    68.49%\n",
      "Epoch    307 Step:      0 Loss:   0.5798 Training Acc:    68.51    68.51    68.51%\n",
      "Epoch    308 Step:      0 Loss:   0.5796 Training Acc:    68.52    68.52    68.52%\n",
      "Epoch    309 Step:      0 Loss:   0.5794 Training Acc:    68.51    68.51    68.51%\n",
      "Epoch    310 Step:      0 Loss:   0.5793 Training Acc:    68.52    68.52    68.52%\n",
      "Epoch    311 Step:      0 Loss:   0.5791 Training Acc:    68.52    68.52    68.52%\n",
      "Epoch    312 Step:      0 Loss:   0.5789 Training Acc:    68.54    68.54    68.54%\n",
      "Epoch    313 Step:      0 Loss:   0.5788 Training Acc:    68.56    68.56    68.56%\n",
      "Epoch    314 Step:      0 Loss:   0.5786 Training Acc:    68.55    68.55    68.55%\n",
      "Epoch    315 Step:      0 Loss:   0.5785 Training Acc:    68.55    68.55    68.55%\n",
      "Epoch    316 Step:      0 Loss:   0.5783 Training Acc:    68.57    68.57    68.57%\n",
      "Epoch    317 Step:      0 Loss:   0.5782 Training Acc:    68.58    68.58    68.58%\n",
      "Epoch    318 Step:      0 Loss:   0.5780 Training Acc:    68.60    68.60    68.60%\n",
      "Epoch    319 Step:      0 Loss:   0.5778 Training Acc:    68.59    68.59    68.59%\n",
      "Epoch    320 Step:      0 Loss:   0.5777 Training Acc:    68.60    68.60    68.60%\n",
      "Epoch    321 Step:      0 Loss:   0.5775 Training Acc:    68.62    68.62    68.62%\n",
      "Epoch    322 Step:      0 Loss:   0.5774 Training Acc:    68.63    68.63    68.63%\n",
      "Epoch    323 Step:      0 Loss:   0.5772 Training Acc:    68.62    68.62    68.62%\n",
      "Epoch    324 Step:      0 Loss:   0.5771 Training Acc:    68.64    68.64    68.64%\n",
      "Epoch    325 Step:      0 Loss:   0.5769 Training Acc:    68.65    68.65    68.65%\n",
      "Epoch    326 Step:      0 Loss:   0.5768 Training Acc:    68.65    68.65    68.65%\n",
      "Epoch    327 Step:      0 Loss:   0.5766 Training Acc:    68.67    68.67    68.67%\n",
      "Epoch    328 Step:      0 Loss:   0.5765 Training Acc:    68.67    68.67    68.67%\n",
      "Epoch    329 Step:      0 Loss:   0.5763 Training Acc:    68.68    68.68    68.68%\n",
      "Epoch    330 Step:      0 Loss:   0.5762 Training Acc:    68.67    68.67    68.67%\n",
      "Epoch    331 Step:      0 Loss:   0.5761 Training Acc:    68.69    68.69    68.69%\n",
      "Epoch    332 Step:      0 Loss:   0.5760 Training Acc:    68.69    68.69    68.69%\n",
      "Epoch    333 Step:      0 Loss:   0.5759 Training Acc:    68.69    68.69    68.69%\n",
      "Epoch    334 Step:      0 Loss:   0.5757 Training Acc:    68.73    68.73    68.73%\n",
      "Epoch    335 Step:      0 Loss:   0.5756 Training Acc:    68.71    68.71    68.71%\n",
      "Epoch    336 Step:      0 Loss:   0.5754 Training Acc:    68.71    68.71    68.71%\n",
      "Epoch    337 Step:      0 Loss:   0.5752 Training Acc:    68.74    68.74    68.74%\n",
      "Epoch    338 Step:      0 Loss:   0.5751 Training Acc:    68.73    68.73    68.73%\n",
      "Epoch    339 Step:      0 Loss:   0.5750 Training Acc:    68.76    68.76    68.76%\n",
      "Epoch    340 Step:      0 Loss:   0.5748 Training Acc:    68.76    68.76    68.76%\n",
      "Epoch    341 Step:      0 Loss:   0.5747 Training Acc:    68.77    68.77    68.77%\n",
      "Epoch    342 Step:      0 Loss:   0.5745 Training Acc:    68.76    68.76    68.76%\n",
      "Epoch    343 Step:      0 Loss:   0.5744 Training Acc:    68.78    68.78    68.78%\n",
      "Epoch    344 Step:      0 Loss:   0.5743 Training Acc:    68.80    68.80    68.80%\n",
      "Epoch    345 Step:      0 Loss:   0.5741 Training Acc:    68.80    68.80    68.80%\n",
      "Epoch    346 Step:      0 Loss:   0.5740 Training Acc:    68.82    68.82    68.82%\n",
      "Epoch    347 Step:      0 Loss:   0.5738 Training Acc:    68.82    68.82    68.82%\n",
      "Epoch    348 Step:      0 Loss:   0.5737 Training Acc:    68.82    68.82    68.82%\n",
      "Epoch    349 Step:      0 Loss:   0.5736 Training Acc:    68.86    68.86    68.86%\n",
      "Epoch    350 Step:      0 Loss:   0.5735 Training Acc:    68.84    68.84    68.84%\n",
      "Epoch    351 Step:      0 Loss:   0.5733 Training Acc:    68.86    68.86    68.86%\n",
      "Epoch    352 Step:      0 Loss:   0.5732 Training Acc:    68.85    68.85    68.85%\n",
      "Epoch    353 Step:      0 Loss:   0.5730 Training Acc:    68.86    68.86    68.86%\n",
      "Epoch    354 Step:      0 Loss:   0.5729 Training Acc:    68.89    68.89    68.89%\n",
      "Epoch    355 Step:      0 Loss:   0.5728 Training Acc:    68.89    68.89    68.89%\n",
      "Epoch    356 Step:      0 Loss:   0.5726 Training Acc:    68.92    68.92    68.92%\n",
      "Epoch    357 Step:      0 Loss:   0.5725 Training Acc:    68.91    68.91    68.91%\n",
      "Epoch    358 Step:      0 Loss:   0.5723 Training Acc:    68.92    68.92    68.92%\n",
      "Epoch    359 Step:      0 Loss:   0.5722 Training Acc:    68.94    68.94    68.94%\n",
      "Epoch    360 Step:      0 Loss:   0.5721 Training Acc:    68.94    68.94    68.94%\n",
      "Epoch    361 Step:      0 Loss:   0.5720 Training Acc:    68.96    68.96    68.96%\n",
      "Epoch    362 Step:      0 Loss:   0.5718 Training Acc:    68.96    68.96    68.96%\n",
      "Epoch    363 Step:      0 Loss:   0.5717 Training Acc:    68.98    68.98    68.98%\n",
      "Epoch    364 Step:      0 Loss:   0.5715 Training Acc:    68.98    68.98    68.98%\n",
      "Epoch    365 Step:      0 Loss:   0.5714 Training Acc:    68.99    68.99    68.99%\n",
      "Epoch    366 Step:      0 Loss:   0.5713 Training Acc:    69.01    69.01    69.01%\n",
      "Epoch    367 Step:      0 Loss:   0.5711 Training Acc:    69.01    69.01    69.01%\n",
      "Epoch    368 Step:      0 Loss:   0.5710 Training Acc:    69.04    69.04    69.04%\n",
      "Epoch    369 Step:      0 Loss:   0.5709 Training Acc:    69.04    69.04    69.04%\n",
      "Epoch    370 Step:      0 Loss:   0.5708 Training Acc:    69.07    69.07    69.07%\n",
      "Epoch    371 Step:      0 Loss:   0.5706 Training Acc:    69.06    69.06    69.06%\n",
      "Epoch    372 Step:      0 Loss:   0.5705 Training Acc:    69.09    69.09    69.09%\n",
      "Epoch    373 Step:      0 Loss:   0.5703 Training Acc:    69.10    69.10    69.10%\n",
      "Epoch    374 Step:      0 Loss:   0.5702 Training Acc:    69.11    69.11    69.11%\n",
      "Epoch    375 Step:      0 Loss:   0.5701 Training Acc:    69.11    69.11    69.11%\n",
      "Epoch    376 Step:      0 Loss:   0.5700 Training Acc:    69.13    69.13    69.13%\n",
      "Epoch    377 Step:      0 Loss:   0.5698 Training Acc:    69.13    69.13    69.13%\n",
      "Epoch    378 Step:      0 Loss:   0.5697 Training Acc:    69.14    69.14    69.14%\n",
      "Epoch    379 Step:      0 Loss:   0.5696 Training Acc:    69.15    69.15    69.15%\n",
      "Epoch    380 Step:      0 Loss:   0.5694 Training Acc:    69.15    69.15    69.15%\n",
      "Epoch    381 Step:      0 Loss:   0.5693 Training Acc:    69.17    69.17    69.17%\n",
      "Epoch    382 Step:      0 Loss:   0.5692 Training Acc:    69.18    69.18    69.18%\n",
      "Epoch    383 Step:      0 Loss:   0.5691 Training Acc:    69.19    69.19    69.19%\n",
      "Epoch    384 Step:      0 Loss:   0.5689 Training Acc:    69.21    69.21    69.21%\n",
      "Epoch    385 Step:      0 Loss:   0.5688 Training Acc:    69.18    69.18    69.18%\n",
      "Epoch    386 Step:      0 Loss:   0.5687 Training Acc:    69.22    69.22    69.22%\n",
      "Epoch    387 Step:      0 Loss:   0.5686 Training Acc:    69.20    69.20    69.20%\n",
      "Epoch    388 Step:      0 Loss:   0.5686 Training Acc:    69.24    69.24    69.24%\n",
      "Epoch    389 Step:      0 Loss:   0.5686 Training Acc:    69.23    69.23    69.23%\n",
      "Epoch    390 Step:      0 Loss:   0.5686 Training Acc:    69.23    69.23    69.23%\n",
      "Epoch    391 Step:      0 Loss:   0.5683 Training Acc:    69.26    69.26    69.26%\n",
      "Epoch    392 Step:      0 Loss:   0.5679 Training Acc:    69.27    69.27    69.27%\n",
      "Epoch    393 Step:      0 Loss:   0.5679 Training Acc:    69.29    69.29    69.29%\n",
      "Epoch    394 Step:      0 Loss:   0.5679 Training Acc:    69.25    69.25    69.25%\n",
      "Epoch    395 Step:      0 Loss:   0.5677 Training Acc:    69.30    69.30    69.30%\n",
      "Epoch    396 Step:      0 Loss:   0.5675 Training Acc:    69.30    69.30    69.30%\n",
      "Epoch    397 Step:      0 Loss:   0.5674 Training Acc:    69.30    69.30    69.30%\n",
      "Epoch    398 Step:      0 Loss:   0.5674 Training Acc:    69.32    69.32    69.32%\n",
      "Epoch    399 Step:      0 Loss:   0.5672 Training Acc:    69.33    69.33    69.33%\n",
      "Epoch    400 Step:      0 Loss:   0.5670 Training Acc:    69.32    69.32    69.32%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    401 Step:      0 Loss:   0.5670 Training Acc:    69.35    69.35    69.35%\n",
      "Epoch    402 Step:      0 Loss:   0.5669 Training Acc:    69.38    69.38    69.38%\n",
      "Epoch    403 Step:      0 Loss:   0.5667 Training Acc:    69.36    69.36    69.36%\n",
      "Epoch    404 Step:      0 Loss:   0.5666 Training Acc:    69.37    69.37    69.37%\n",
      "Epoch    405 Step:      0 Loss:   0.5665 Training Acc:    69.39    69.39    69.39%\n",
      "Epoch    406 Step:      0 Loss:   0.5664 Training Acc:    69.38    69.38    69.38%\n",
      "Epoch    407 Step:      0 Loss:   0.5662 Training Acc:    69.41    69.41    69.41%\n",
      "Epoch    408 Step:      0 Loss:   0.5662 Training Acc:    69.42    69.42    69.42%\n",
      "Epoch    409 Step:      0 Loss:   0.5661 Training Acc:    69.41    69.41    69.41%\n",
      "Epoch    410 Step:      0 Loss:   0.5659 Training Acc:    69.44    69.44    69.44%\n",
      "Epoch    411 Step:      0 Loss:   0.5658 Training Acc:    69.45    69.45    69.45%\n",
      "Epoch    412 Step:      0 Loss:   0.5657 Training Acc:    69.44    69.44    69.44%\n",
      "Epoch    413 Step:      0 Loss:   0.5656 Training Acc:    69.48    69.48    69.48%\n",
      "Epoch    414 Step:      0 Loss:   0.5655 Training Acc:    69.48    69.48    69.48%\n",
      "Epoch    415 Step:      0 Loss:   0.5654 Training Acc:    69.48    69.48    69.48%\n",
      "Epoch    416 Step:      0 Loss:   0.5653 Training Acc:    69.49    69.49    69.49%\n",
      "Epoch    417 Step:      0 Loss:   0.5652 Training Acc:    69.50    69.50    69.50%\n",
      "Epoch    418 Step:      0 Loss:   0.5650 Training Acc:    69.51    69.51    69.51%\n",
      "Epoch    419 Step:      0 Loss:   0.5649 Training Acc:    69.52    69.52    69.52%\n",
      "Epoch    420 Step:      0 Loss:   0.5648 Training Acc:    69.53    69.53    69.53%\n",
      "Epoch    421 Step:      0 Loss:   0.5647 Training Acc:    69.54    69.54    69.54%\n",
      "Epoch    422 Step:      0 Loss:   0.5646 Training Acc:    69.55    69.55    69.55%\n",
      "Epoch    423 Step:      0 Loss:   0.5645 Training Acc:    69.55    69.55    69.55%\n",
      "Epoch    424 Step:      0 Loss:   0.5644 Training Acc:    69.57    69.57    69.57%\n",
      "Epoch    425 Step:      0 Loss:   0.5643 Training Acc:    69.58    69.58    69.58%\n",
      "Epoch    426 Step:      0 Loss:   0.5642 Training Acc:    69.58    69.58    69.58%\n",
      "Epoch    427 Step:      0 Loss:   0.5641 Training Acc:    69.59    69.59    69.59%\n",
      "Epoch    428 Step:      0 Loss:   0.5640 Training Acc:    69.60    69.60    69.60%\n",
      "Epoch    429 Step:      0 Loss:   0.5639 Training Acc:    69.61    69.61    69.61%\n",
      "Epoch    430 Step:      0 Loss:   0.5638 Training Acc:    69.63    69.63    69.63%\n",
      "Epoch    431 Step:      0 Loss:   0.5637 Training Acc:    69.63    69.63    69.63%\n",
      "Epoch    432 Step:      0 Loss:   0.5636 Training Acc:    69.64    69.64    69.64%\n",
      "Epoch    433 Step:      0 Loss:   0.5635 Training Acc:    69.66    69.66    69.66%\n",
      "Epoch    434 Step:      0 Loss:   0.5633 Training Acc:    69.66    69.66    69.66%\n",
      "Epoch    435 Step:      0 Loss:   0.5632 Training Acc:    69.67    69.67    69.67%\n",
      "Epoch    436 Step:      0 Loss:   0.5631 Training Acc:    69.68    69.68    69.68%\n",
      "Epoch    437 Step:      0 Loss:   0.5630 Training Acc:    69.68    69.68    69.68%\n",
      "Epoch    438 Step:      0 Loss:   0.5629 Training Acc:    69.70    69.70    69.70%\n",
      "Epoch    439 Step:      0 Loss:   0.5628 Training Acc:    69.71    69.71    69.71%\n",
      "Epoch    440 Step:      0 Loss:   0.5627 Training Acc:    69.71    69.71    69.71%\n",
      "Epoch    441 Step:      0 Loss:   0.5626 Training Acc:    69.71    69.71    69.71%\n",
      "Epoch    442 Step:      0 Loss:   0.5625 Training Acc:    69.73    69.73    69.73%\n",
      "Epoch    443 Step:      0 Loss:   0.5624 Training Acc:    69.74    69.74    69.74%\n",
      "Epoch    444 Step:      0 Loss:   0.5623 Training Acc:    69.74    69.74    69.74%\n",
      "Epoch    445 Step:      0 Loss:   0.5622 Training Acc:    69.74    69.74    69.74%\n",
      "Epoch    446 Step:      0 Loss:   0.5621 Training Acc:    69.76    69.76    69.76%\n",
      "Epoch    447 Step:      0 Loss:   0.5620 Training Acc:    69.76    69.76    69.76%\n",
      "Epoch    448 Step:      0 Loss:   0.5619 Training Acc:    69.77    69.77    69.77%\n",
      "Epoch    449 Step:      0 Loss:   0.5618 Training Acc:    69.77    69.77    69.77%\n",
      "Epoch    450 Step:      0 Loss:   0.5617 Training Acc:    69.78    69.78    69.78%\n",
      "Epoch    451 Step:      0 Loss:   0.5616 Training Acc:    69.78    69.78    69.78%\n",
      "Epoch    452 Step:      0 Loss:   0.5615 Training Acc:    69.79    69.79    69.79%\n",
      "Epoch    453 Step:      0 Loss:   0.5614 Training Acc:    69.78    69.78    69.78%\n",
      "Epoch    454 Step:      0 Loss:   0.5613 Training Acc:    69.79    69.79    69.79%\n",
      "Epoch    455 Step:      0 Loss:   0.5612 Training Acc:    69.80    69.80    69.80%\n",
      "Epoch    456 Step:      0 Loss:   0.5611 Training Acc:    69.82    69.82    69.82%\n",
      "Epoch    457 Step:      0 Loss:   0.5610 Training Acc:    69.83    69.83    69.83%\n",
      "Epoch    458 Step:      0 Loss:   0.5609 Training Acc:    69.83    69.83    69.83%\n",
      "Epoch    459 Step:      0 Loss:   0.5607 Training Acc:    69.85    69.85    69.85%\n",
      "Epoch    460 Step:      0 Loss:   0.5606 Training Acc:    69.86    69.86    69.86%\n",
      "Epoch    461 Step:      0 Loss:   0.5605 Training Acc:    69.87    69.87    69.87%\n",
      "Epoch    462 Step:      0 Loss:   0.5604 Training Acc:    69.87    69.87    69.87%\n",
      "Epoch    463 Step:      0 Loss:   0.5603 Training Acc:    69.89    69.89    69.89%\n",
      "Epoch    464 Step:      0 Loss:   0.5602 Training Acc:    69.89    69.89    69.89%\n",
      "Epoch    465 Step:      0 Loss:   0.5601 Training Acc:    69.91    69.91    69.91%\n",
      "Epoch    466 Step:      0 Loss:   0.5600 Training Acc:    69.90    69.90    69.90%\n",
      "Epoch    467 Step:      0 Loss:   0.5599 Training Acc:    69.95    69.95    69.95%\n",
      "Epoch    468 Step:      0 Loss:   0.5598 Training Acc:    69.93    69.93    69.93%\n",
      "Epoch    469 Step:      0 Loss:   0.5598 Training Acc:    69.96    69.96    69.96%\n",
      "Epoch    470 Step:      0 Loss:   0.5597 Training Acc:    69.95    69.95    69.95%\n",
      "Epoch    471 Step:      0 Loss:   0.5597 Training Acc:    70.00    70.00    70.00%\n",
      "Epoch    472 Step:      0 Loss:   0.5596 Training Acc:    69.94    69.94    69.94%\n",
      "Epoch    473 Step:      0 Loss:   0.5596 Training Acc:    69.99    69.99    69.99%\n",
      "Epoch    474 Step:      0 Loss:   0.5594 Training Acc:    69.96    69.96    69.96%\n",
      "Epoch    475 Step:      0 Loss:   0.5592 Training Acc:    70.02    70.02    70.02%\n",
      "Epoch    476 Step:      0 Loss:   0.5590 Training Acc:    70.01    70.01    70.01%\n",
      "Epoch    477 Step:      0 Loss:   0.5589 Training Acc:    70.03    70.03    70.03%\n",
      "Epoch    478 Step:      0 Loss:   0.5589 Training Acc:    70.05    70.05    70.05%\n",
      "Epoch    479 Step:      0 Loss:   0.5588 Training Acc:    70.02    70.02    70.02%\n",
      "Epoch    480 Step:      0 Loss:   0.5587 Training Acc:    70.06    70.06    70.06%\n",
      "Epoch    481 Step:      0 Loss:   0.5585 Training Acc:    70.07    70.07    70.07%\n",
      "Epoch    482 Step:      0 Loss:   0.5584 Training Acc:    70.06    70.06    70.06%\n",
      "Epoch    483 Step:      0 Loss:   0.5584 Training Acc:    70.09    70.09    70.09%\n",
      "Epoch    484 Step:      0 Loss:   0.5583 Training Acc:    70.06    70.06    70.06%\n",
      "Epoch    485 Step:      0 Loss:   0.5582 Training Acc:    70.10    70.10    70.10%\n",
      "Epoch    486 Step:      0 Loss:   0.5580 Training Acc:    70.11    70.11    70.11%\n",
      "Epoch    487 Step:      0 Loss:   0.5579 Training Acc:    70.13    70.13    70.13%\n",
      "Epoch    488 Step:      0 Loss:   0.5578 Training Acc:    70.14    70.14    70.14%\n",
      "Epoch    489 Step:      0 Loss:   0.5578 Training Acc:    70.14    70.14    70.14%\n",
      "Epoch    490 Step:      0 Loss:   0.5577 Training Acc:    70.16    70.16    70.16%\n",
      "Epoch    491 Step:      0 Loss:   0.5575 Training Acc:    70.16    70.16    70.16%\n",
      "Epoch    492 Step:      0 Loss:   0.5574 Training Acc:    70.18    70.18    70.18%\n",
      "Epoch    493 Step:      0 Loss:   0.5573 Training Acc:    70.20    70.20    70.20%\n",
      "Epoch    494 Step:      0 Loss:   0.5572 Training Acc:    70.18    70.18    70.18%\n",
      "Epoch    495 Step:      0 Loss:   0.5571 Training Acc:    70.20    70.20    70.20%\n",
      "Epoch    496 Step:      0 Loss:   0.5570 Training Acc:    70.20    70.20    70.20%\n",
      "Epoch    497 Step:      0 Loss:   0.5569 Training Acc:    70.22    70.22    70.22%\n",
      "Epoch    498 Step:      0 Loss:   0.5568 Training Acc:    70.24    70.24    70.24%\n",
      "Epoch    499 Step:      0 Loss:   0.5567 Training Acc:    70.25    70.25    70.25%\n",
      "Epoch    500 Step:      0 Loss:   0.5566 Training Acc:    70.24    70.24    70.24%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    501 Step:      0 Loss:   0.5565 Training Acc:    70.24    70.24    70.24%\n",
      "Epoch    502 Step:      0 Loss:   0.5564 Training Acc:    70.25    70.25    70.25%\n",
      "Epoch    503 Step:      0 Loss:   0.5563 Training Acc:    70.25    70.25    70.25%\n",
      "Epoch    504 Step:      0 Loss:   0.5562 Training Acc:    70.27    70.27    70.27%\n",
      "Epoch    505 Step:      0 Loss:   0.5561 Training Acc:    70.28    70.28    70.28%\n",
      "Epoch    506 Step:      0 Loss:   0.5560 Training Acc:    70.29    70.29    70.29%\n",
      "Epoch    507 Step:      0 Loss:   0.5559 Training Acc:    70.29    70.29    70.29%\n",
      "Epoch    508 Step:      0 Loss:   0.5558 Training Acc:    70.29    70.29    70.29%\n",
      "Epoch    509 Step:      0 Loss:   0.5557 Training Acc:    70.30    70.30    70.30%\n",
      "Epoch    510 Step:      0 Loss:   0.5556 Training Acc:    70.30    70.30    70.30%\n",
      "Epoch    511 Step:      0 Loss:   0.5555 Training Acc:    70.31    70.31    70.31%\n",
      "Epoch    512 Step:      0 Loss:   0.5554 Training Acc:    70.32    70.32    70.32%\n",
      "Epoch    513 Step:      0 Loss:   0.5553 Training Acc:    70.33    70.33    70.33%\n",
      "Epoch    514 Step:      0 Loss:   0.5552 Training Acc:    70.33    70.33    70.33%\n",
      "Epoch    515 Step:      0 Loss:   0.5551 Training Acc:    70.34    70.34    70.34%\n",
      "Epoch    516 Step:      0 Loss:   0.5550 Training Acc:    70.33    70.33    70.33%\n",
      "Epoch    517 Step:      0 Loss:   0.5549 Training Acc:    70.36    70.36    70.36%\n",
      "Epoch    518 Step:      0 Loss:   0.5548 Training Acc:    70.35    70.35    70.35%\n",
      "Epoch    519 Step:      0 Loss:   0.5547 Training Acc:    70.38    70.38    70.38%\n",
      "Epoch    520 Step:      0 Loss:   0.5546 Training Acc:    70.37    70.37    70.37%\n",
      "Epoch    521 Step:      0 Loss:   0.5545 Training Acc:    70.38    70.38    70.38%\n",
      "Epoch    522 Step:      0 Loss:   0.5544 Training Acc:    70.39    70.39    70.39%\n",
      "Epoch    523 Step:      0 Loss:   0.5543 Training Acc:    70.40    70.40    70.40%\n",
      "Epoch    524 Step:      0 Loss:   0.5542 Training Acc:    70.41    70.41    70.41%\n",
      "Epoch    525 Step:      0 Loss:   0.5541 Training Acc:    70.41    70.41    70.41%\n",
      "Epoch    526 Step:      0 Loss:   0.5541 Training Acc:    70.43    70.43    70.43%\n",
      "Epoch    527 Step:      0 Loss:   0.5540 Training Acc:    70.42    70.42    70.42%\n",
      "Epoch    528 Step:      0 Loss:   0.5539 Training Acc:    70.45    70.45    70.45%\n",
      "Epoch    529 Step:      0 Loss:   0.5538 Training Acc:    70.42    70.42    70.42%\n",
      "Epoch    530 Step:      0 Loss:   0.5538 Training Acc:    70.45    70.45    70.45%\n",
      "Epoch    531 Step:      0 Loss:   0.5539 Training Acc:    70.45    70.45    70.45%\n",
      "Epoch    532 Step:      0 Loss:   0.5540 Training Acc:    70.44    70.44    70.44%\n",
      "Epoch    533 Step:      0 Loss:   0.5539 Training Acc:    70.47    70.47    70.47%\n",
      "Epoch    534 Step:      0 Loss:   0.5536 Training Acc:    70.48    70.48    70.48%\n",
      "Epoch    535 Step:      0 Loss:   0.5532 Training Acc:    70.49    70.49    70.49%\n",
      "Epoch    536 Step:      0 Loss:   0.5532 Training Acc:    70.48    70.48    70.48%\n",
      "Epoch    537 Step:      0 Loss:   0.5533 Training Acc:    70.51    70.51    70.51%\n",
      "Epoch    538 Step:      0 Loss:   0.5532 Training Acc:    70.49    70.49    70.49%\n",
      "Epoch    539 Step:      0 Loss:   0.5529 Training Acc:    70.53    70.53    70.53%\n",
      "Epoch    540 Step:      0 Loss:   0.5527 Training Acc:    70.52    70.52    70.52%\n",
      "Epoch    541 Step:      0 Loss:   0.5528 Training Acc:    70.53    70.53    70.53%\n",
      "Epoch    542 Step:      0 Loss:   0.5528 Training Acc:    70.53    70.53    70.53%\n",
      "Epoch    543 Step:      0 Loss:   0.5526 Training Acc:    70.54    70.54    70.54%\n",
      "Epoch    544 Step:      0 Loss:   0.5523 Training Acc:    70.56    70.56    70.56%\n",
      "Epoch    545 Step:      0 Loss:   0.5523 Training Acc:    70.57    70.57    70.57%\n",
      "Epoch    546 Step:      0 Loss:   0.5523 Training Acc:    70.55    70.55    70.55%\n",
      "Epoch    547 Step:      0 Loss:   0.5522 Training Acc:    70.59    70.59    70.59%\n",
      "Epoch    548 Step:      0 Loss:   0.5520 Training Acc:    70.56    70.56    70.56%\n",
      "Epoch    549 Step:      0 Loss:   0.5520 Training Acc:    70.60    70.60    70.60%\n",
      "Epoch    550 Step:      0 Loss:   0.5520 Training Acc:    70.57    70.57    70.57%\n",
      "Epoch    551 Step:      0 Loss:   0.5519 Training Acc:    70.60    70.60    70.60%\n",
      "Epoch    552 Step:      0 Loss:   0.5517 Training Acc:    70.58    70.58    70.58%\n",
      "Epoch    553 Step:      0 Loss:   0.5515 Training Acc:    70.64    70.64    70.64%\n",
      "Epoch    554 Step:      0 Loss:   0.5515 Training Acc:    70.62    70.62    70.62%\n",
      "Epoch    555 Step:      0 Loss:   0.5514 Training Acc:    70.64    70.64    70.64%\n",
      "Epoch    556 Step:      0 Loss:   0.5513 Training Acc:    70.66    70.66    70.66%\n",
      "Epoch    557 Step:      0 Loss:   0.5512 Training Acc:    70.64    70.64    70.64%\n",
      "Epoch    558 Step:      0 Loss:   0.5511 Training Acc:    70.68    70.68    70.68%\n",
      "Epoch    559 Step:      0 Loss:   0.5510 Training Acc:    70.64    70.64    70.64%\n",
      "Epoch    560 Step:      0 Loss:   0.5509 Training Acc:    70.69    70.69    70.69%\n",
      "Epoch    561 Step:      0 Loss:   0.5508 Training Acc:    70.69    70.69    70.69%\n",
      "Epoch    562 Step:      0 Loss:   0.5507 Training Acc:    70.70    70.70    70.70%\n",
      "Epoch    563 Step:      0 Loss:   0.5506 Training Acc:    70.70    70.70    70.70%\n",
      "Epoch    564 Step:      0 Loss:   0.5505 Training Acc:    70.69    70.69    70.69%\n",
      "Epoch    565 Step:      0 Loss:   0.5504 Training Acc:    70.72    70.72    70.72%\n",
      "Epoch    566 Step:      0 Loss:   0.5503 Training Acc:    70.73    70.73    70.73%\n",
      "Epoch    567 Step:      0 Loss:   0.5502 Training Acc:    70.74    70.74    70.74%\n",
      "Epoch    568 Step:      0 Loss:   0.5501 Training Acc:    70.75    70.75    70.75%\n",
      "Epoch    569 Step:      0 Loss:   0.5500 Training Acc:    70.74    70.74    70.74%\n",
      "Epoch    570 Step:      0 Loss:   0.5499 Training Acc:    70.76    70.76    70.76%\n",
      "Epoch    571 Step:      0 Loss:   0.5498 Training Acc:    70.76    70.76    70.76%\n",
      "Epoch    572 Step:      0 Loss:   0.5497 Training Acc:    70.78    70.78    70.78%\n",
      "Epoch    573 Step:      0 Loss:   0.5496 Training Acc:    70.78    70.78    70.78%\n",
      "Epoch    574 Step:      0 Loss:   0.5495 Training Acc:    70.79    70.79    70.79%\n",
      "Epoch    575 Step:      0 Loss:   0.5494 Training Acc:    70.79    70.79    70.79%\n",
      "Epoch    576 Step:      0 Loss:   0.5493 Training Acc:    70.81    70.81    70.81%\n",
      "Epoch    577 Step:      0 Loss:   0.5492 Training Acc:    70.82    70.82    70.82%\n",
      "Epoch    578 Step:      0 Loss:   0.5491 Training Acc:    70.82    70.82    70.82%\n",
      "Epoch    579 Step:      0 Loss:   0.5490 Training Acc:    70.83    70.83    70.83%\n",
      "Epoch    580 Step:      0 Loss:   0.5489 Training Acc:    70.82    70.82    70.82%\n",
      "Epoch    581 Step:      0 Loss:   0.5488 Training Acc:    70.83    70.83    70.83%\n",
      "Epoch    582 Step:      0 Loss:   0.5487 Training Acc:    70.84    70.84    70.84%\n",
      "Epoch    583 Step:      0 Loss:   0.5486 Training Acc:    70.85    70.85    70.85%\n",
      "Epoch    584 Step:      0 Loss:   0.5485 Training Acc:    70.85    70.85    70.85%\n",
      "Epoch    585 Step:      0 Loss:   0.5484 Training Acc:    70.87    70.87    70.87%\n",
      "Epoch    586 Step:      0 Loss:   0.5483 Training Acc:    70.87    70.87    70.87%\n",
      "Epoch    587 Step:      0 Loss:   0.5483 Training Acc:    70.88    70.88    70.88%\n",
      "Epoch    588 Step:      0 Loss:   0.5482 Training Acc:    70.88    70.88    70.88%\n",
      "Epoch    589 Step:      0 Loss:   0.5481 Training Acc:    70.89    70.89    70.89%\n",
      "Epoch    590 Step:      0 Loss:   0.5480 Training Acc:    70.89    70.89    70.89%\n",
      "Epoch    591 Step:      0 Loss:   0.5479 Training Acc:    70.90    70.90    70.90%\n",
      "Epoch    592 Step:      0 Loss:   0.5479 Training Acc:    70.89    70.89    70.89%\n",
      "Epoch    593 Step:      0 Loss:   0.5478 Training Acc:    70.92    70.92    70.92%\n",
      "Epoch    594 Step:      0 Loss:   0.5477 Training Acc:    70.91    70.91    70.91%\n",
      "Epoch    595 Step:      0 Loss:   0.5475 Training Acc:    70.95    70.95    70.95%\n",
      "Epoch    596 Step:      0 Loss:   0.5474 Training Acc:    70.93    70.93    70.93%\n",
      "Epoch    597 Step:      0 Loss:   0.5473 Training Acc:    70.95    70.95    70.95%\n",
      "Epoch    598 Step:      0 Loss:   0.5472 Training Acc:    70.96    70.96    70.96%\n",
      "Epoch    599 Step:      0 Loss:   0.5472 Training Acc:    70.96    70.96    70.96%\n",
      "Epoch    600 Step:      0 Loss:   0.5471 Training Acc:    70.96    70.96    70.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    601 Step:      0 Loss:   0.5470 Training Acc:    70.96    70.96    70.96%\n",
      "Epoch    602 Step:      0 Loss:   0.5469 Training Acc:    70.98    70.98    70.98%\n",
      "Epoch    603 Step:      0 Loss:   0.5467 Training Acc:    70.99    70.99    70.99%\n",
      "Epoch    604 Step:      0 Loss:   0.5466 Training Acc:    71.01    71.01    71.01%\n",
      "Epoch    605 Step:      0 Loss:   0.5465 Training Acc:    71.03    71.03    71.03%\n",
      "Epoch    606 Step:      0 Loss:   0.5464 Training Acc:    71.02    71.02    71.02%\n",
      "Epoch    607 Step:      0 Loss:   0.5463 Training Acc:    71.03    71.03    71.03%\n",
      "Epoch    608 Step:      0 Loss:   0.5462 Training Acc:    71.05    71.05    71.05%\n",
      "Epoch    609 Step:      0 Loss:   0.5461 Training Acc:    71.04    71.04    71.04%\n",
      "Epoch    610 Step:      0 Loss:   0.5460 Training Acc:    71.06    71.06    71.06%\n",
      "Epoch    611 Step:      0 Loss:   0.5459 Training Acc:    71.05    71.05    71.05%\n",
      "Epoch    612 Step:      0 Loss:   0.5458 Training Acc:    71.08    71.08    71.08%\n",
      "Epoch    613 Step:      0 Loss:   0.5457 Training Acc:    71.06    71.06    71.06%\n",
      "Epoch    614 Step:      0 Loss:   0.5457 Training Acc:    71.10    71.10    71.10%\n",
      "Epoch    615 Step:      0 Loss:   0.5456 Training Acc:    71.07    71.07    71.07%\n",
      "Epoch    616 Step:      0 Loss:   0.5455 Training Acc:    71.12    71.12    71.12%\n",
      "Epoch    617 Step:      0 Loss:   0.5453 Training Acc:    71.10    71.10    71.10%\n",
      "Epoch    618 Step:      0 Loss:   0.5452 Training Acc:    71.13    71.13    71.13%\n",
      "Epoch    619 Step:      0 Loss:   0.5451 Training Acc:    71.12    71.12    71.12%\n",
      "Epoch    620 Step:      0 Loss:   0.5450 Training Acc:    71.14    71.14    71.14%\n",
      "Epoch    621 Step:      0 Loss:   0.5449 Training Acc:    71.14    71.14    71.14%\n",
      "Epoch    622 Step:      0 Loss:   0.5448 Training Acc:    71.14    71.14    71.14%\n",
      "Epoch    623 Step:      0 Loss:   0.5447 Training Acc:    71.16    71.16    71.16%\n",
      "Epoch    624 Step:      0 Loss:   0.5446 Training Acc:    71.15    71.15    71.15%\n",
      "Epoch    625 Step:      0 Loss:   0.5445 Training Acc:    71.19    71.19    71.19%\n",
      "Epoch    626 Step:      0 Loss:   0.5443 Training Acc:    71.18    71.18    71.18%\n",
      "Epoch    627 Step:      0 Loss:   0.5442 Training Acc:    71.20    71.20    71.20%\n",
      "Epoch    628 Step:      0 Loss:   0.5441 Training Acc:    71.21    71.21    71.21%\n",
      "Epoch    629 Step:      0 Loss:   0.5440 Training Acc:    71.21    71.21    71.21%\n",
      "Epoch    630 Step:      0 Loss:   0.5439 Training Acc:    71.22    71.22    71.22%\n",
      "Epoch    631 Step:      0 Loss:   0.5438 Training Acc:    71.22    71.22    71.22%\n",
      "Epoch    632 Step:      0 Loss:   0.5438 Training Acc:    71.19    71.19    71.19%\n",
      "Epoch    633 Step:      0 Loss:   0.5437 Training Acc:    71.25    71.25    71.25%\n",
      "Epoch    634 Step:      0 Loss:   0.5437 Training Acc:    71.20    71.20    71.20%\n",
      "Epoch    635 Step:      0 Loss:   0.5437 Training Acc:    71.24    71.24    71.24%\n",
      "Epoch    636 Step:      0 Loss:   0.5438 Training Acc:    71.21    71.21    71.21%\n",
      "Epoch    637 Step:      0 Loss:   0.5440 Training Acc:    71.21    71.21    71.21%\n",
      "Epoch    638 Step:      0 Loss:   0.5438 Training Acc:    71.23    71.23    71.23%\n",
      "Epoch    639 Step:      0 Loss:   0.5433 Training Acc:    71.26    71.26    71.26%\n",
      "Epoch    640 Step:      0 Loss:   0.5429 Training Acc:    71.29    71.29    71.29%\n",
      "Epoch    641 Step:      0 Loss:   0.5429 Training Acc:    71.28    71.28    71.28%\n",
      "Epoch    642 Step:      0 Loss:   0.5431 Training Acc:    71.27    71.27    71.27%\n",
      "Epoch    643 Step:      0 Loss:   0.5429 Training Acc:    71.28    71.28    71.28%\n",
      "Epoch    644 Step:      0 Loss:   0.5426 Training Acc:    71.32    71.32    71.32%\n",
      "Epoch    645 Step:      0 Loss:   0.5423 Training Acc:    71.36    71.36    71.36%\n",
      "Epoch    646 Step:      0 Loss:   0.5423 Training Acc:    71.30    71.30    71.30%\n",
      "Epoch    647 Step:      0 Loss:   0.5424 Training Acc:    71.32    71.32    71.32%\n",
      "Epoch    648 Step:      0 Loss:   0.5422 Training Acc:    71.32    71.32    71.32%\n",
      "Epoch    649 Step:      0 Loss:   0.5419 Training Acc:    71.39    71.39    71.39%\n",
      "Epoch    650 Step:      0 Loss:   0.5417 Training Acc:    71.41    71.41    71.41%\n",
      "Epoch    651 Step:      0 Loss:   0.5417 Training Acc:    71.37    71.37    71.37%\n",
      "Epoch    652 Step:      0 Loss:   0.5417 Training Acc:    71.38    71.38    71.38%\n",
      "Epoch    653 Step:      0 Loss:   0.5415 Training Acc:    71.38    71.38    71.38%\n",
      "Epoch    654 Step:      0 Loss:   0.5413 Training Acc:    71.43    71.43    71.43%\n",
      "Epoch    655 Step:      0 Loss:   0.5412 Training Acc:    71.46    71.46    71.46%\n",
      "Epoch    656 Step:      0 Loss:   0.5412 Training Acc:    71.44    71.44    71.44%\n",
      "Epoch    657 Step:      0 Loss:   0.5411 Training Acc:    71.45    71.45    71.45%\n",
      "Epoch    658 Step:      0 Loss:   0.5409 Training Acc:    71.46    71.46    71.46%\n",
      "Epoch    659 Step:      0 Loss:   0.5407 Training Acc:    71.47    71.47    71.47%\n",
      "Epoch    660 Step:      0 Loss:   0.5406 Training Acc:    71.48    71.48    71.48%\n",
      "Epoch    661 Step:      0 Loss:   0.5405 Training Acc:    71.49    71.49    71.49%\n",
      "Epoch    662 Step:      0 Loss:   0.5404 Training Acc:    71.50    71.50    71.50%\n",
      "Epoch    663 Step:      0 Loss:   0.5403 Training Acc:    71.52    71.52    71.52%\n",
      "Epoch    664 Step:      0 Loss:   0.5401 Training Acc:    71.53    71.53    71.53%\n",
      "Epoch    665 Step:      0 Loss:   0.5400 Training Acc:    71.53    71.53    71.53%\n",
      "Epoch    666 Step:      0 Loss:   0.5399 Training Acc:    71.54    71.54    71.54%\n",
      "Epoch    667 Step:      0 Loss:   0.5398 Training Acc:    71.55    71.55    71.55%\n",
      "Epoch    668 Step:      0 Loss:   0.5396 Training Acc:    71.56    71.56    71.56%\n",
      "Epoch    669 Step:      0 Loss:   0.5395 Training Acc:    71.56    71.56    71.56%\n",
      "Epoch    670 Step:      0 Loss:   0.5394 Training Acc:    71.61    71.61    71.61%\n",
      "Epoch    671 Step:      0 Loss:   0.5393 Training Acc:    71.60    71.60    71.60%\n",
      "Epoch    672 Step:      0 Loss:   0.5392 Training Acc:    71.61    71.61    71.61%\n",
      "Epoch    673 Step:      0 Loss:   0.5391 Training Acc:    71.59    71.59    71.59%\n",
      "Epoch    674 Step:      0 Loss:   0.5390 Training Acc:    71.63    71.63    71.63%\n",
      "Epoch    675 Step:      0 Loss:   0.5388 Training Acc:    71.62    71.62    71.62%\n",
      "Epoch    676 Step:      0 Loss:   0.5387 Training Acc:    71.64    71.64    71.64%\n",
      "Epoch    677 Step:      0 Loss:   0.5385 Training Acc:    71.67    71.67    71.67%\n",
      "Epoch    678 Step:      0 Loss:   0.5383 Training Acc:    71.67    71.67    71.67%\n",
      "Epoch    679 Step:      0 Loss:   0.5381 Training Acc:    71.68    71.68    71.68%\n",
      "Epoch    680 Step:      0 Loss:   0.5380 Training Acc:    71.71    71.71    71.71%\n",
      "Epoch    681 Step:      0 Loss:   0.5378 Training Acc:    71.72    71.72    71.72%\n",
      "Epoch    682 Step:      0 Loss:   0.5377 Training Acc:    71.73    71.73    71.73%\n",
      "Epoch    683 Step:      0 Loss:   0.5375 Training Acc:    71.75    71.75    71.75%\n",
      "Epoch    684 Step:      0 Loss:   0.5374 Training Acc:    71.75    71.75    71.75%\n",
      "Epoch    685 Step:      0 Loss:   0.5373 Training Acc:    71.74    71.74    71.74%\n",
      "Epoch    686 Step:      0 Loss:   0.5372 Training Acc:    71.77    71.77    71.77%\n",
      "Epoch    687 Step:      0 Loss:   0.5371 Training Acc:    71.76    71.76    71.76%\n",
      "Epoch    688 Step:      0 Loss:   0.5369 Training Acc:    71.78    71.78    71.78%\n",
      "Epoch    689 Step:      0 Loss:   0.5368 Training Acc:    71.78    71.78    71.78%\n",
      "Epoch    690 Step:      0 Loss:   0.5366 Training Acc:    71.80    71.80    71.80%\n",
      "Epoch    691 Step:      0 Loss:   0.5364 Training Acc:    71.81    71.81    71.81%\n",
      "Epoch    692 Step:      0 Loss:   0.5362 Training Acc:    71.85    71.85    71.85%\n",
      "Epoch    693 Step:      0 Loss:   0.5360 Training Acc:    71.87    71.87    71.87%\n",
      "Epoch    694 Step:      0 Loss:   0.5358 Training Acc:    71.87    71.87    71.87%\n",
      "Epoch    695 Step:      0 Loss:   0.5357 Training Acc:    71.88    71.88    71.88%\n",
      "Epoch    696 Step:      0 Loss:   0.5356 Training Acc:    71.87    71.87    71.87%\n",
      "Epoch    697 Step:      0 Loss:   0.5355 Training Acc:    71.87    71.87    71.87%\n",
      "Epoch    698 Step:      0 Loss:   0.5354 Training Acc:    71.87    71.87    71.87%\n",
      "Epoch    699 Step:      0 Loss:   0.5353 Training Acc:    71.90    71.90    71.90%\n",
      "Epoch    700 Step:      0 Loss:   0.5351 Training Acc:    71.89    71.89    71.89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    701 Step:      0 Loss:   0.5349 Training Acc:    71.94    71.94    71.94%\n",
      "Epoch    702 Step:      0 Loss:   0.5347 Training Acc:    71.95    71.95    71.95%\n",
      "Epoch    703 Step:      0 Loss:   0.5345 Training Acc:    71.97    71.97    71.97%\n",
      "Epoch    704 Step:      0 Loss:   0.5344 Training Acc:    71.96    71.96    71.96%\n",
      "Epoch    705 Step:      0 Loss:   0.5343 Training Acc:    71.96    71.96    71.96%\n",
      "Epoch    706 Step:      0 Loss:   0.5343 Training Acc:    71.97    71.97    71.97%\n",
      "Epoch    707 Step:      0 Loss:   0.5343 Training Acc:    71.94    71.94    71.94%\n",
      "Epoch    708 Step:      0 Loss:   0.5342 Training Acc:    71.98    71.98    71.98%\n",
      "Epoch    709 Step:      0 Loss:   0.5338 Training Acc:    71.99    71.99    71.99%\n",
      "Epoch    710 Step:      0 Loss:   0.5334 Training Acc:    72.03    72.03    72.03%\n",
      "Epoch    711 Step:      0 Loss:   0.5331 Training Acc:    72.06    72.06    72.06%\n",
      "Epoch    712 Step:      0 Loss:   0.5330 Training Acc:    72.08    72.08    72.08%\n",
      "Epoch    713 Step:      0 Loss:   0.5329 Training Acc:    72.07    72.07    72.07%\n",
      "Epoch    714 Step:      0 Loss:   0.5327 Training Acc:    72.07    72.07    72.07%\n",
      "Epoch    715 Step:      0 Loss:   0.5324 Training Acc:    72.11    72.11    72.11%\n",
      "Epoch    716 Step:      0 Loss:   0.5322 Training Acc:    72.14    72.14    72.14%\n",
      "Epoch    717 Step:      0 Loss:   0.5322 Training Acc:    72.13    72.13    72.13%\n",
      "Epoch    718 Step:      0 Loss:   0.5321 Training Acc:    72.16    72.16    72.16%\n",
      "Epoch    719 Step:      0 Loss:   0.5320 Training Acc:    72.15    72.15    72.15%\n",
      "Epoch    720 Step:      0 Loss:   0.5317 Training Acc:    72.19    72.19    72.19%\n",
      "Epoch    721 Step:      0 Loss:   0.5313 Training Acc:    72.19    72.19    72.19%\n",
      "Epoch    722 Step:      0 Loss:   0.5311 Training Acc:    72.21    72.21    72.21%\n",
      "Epoch    723 Step:      0 Loss:   0.5309 Training Acc:    72.22    72.22    72.22%\n",
      "Epoch    724 Step:      0 Loss:   0.5308 Training Acc:    72.23    72.23    72.23%\n",
      "Epoch    725 Step:      0 Loss:   0.5306 Training Acc:    72.26    72.26    72.26%\n",
      "Epoch    726 Step:      0 Loss:   0.5304 Training Acc:    72.26    72.26    72.26%\n",
      "Epoch    727 Step:      0 Loss:   0.5301 Training Acc:    72.30    72.30    72.30%\n",
      "Epoch    728 Step:      0 Loss:   0.5299 Training Acc:    72.29    72.29    72.29%\n",
      "Epoch    729 Step:      0 Loss:   0.5298 Training Acc:    72.32    72.32    72.32%\n",
      "Epoch    730 Step:      0 Loss:   0.5297 Training Acc:    72.31    72.31    72.31%\n",
      "Epoch    731 Step:      0 Loss:   0.5296 Training Acc:    72.33    72.33    72.33%\n",
      "Epoch    732 Step:      0 Loss:   0.5296 Training Acc:    72.30    72.30    72.30%\n",
      "Epoch    733 Step:      0 Loss:   0.5295 Training Acc:    72.37    72.37    72.37%\n",
      "Epoch    734 Step:      0 Loss:   0.5295 Training Acc:    72.32    72.32    72.32%\n",
      "Epoch    735 Step:      0 Loss:   0.5294 Training Acc:    72.37    72.37    72.37%\n",
      "Epoch    736 Step:      0 Loss:   0.5292 Training Acc:    72.32    72.32    72.32%\n",
      "Epoch    737 Step:      0 Loss:   0.5287 Training Acc:    72.43    72.43    72.43%\n",
      "Epoch    738 Step:      0 Loss:   0.5281 Training Acc:    72.44    72.44    72.44%\n",
      "Epoch    739 Step:      0 Loss:   0.5277 Training Acc:    72.47    72.47    72.47%\n",
      "Epoch    740 Step:      0 Loss:   0.5277 Training Acc:    72.49    72.49    72.49%\n",
      "Epoch    741 Step:      0 Loss:   0.5278 Training Acc:    72.48    72.48    72.48%\n",
      "Epoch    742 Step:      0 Loss:   0.5277 Training Acc:    72.50    72.50    72.50%\n",
      "Epoch    743 Step:      0 Loss:   0.5275 Training Acc:    72.49    72.49    72.49%\n",
      "Epoch    744 Step:      0 Loss:   0.5270 Training Acc:    72.55    72.55    72.55%\n",
      "Epoch    745 Step:      0 Loss:   0.5266 Training Acc:    72.55    72.55    72.55%\n",
      "Epoch    746 Step:      0 Loss:   0.5264 Training Acc:    72.57    72.57    72.57%\n",
      "Epoch    747 Step:      0 Loss:   0.5263 Training Acc:    72.60    72.60    72.60%\n",
      "Epoch    748 Step:      0 Loss:   0.5262 Training Acc:    72.58    72.58    72.58%\n",
      "Epoch    749 Step:      0 Loss:   0.5260 Training Acc:    72.62    72.62    72.62%\n",
      "Epoch    750 Step:      0 Loss:   0.5257 Training Acc:    72.62    72.62    72.62%\n",
      "Epoch    751 Step:      0 Loss:   0.5254 Training Acc:    72.66    72.66    72.66%\n",
      "Epoch    752 Step:      0 Loss:   0.5252 Training Acc:    72.66    72.66    72.66%\n",
      "Epoch    753 Step:      0 Loss:   0.5250 Training Acc:    72.71    72.71    72.71%\n",
      "Epoch    754 Step:      0 Loss:   0.5249 Training Acc:    72.72    72.72    72.72%\n",
      "Epoch    755 Step:      0 Loss:   0.5247 Training Acc:    72.73    72.73    72.73%\n",
      "Epoch    756 Step:      0 Loss:   0.5244 Training Acc:    72.76    72.76    72.76%\n",
      "Epoch    757 Step:      0 Loss:   0.5241 Training Acc:    72.79    72.79    72.79%\n",
      "Epoch    758 Step:      0 Loss:   0.5238 Training Acc:    72.80    72.80    72.80%\n",
      "Epoch    759 Step:      0 Loss:   0.5236 Training Acc:    72.82    72.82    72.82%\n",
      "Epoch    760 Step:      0 Loss:   0.5234 Training Acc:    72.83    72.83    72.83%\n",
      "Epoch    761 Step:      0 Loss:   0.5232 Training Acc:    72.87    72.87    72.87%\n",
      "Epoch    762 Step:      0 Loss:   0.5231 Training Acc:    72.87    72.87    72.87%\n",
      "Epoch    763 Step:      0 Loss:   0.5228 Training Acc:    72.91    72.91    72.91%\n",
      "Epoch    764 Step:      0 Loss:   0.5225 Training Acc:    72.91    72.91    72.91%\n",
      "Epoch    765 Step:      0 Loss:   0.5222 Training Acc:    72.94    72.94    72.94%\n",
      "Epoch    766 Step:      0 Loss:   0.5220 Training Acc:    72.98    72.98    72.98%\n",
      "Epoch    767 Step:      0 Loss:   0.5218 Training Acc:    72.97    72.97    72.97%\n",
      "Epoch    768 Step:      0 Loss:   0.5217 Training Acc:    73.03    73.03    73.03%\n",
      "Epoch    769 Step:      0 Loss:   0.5216 Training Acc:    72.96    72.96    72.96%\n",
      "Epoch    770 Step:      0 Loss:   0.5217 Training Acc:    73.06    73.06    73.06%\n",
      "Epoch    771 Step:      0 Loss:   0.5221 Training Acc:    72.87    72.87    72.87%\n",
      "Epoch    772 Step:      0 Loss:   0.5229 Training Acc:    73.03    73.03    73.03%\n",
      "Epoch    773 Step:      0 Loss:   0.5237 Training Acc:    72.69    72.69    72.69%\n",
      "Epoch    774 Step:      0 Loss:   0.5233 Training Acc:    72.99    72.99    72.99%\n",
      "Epoch    775 Step:      0 Loss:   0.5213 Training Acc:    72.92    72.92    72.92%\n",
      "Epoch    776 Step:      0 Loss:   0.5200 Training Acc:    73.14    73.14    73.14%\n",
      "Epoch    777 Step:      0 Loss:   0.5208 Training Acc:    73.17    73.17    73.17%\n",
      "Epoch    778 Step:      0 Loss:   0.5215 Training Acc:    72.86    72.86    72.86%\n",
      "Epoch    779 Step:      0 Loss:   0.5203 Training Acc:    73.19    73.19    73.19%\n",
      "Epoch    780 Step:      0 Loss:   0.5190 Training Acc:    73.22    73.22    73.22%\n",
      "Epoch    781 Step:      0 Loss:   0.5195 Training Acc:    73.10    73.10    73.10%\n",
      "Epoch    782 Step:      0 Loss:   0.5200 Training Acc:    73.25    73.25    73.25%\n",
      "Epoch    783 Step:      0 Loss:   0.5191 Training Acc:    73.13    73.13    73.13%\n",
      "Epoch    784 Step:      0 Loss:   0.5181 Training Acc:    73.31    73.31    73.31%\n",
      "Epoch    785 Step:      0 Loss:   0.5184 Training Acc:    73.37    73.37    73.37%\n",
      "Epoch    786 Step:      0 Loss:   0.5187 Training Acc:    73.15    73.15    73.15%\n",
      "Epoch    787 Step:      0 Loss:   0.5179 Training Acc:    73.39    73.39    73.39%\n",
      "Epoch    788 Step:      0 Loss:   0.5173 Training Acc:    73.37    73.37    73.37%\n",
      "Epoch    789 Step:      0 Loss:   0.5174 Training Acc:    73.28    73.28    73.28%\n",
      "Epoch    790 Step:      0 Loss:   0.5175 Training Acc:    73.45    73.45    73.45%\n",
      "Epoch    791 Step:      0 Loss:   0.5168 Training Acc:    73.33    73.33    73.33%\n",
      "Epoch    792 Step:      0 Loss:   0.5164 Training Acc:    73.41    73.41    73.41%\n",
      "Epoch    793 Step:      0 Loss:   0.5165 Training Acc:    73.51    73.51    73.51%\n",
      "Epoch    794 Step:      0 Loss:   0.5164 Training Acc:    73.38    73.38    73.38%\n",
      "Epoch    795 Step:      0 Loss:   0.5159 Training Acc:    73.53    73.53    73.53%\n",
      "Epoch    796 Step:      0 Loss:   0.5156 Training Acc:    73.51    73.51    73.51%\n",
      "Epoch    797 Step:      0 Loss:   0.5155 Training Acc:    73.44    73.44    73.44%\n",
      "Epoch    798 Step:      0 Loss:   0.5153 Training Acc:    73.60    73.60    73.60%\n",
      "Epoch    799 Step:      0 Loss:   0.5150 Training Acc:    73.51    73.51    73.51%\n",
      "Epoch    800 Step:      0 Loss:   0.5147 Training Acc:    73.58    73.58    73.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    801 Step:      0 Loss:   0.5146 Training Acc:    73.60    73.60    73.60%\n",
      "Epoch    802 Step:      0 Loss:   0.5144 Training Acc:    73.57    73.57    73.57%\n",
      "Epoch    803 Step:      0 Loss:   0.5141 Training Acc:    73.65    73.65    73.65%\n",
      "Epoch    804 Step:      0 Loss:   0.5138 Training Acc:    73.61    73.61    73.61%\n",
      "Epoch    805 Step:      0 Loss:   0.5136 Training Acc:    73.65    73.65    73.65%\n",
      "Epoch    806 Step:      0 Loss:   0.5135 Training Acc:    73.68    73.68    73.68%\n",
      "Epoch    807 Step:      0 Loss:   0.5132 Training Acc:    73.67    73.67    73.67%\n",
      "Epoch    808 Step:      0 Loss:   0.5129 Training Acc:    73.72    73.72    73.72%\n",
      "Epoch    809 Step:      0 Loss:   0.5127 Training Acc:    73.74    73.74    73.74%\n",
      "Epoch    810 Step:      0 Loss:   0.5125 Training Acc:    73.74    73.74    73.74%\n",
      "Epoch    811 Step:      0 Loss:   0.5123 Training Acc:    73.78    73.78    73.78%\n",
      "Epoch    812 Step:      0 Loss:   0.5120 Training Acc:    73.78    73.78    73.78%\n",
      "Epoch    813 Step:      0 Loss:   0.5118 Training Acc:    73.81    73.81    73.81%\n",
      "Epoch    814 Step:      0 Loss:   0.5116 Training Acc:    73.85    73.85    73.85%\n",
      "Epoch    815 Step:      0 Loss:   0.5114 Training Acc:    73.82    73.82    73.82%\n",
      "Epoch    816 Step:      0 Loss:   0.5112 Training Acc:    73.87    73.87    73.87%\n",
      "Epoch    817 Step:      0 Loss:   0.5109 Training Acc:    73.86    73.86    73.86%\n",
      "Epoch    818 Step:      0 Loss:   0.5107 Training Acc:    73.91    73.91    73.91%\n",
      "Epoch    819 Step:      0 Loss:   0.5104 Training Acc:    73.92    73.92    73.92%\n",
      "Epoch    820 Step:      0 Loss:   0.5102 Training Acc:    73.93    73.93    73.93%\n",
      "Epoch    821 Step:      0 Loss:   0.5100 Training Acc:    73.96    73.96    73.96%\n",
      "Epoch    822 Step:      0 Loss:   0.5098 Training Acc:    73.97    73.97    73.97%\n",
      "Epoch    823 Step:      0 Loss:   0.5095 Training Acc:    73.99    73.99    73.99%\n",
      "Epoch    824 Step:      0 Loss:   0.5093 Training Acc:    74.01    74.01    74.01%\n",
      "Epoch    825 Step:      0 Loss:   0.5091 Training Acc:    74.03    74.03    74.03%\n",
      "Epoch    826 Step:      0 Loss:   0.5089 Training Acc:    74.05    74.05    74.05%\n",
      "Epoch    827 Step:      0 Loss:   0.5086 Training Acc:    74.06    74.06    74.06%\n",
      "Epoch    828 Step:      0 Loss:   0.5084 Training Acc:    74.10    74.10    74.10%\n",
      "Epoch    829 Step:      0 Loss:   0.5082 Training Acc:    74.11    74.11    74.11%\n",
      "Epoch    830 Step:      0 Loss:   0.5080 Training Acc:    74.13    74.13    74.13%\n",
      "Epoch    831 Step:      0 Loss:   0.5077 Training Acc:    74.16    74.16    74.16%\n",
      "Epoch    832 Step:      0 Loss:   0.5075 Training Acc:    74.16    74.16    74.16%\n",
      "Epoch    833 Step:      0 Loss:   0.5073 Training Acc:    74.18    74.18    74.18%\n",
      "Epoch    834 Step:      0 Loss:   0.5070 Training Acc:    74.18    74.18    74.18%\n",
      "Epoch    835 Step:      0 Loss:   0.5068 Training Acc:    74.23    74.23    74.23%\n",
      "Epoch    836 Step:      0 Loss:   0.5067 Training Acc:    74.19    74.19    74.19%\n",
      "Epoch    837 Step:      0 Loss:   0.5065 Training Acc:    74.27    74.27    74.27%\n",
      "Epoch    838 Step:      0 Loss:   0.5064 Training Acc:    74.21    74.21    74.21%\n",
      "Epoch    839 Step:      0 Loss:   0.5065 Training Acc:    74.27    74.27    74.27%\n",
      "Epoch    840 Step:      0 Loss:   0.5069 Training Acc:    74.12    74.12    74.12%\n",
      "Epoch    841 Step:      0 Loss:   0.5075 Training Acc:    74.19    74.19    74.19%\n",
      "Epoch    842 Step:      0 Loss:   0.5082 Training Acc:    74.00    74.00    74.00%\n",
      "Epoch    843 Step:      0 Loss:   0.5077 Training Acc:    74.21    74.21    74.21%\n",
      "Epoch    844 Step:      0 Loss:   0.5060 Training Acc:    74.17    74.17    74.17%\n",
      "Epoch    845 Step:      0 Loss:   0.5047 Training Acc:    74.37    74.37    74.37%\n",
      "Epoch    846 Step:      0 Loss:   0.5049 Training Acc:    74.35    74.35    74.35%\n",
      "Epoch    847 Step:      0 Loss:   0.5058 Training Acc:    74.17    74.17    74.17%\n",
      "Epoch    848 Step:      0 Loss:   0.5055 Training Acc:    74.33    74.33    74.33%\n",
      "Epoch    849 Step:      0 Loss:   0.5042 Training Acc:    74.35    74.35    74.35%\n",
      "Epoch    850 Step:      0 Loss:   0.5036 Training Acc:    74.43    74.43    74.43%\n",
      "Epoch    851 Step:      0 Loss:   0.5041 Training Acc:    74.41    74.41    74.41%\n",
      "Epoch    852 Step:      0 Loss:   0.5043 Training Acc:    74.32    74.32    74.32%\n",
      "Epoch    853 Step:      0 Loss:   0.5036 Training Acc:    74.43    74.43    74.43%\n",
      "Epoch    854 Step:      0 Loss:   0.5027 Training Acc:    74.49    74.49    74.49%\n",
      "Epoch    855 Step:      0 Loss:   0.5027 Training Acc:    74.46    74.46    74.46%\n",
      "Epoch    856 Step:      0 Loss:   0.5030 Training Acc:    74.49    74.49    74.49%\n",
      "Epoch    857 Step:      0 Loss:   0.5027 Training Acc:    74.44    74.44    74.44%\n",
      "Epoch    858 Step:      0 Loss:   0.5020 Training Acc:    74.56    74.56    74.56%\n",
      "Epoch    859 Step:      0 Loss:   0.5017 Training Acc:    74.60    74.60    74.60%\n",
      "Epoch    860 Step:      0 Loss:   0.5018 Training Acc:    74.52    74.52    74.52%\n",
      "Epoch    861 Step:      0 Loss:   0.5017 Training Acc:    74.62    74.62    74.62%\n",
      "Epoch    862 Step:      0 Loss:   0.5013 Training Acc:    74.56    74.56    74.56%\n",
      "Epoch    863 Step:      0 Loss:   0.5008 Training Acc:    74.63    74.63    74.63%\n",
      "Epoch    864 Step:      0 Loss:   0.5007 Training Acc:    74.65    74.65    74.65%\n",
      "Epoch    865 Step:      0 Loss:   0.5007 Training Acc:    74.60    74.60    74.60%\n",
      "Epoch    866 Step:      0 Loss:   0.5004 Training Acc:    74.67    74.67    74.67%\n",
      "Epoch    867 Step:      0 Loss:   0.5000 Training Acc:    74.67    74.67    74.67%\n",
      "Epoch    868 Step:      0 Loss:   0.4997 Training Acc:    74.68    74.68    74.68%\n",
      "Epoch    869 Step:      0 Loss:   0.4997 Training Acc:    74.71    74.71    74.71%\n",
      "Epoch    870 Step:      0 Loss:   0.4996 Training Acc:    74.67    74.67    74.67%\n",
      "Epoch    871 Step:      0 Loss:   0.4993 Training Acc:    74.75    74.75    74.75%\n",
      "Epoch    872 Step:      0 Loss:   0.4989 Training Acc:    74.74    74.74    74.74%\n",
      "Epoch    873 Step:      0 Loss:   0.4987 Training Acc:    74.77    74.77    74.77%\n",
      "Epoch    874 Step:      0 Loss:   0.4986 Training Acc:    74.79    74.79    74.79%\n",
      "Epoch    875 Step:      0 Loss:   0.4984 Training Acc:    74.77    74.77    74.77%\n",
      "Epoch    876 Step:      0 Loss:   0.4982 Training Acc:    74.80    74.80    74.80%\n",
      "Epoch    877 Step:      0 Loss:   0.4979 Training Acc:    74.84    74.84    74.84%\n",
      "Epoch    878 Step:      0 Loss:   0.4977 Training Acc:    74.81    74.81    74.81%\n",
      "Epoch    879 Step:      0 Loss:   0.4976 Training Acc:    74.91    74.91    74.91%\n",
      "Epoch    880 Step:      0 Loss:   0.4976 Training Acc:    74.80    74.80    74.80%\n",
      "Epoch    881 Step:      0 Loss:   0.4976 Training Acc:    74.92    74.92    74.92%\n",
      "Epoch    882 Step:      0 Loss:   0.4979 Training Acc:    74.70    74.70    74.70%\n",
      "Epoch    883 Step:      0 Loss:   0.4986 Training Acc:    74.85    74.85    74.85%\n",
      "Epoch    884 Step:      0 Loss:   0.4997 Training Acc:    74.53    74.53    74.53%\n",
      "Epoch    885 Step:      0 Loss:   0.4998 Training Acc:    74.77    74.77    74.77%\n",
      "Epoch    886 Step:      0 Loss:   0.4979 Training Acc:    74.70    74.70    74.70%\n",
      "Epoch    887 Step:      0 Loss:   0.4962 Training Acc:    74.99    74.99    74.99%\n",
      "Epoch    888 Step:      0 Loss:   0.4964 Training Acc:    74.94    74.94    74.94%\n",
      "Epoch    889 Step:      0 Loss:   0.4971 Training Acc:    74.75    74.75    74.75%\n",
      "Epoch    890 Step:      0 Loss:   0.4966 Training Acc:    74.98    74.98    74.98%\n",
      "Epoch    891 Step:      0 Loss:   0.4958 Training Acc:    74.89    74.89    74.89%\n",
      "Epoch    892 Step:      0 Loss:   0.4956 Training Acc:    75.00    75.00    75.00%\n",
      "Epoch    893 Step:      0 Loss:   0.4956 Training Acc:    75.02    75.02    75.02%\n",
      "Epoch    894 Step:      0 Loss:   0.4951 Training Acc:    74.95    74.95    74.95%\n",
      "Epoch    895 Step:      0 Loss:   0.4948 Training Acc:    75.10    75.10    75.10%\n",
      "Epoch    896 Step:      0 Loss:   0.4949 Training Acc:    74.99    74.99    74.99%\n",
      "Epoch    897 Step:      0 Loss:   0.4946 Training Acc:    75.08    75.08    75.08%\n",
      "Epoch    898 Step:      0 Loss:   0.4939 Training Acc:    75.14    75.14    75.14%\n",
      "Epoch    899 Step:      0 Loss:   0.4937 Training Acc:    75.07    75.07    75.07%\n",
      "Epoch    900 Step:      0 Loss:   0.4940 Training Acc:    75.14    75.14    75.14%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    901 Step:      0 Loss:   0.4937 Training Acc:    75.08    75.08    75.08%\n",
      "Epoch    902 Step:      0 Loss:   0.4930 Training Acc:    75.17    75.17    75.17%\n",
      "Epoch    903 Step:      0 Loss:   0.4928 Training Acc:    75.17    75.17    75.17%\n",
      "Epoch    904 Step:      0 Loss:   0.4930 Training Acc:    75.15    75.15    75.15%\n",
      "Epoch    905 Step:      0 Loss:   0.4928 Training Acc:    75.21    75.21    75.21%\n",
      "Epoch    906 Step:      0 Loss:   0.4923 Training Acc:    75.19    75.19    75.19%\n",
      "Epoch    907 Step:      0 Loss:   0.4921 Training Acc:    75.26    75.26    75.26%\n",
      "Epoch    908 Step:      0 Loss:   0.4920 Training Acc:    75.21    75.21    75.21%\n",
      "Epoch    909 Step:      0 Loss:   0.4918 Training Acc:    75.25    75.25    75.25%\n",
      "Epoch    910 Step:      0 Loss:   0.4915 Training Acc:    75.29    75.29    75.29%\n",
      "Epoch    911 Step:      0 Loss:   0.4913 Training Acc:    75.21    75.21    75.21%\n",
      "Epoch    912 Step:      0 Loss:   0.4912 Training Acc:    75.32    75.32    75.32%\n",
      "Epoch    913 Step:      0 Loss:   0.4910 Training Acc:    75.27    75.27    75.27%\n",
      "Epoch    914 Step:      0 Loss:   0.4907 Training Acc:    75.33    75.33    75.33%\n",
      "Epoch    915 Step:      0 Loss:   0.4905 Training Acc:    75.36    75.36    75.36%\n",
      "Epoch    916 Step:      0 Loss:   0.4904 Training Acc:    75.29    75.29    75.29%\n",
      "Epoch    917 Step:      0 Loss:   0.4902 Training Acc:    75.38    75.38    75.38%\n",
      "Epoch    918 Step:      0 Loss:   0.4899 Training Acc:    75.33    75.33    75.33%\n",
      "Epoch    919 Step:      0 Loss:   0.4897 Training Acc:    75.42    75.42    75.42%\n",
      "Epoch    920 Step:      0 Loss:   0.4895 Training Acc:    75.40    75.40    75.40%\n",
      "Epoch    921 Step:      0 Loss:   0.4894 Training Acc:    75.40    75.40    75.40%\n",
      "Epoch    922 Step:      0 Loss:   0.4892 Training Acc:    75.45    75.45    75.45%\n",
      "Epoch    923 Step:      0 Loss:   0.4889 Training Acc:    75.41    75.41    75.41%\n",
      "Epoch    924 Step:      0 Loss:   0.4887 Training Acc:    75.49    75.49    75.49%\n",
      "Epoch    925 Step:      0 Loss:   0.4886 Training Acc:    75.45    75.45    75.45%\n",
      "Epoch    926 Step:      0 Loss:   0.4884 Training Acc:    75.48    75.48    75.48%\n",
      "Epoch    927 Step:      0 Loss:   0.4881 Training Acc:    75.49    75.49    75.49%\n",
      "Epoch    928 Step:      0 Loss:   0.4879 Training Acc:    75.51    75.51    75.51%\n",
      "Epoch    929 Step:      0 Loss:   0.4877 Training Acc:    75.53    75.53    75.53%\n",
      "Epoch    930 Step:      0 Loss:   0.4876 Training Acc:    75.53    75.53    75.53%\n",
      "Epoch    931 Step:      0 Loss:   0.4874 Training Acc:    75.55    75.55    75.55%\n",
      "Epoch    932 Step:      0 Loss:   0.4872 Training Acc:    75.54    75.54    75.54%\n",
      "Epoch    933 Step:      0 Loss:   0.4869 Training Acc:    75.59    75.59    75.59%\n",
      "Epoch    934 Step:      0 Loss:   0.4867 Training Acc:    75.59    75.59    75.59%\n",
      "Epoch    935 Step:      0 Loss:   0.4865 Training Acc:    75.63    75.63    75.63%\n",
      "Epoch    936 Step:      0 Loss:   0.4864 Training Acc:    75.62    75.62    75.62%\n",
      "Epoch    937 Step:      0 Loss:   0.4862 Training Acc:    75.66    75.66    75.66%\n",
      "Epoch    938 Step:      0 Loss:   0.4859 Training Acc:    75.66    75.66    75.66%\n",
      "Epoch    939 Step:      0 Loss:   0.4857 Training Acc:    75.68    75.68    75.68%\n",
      "Epoch    940 Step:      0 Loss:   0.4855 Training Acc:    75.68    75.68    75.68%\n",
      "Epoch    941 Step:      0 Loss:   0.4853 Training Acc:    75.68    75.68    75.68%\n",
      "Epoch    942 Step:      0 Loss:   0.4851 Training Acc:    75.71    75.71    75.71%\n",
      "Epoch    943 Step:      0 Loss:   0.4849 Training Acc:    75.71    75.71    75.71%\n",
      "Epoch    944 Step:      0 Loss:   0.4847 Training Acc:    75.73    75.73    75.73%\n",
      "Epoch    945 Step:      0 Loss:   0.4845 Training Acc:    75.73    75.73    75.73%\n",
      "Epoch    946 Step:      0 Loss:   0.4843 Training Acc:    75.75    75.75    75.75%\n",
      "Epoch    947 Step:      0 Loss:   0.4841 Training Acc:    75.75    75.75    75.75%\n",
      "Epoch    948 Step:      0 Loss:   0.4839 Training Acc:    75.77    75.77    75.77%\n",
      "Epoch    949 Step:      0 Loss:   0.4837 Training Acc:    75.78    75.78    75.78%\n",
      "Epoch    950 Step:      0 Loss:   0.4835 Training Acc:    75.80    75.80    75.80%\n",
      "Epoch    951 Step:      0 Loss:   0.4834 Training Acc:    75.79    75.79    75.79%\n",
      "Epoch    952 Step:      0 Loss:   0.4833 Training Acc:    75.84    75.84    75.84%\n",
      "Epoch    953 Step:      0 Loss:   0.4833 Training Acc:    75.77    75.77    75.77%\n",
      "Epoch    954 Step:      0 Loss:   0.4835 Training Acc:    75.83    75.83    75.83%\n",
      "Epoch    955 Step:      0 Loss:   0.4840 Training Acc:    75.66    75.66    75.66%\n",
      "Epoch    956 Step:      0 Loss:   0.4851 Training Acc:    75.75    75.75    75.75%\n",
      "Epoch    957 Step:      0 Loss:   0.4863 Training Acc:    75.46    75.46    75.46%\n",
      "Epoch    958 Step:      0 Loss:   0.4864 Training Acc:    75.67    75.67    75.67%\n",
      "Epoch    959 Step:      0 Loss:   0.4847 Training Acc:    75.59    75.59    75.59%\n",
      "Epoch    960 Step:      0 Loss:   0.4823 Training Acc:    75.92    75.92    75.92%\n",
      "Epoch    961 Step:      0 Loss:   0.4817 Training Acc:    75.93    75.93    75.93%\n",
      "Epoch    962 Step:      0 Loss:   0.4828 Training Acc:    75.73    75.73    75.73%\n",
      "Epoch    963 Step:      0 Loss:   0.4833 Training Acc:    75.87    75.87    75.87%\n",
      "Epoch    964 Step:      0 Loss:   0.4822 Training Acc:    75.77    75.77    75.77%\n",
      "Epoch    965 Step:      0 Loss:   0.4809 Training Acc:    76.00    76.00    76.00%\n",
      "Epoch    966 Step:      0 Loss:   0.4810 Training Acc:    75.97    75.97    75.97%\n",
      "Epoch    967 Step:      0 Loss:   0.4816 Training Acc:    75.81    75.81    75.81%\n",
      "Epoch    968 Step:      0 Loss:   0.4812 Training Acc:    75.99    75.99    75.99%\n",
      "Epoch    969 Step:      0 Loss:   0.4802 Training Acc:    75.97    75.97    75.97%\n",
      "Epoch    970 Step:      0 Loss:   0.4799 Training Acc:    76.05    76.05    76.05%\n",
      "Epoch    971 Step:      0 Loss:   0.4804 Training Acc:    76.01    76.01    76.01%\n",
      "Epoch    972 Step:      0 Loss:   0.4803 Training Acc:    75.94    75.94    75.94%\n",
      "Epoch    973 Step:      0 Loss:   0.4795 Training Acc:    76.08    76.08    76.08%\n",
      "Epoch    974 Step:      0 Loss:   0.4790 Training Acc:    76.10    76.10    76.10%\n",
      "Epoch    975 Step:      0 Loss:   0.4791 Training Acc:    76.08    76.08    76.08%\n",
      "Epoch    976 Step:      0 Loss:   0.4793 Training Acc:    76.09    76.09    76.09%\n",
      "Epoch    977 Step:      0 Loss:   0.4789 Training Acc:    76.06    76.06    76.06%\n",
      "Epoch    978 Step:      0 Loss:   0.4783 Training Acc:    76.17    76.17    76.17%\n",
      "Epoch    979 Step:      0 Loss:   0.4781 Training Acc:    76.20    76.20    76.20%\n",
      "Epoch    980 Step:      0 Loss:   0.4782 Training Acc:    76.10    76.10    76.10%\n",
      "Epoch    981 Step:      0 Loss:   0.4780 Training Acc:    76.17    76.17    76.17%\n",
      "Epoch    982 Step:      0 Loss:   0.4776 Training Acc:    76.19    76.19    76.19%\n",
      "Epoch    983 Step:      0 Loss:   0.4773 Training Acc:    76.23    76.23    76.23%\n",
      "Epoch    984 Step:      0 Loss:   0.4772 Training Acc:    76.24    76.24    76.24%\n",
      "Epoch    985 Step:      0 Loss:   0.4772 Training Acc:    76.18    76.18    76.18%\n",
      "Epoch    986 Step:      0 Loss:   0.4769 Training Acc:    76.26    76.26    76.26%\n",
      "Epoch    987 Step:      0 Loss:   0.4766 Training Acc:    76.25    76.25    76.25%\n",
      "Epoch    988 Step:      0 Loss:   0.4763 Training Acc:    76.28    76.28    76.28%\n",
      "Epoch    989 Step:      0 Loss:   0.4763 Training Acc:    76.31    76.31    76.31%\n",
      "Epoch    990 Step:      0 Loss:   0.4761 Training Acc:    76.25    76.25    76.25%\n",
      "Epoch    991 Step:      0 Loss:   0.4759 Training Acc:    76.34    76.34    76.34%\n",
      "Epoch    992 Step:      0 Loss:   0.4756 Training Acc:    76.30    76.30    76.30%\n",
      "Epoch    993 Step:      0 Loss:   0.4754 Training Acc:    76.35    76.35    76.35%\n",
      "Epoch    994 Step:      0 Loss:   0.4753 Training Acc:    76.37    76.37    76.37%\n",
      "Epoch    995 Step:      0 Loss:   0.4751 Training Acc:    76.32    76.32    76.32%\n",
      "Epoch    996 Step:      0 Loss:   0.4749 Training Acc:    76.40    76.40    76.40%\n",
      "Epoch    997 Step:      0 Loss:   0.4747 Training Acc:    76.36    76.36    76.36%\n",
      "Epoch    998 Step:      0 Loss:   0.4745 Training Acc:    76.41    76.41    76.41%\n",
      "Epoch    999 Step:      0 Loss:   0.4743 Training Acc:    76.43    76.43    76.43%\n",
      "Epoch   1000 Step:      0 Loss:   0.4741 Training Acc:    76.41    76.41    76.41%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1001 Step:      0 Loss:   0.4740 Training Acc:    76.45    76.45    76.45%\n",
      "Epoch   1002 Step:      0 Loss:   0.4738 Training Acc:    76.44    76.44    76.44%\n",
      "Epoch   1003 Step:      0 Loss:   0.4736 Training Acc:    76.47    76.47    76.47%\n",
      "Epoch   1004 Step:      0 Loss:   0.4733 Training Acc:    76.48    76.48    76.48%\n",
      "Epoch   1005 Step:      0 Loss:   0.4732 Training Acc:    76.49    76.49    76.49%\n",
      "Epoch   1006 Step:      0 Loss:   0.4730 Training Acc:    76.51    76.51    76.51%\n",
      "Epoch   1007 Step:      0 Loss:   0.4728 Training Acc:    76.50    76.50    76.50%\n",
      "Epoch   1008 Step:      0 Loss:   0.4726 Training Acc:    76.53    76.53    76.53%\n",
      "Epoch   1009 Step:      0 Loss:   0.4724 Training Acc:    76.53    76.53    76.53%\n",
      "Epoch   1010 Step:      0 Loss:   0.4722 Training Acc:    76.54    76.54    76.54%\n",
      "Epoch   1011 Step:      0 Loss:   0.4720 Training Acc:    76.57    76.57    76.57%\n",
      "Epoch   1012 Step:      0 Loss:   0.4718 Training Acc:    76.58    76.58    76.58%\n",
      "Epoch   1013 Step:      0 Loss:   0.4717 Training Acc:    76.59    76.59    76.59%\n",
      "Epoch   1014 Step:      0 Loss:   0.4715 Training Acc:    76.59    76.59    76.59%\n",
      "Epoch   1015 Step:      0 Loss:   0.4713 Training Acc:    76.62    76.62    76.62%\n",
      "Epoch   1016 Step:      0 Loss:   0.4711 Training Acc:    76.61    76.61    76.61%\n",
      "Epoch   1017 Step:      0 Loss:   0.4709 Training Acc:    76.65    76.65    76.65%\n",
      "Epoch   1018 Step:      0 Loss:   0.4707 Training Acc:    76.62    76.62    76.62%\n",
      "Epoch   1019 Step:      0 Loss:   0.4705 Training Acc:    76.67    76.67    76.67%\n",
      "Epoch   1020 Step:      0 Loss:   0.4703 Training Acc:    76.64    76.64    76.64%\n",
      "Epoch   1021 Step:      0 Loss:   0.4702 Training Acc:    76.68    76.68    76.68%\n",
      "Epoch   1022 Step:      0 Loss:   0.4700 Training Acc:    76.67    76.67    76.67%\n",
      "Epoch   1023 Step:      0 Loss:   0.4698 Training Acc:    76.70    76.70    76.70%\n",
      "Epoch   1024 Step:      0 Loss:   0.4697 Training Acc:    76.69    76.69    76.69%\n",
      "Epoch   1025 Step:      0 Loss:   0.4696 Training Acc:    76.70    76.70    76.70%\n",
      "Epoch   1026 Step:      0 Loss:   0.4696 Training Acc:    76.71    76.71    76.71%\n",
      "Epoch   1027 Step:      0 Loss:   0.4697 Training Acc:    76.71    76.71    76.71%\n",
      "Epoch   1028 Step:      0 Loss:   0.4701 Training Acc:    76.67    76.67    76.67%\n",
      "Epoch   1029 Step:      0 Loss:   0.4707 Training Acc:    76.65    76.65    76.65%\n",
      "Epoch   1030 Step:      0 Loss:   0.4717 Training Acc:    76.54    76.54    76.54%\n",
      "Epoch   1031 Step:      0 Loss:   0.4718 Training Acc:    76.57    76.57    76.57%\n",
      "Epoch   1032 Step:      0 Loss:   0.4709 Training Acc:    76.59    76.59    76.59%\n",
      "Epoch   1033 Step:      0 Loss:   0.4688 Training Acc:    76.78    76.78    76.78%\n",
      "Epoch   1034 Step:      0 Loss:   0.4678 Training Acc:    76.81    76.81    76.81%\n",
      "Epoch   1035 Step:      0 Loss:   0.4683 Training Acc:    76.79    76.79    76.79%\n",
      "Epoch   1036 Step:      0 Loss:   0.4692 Training Acc:    76.75    76.75    76.75%\n",
      "Epoch   1037 Step:      0 Loss:   0.4693 Training Acc:    76.72    76.72    76.72%\n",
      "Epoch   1038 Step:      0 Loss:   0.4680 Training Acc:    76.83    76.83    76.83%\n",
      "Epoch   1039 Step:      0 Loss:   0.4669 Training Acc:    76.89    76.89    76.89%\n",
      "Epoch   1040 Step:      0 Loss:   0.4671 Training Acc:    76.84    76.84    76.84%\n",
      "Epoch   1041 Step:      0 Loss:   0.4677 Training Acc:    76.83    76.83    76.83%\n",
      "Epoch   1042 Step:      0 Loss:   0.4677 Training Acc:    76.82    76.82    76.82%\n",
      "Epoch   1043 Step:      0 Loss:   0.4667 Training Acc:    76.91    76.91    76.91%\n",
      "Epoch   1044 Step:      0 Loss:   0.4660 Training Acc:    76.98    76.98    76.98%\n",
      "Epoch   1045 Step:      0 Loss:   0.4662 Training Acc:    76.92    76.92    76.92%\n",
      "Epoch   1046 Step:      0 Loss:   0.4665 Training Acc:    76.91    76.91    76.91%\n",
      "Epoch   1047 Step:      0 Loss:   0.4663 Training Acc:    76.91    76.91    76.91%\n",
      "Epoch   1048 Step:      0 Loss:   0.4656 Training Acc:    76.99    76.99    76.99%\n",
      "Epoch   1049 Step:      0 Loss:   0.4651 Training Acc:    77.02    77.02    77.02%\n",
      "Epoch   1050 Step:      0 Loss:   0.4652 Training Acc:    76.99    76.99    76.99%\n",
      "Epoch   1051 Step:      0 Loss:   0.4654 Training Acc:    76.98    76.98    76.98%\n",
      "Epoch   1052 Step:      0 Loss:   0.4651 Training Acc:    77.00    77.00    77.00%\n",
      "Epoch   1053 Step:      0 Loss:   0.4646 Training Acc:    77.07    77.07    77.07%\n",
      "Epoch   1054 Step:      0 Loss:   0.4643 Training Acc:    77.08    77.08    77.08%\n",
      "Epoch   1055 Step:      0 Loss:   0.4643 Training Acc:    77.05    77.05    77.05%\n",
      "Epoch   1056 Step:      0 Loss:   0.4643 Training Acc:    77.07    77.07    77.07%\n",
      "Epoch   1057 Step:      0 Loss:   0.4641 Training Acc:    77.06    77.06    77.06%\n",
      "Epoch   1058 Step:      0 Loss:   0.4637 Training Acc:    77.13    77.13    77.13%\n",
      "Epoch   1059 Step:      0 Loss:   0.4634 Training Acc:    77.14    77.14    77.14%\n",
      "Epoch   1060 Step:      0 Loss:   0.4633 Training Acc:    77.12    77.12    77.12%\n",
      "Epoch   1061 Step:      0 Loss:   0.4633 Training Acc:    77.16    77.16    77.16%\n",
      "Epoch   1062 Step:      0 Loss:   0.4631 Training Acc:    77.13    77.13    77.13%\n",
      "Epoch   1063 Step:      0 Loss:   0.4628 Training Acc:    77.19    77.19    77.19%\n",
      "Epoch   1064 Step:      0 Loss:   0.4625 Training Acc:    77.18    77.18    77.18%\n",
      "Epoch   1065 Step:      0 Loss:   0.4624 Training Acc:    77.17    77.17    77.17%\n",
      "Epoch   1066 Step:      0 Loss:   0.4623 Training Acc:    77.22    77.22    77.22%\n",
      "Epoch   1067 Step:      0 Loss:   0.4622 Training Acc:    77.19    77.19    77.19%\n",
      "Epoch   1068 Step:      0 Loss:   0.4619 Training Acc:    77.24    77.24    77.24%\n",
      "Epoch   1069 Step:      0 Loss:   0.4617 Training Acc:    77.20    77.20    77.20%\n",
      "Epoch   1070 Step:      0 Loss:   0.4616 Training Acc:    77.25    77.25    77.25%\n",
      "Epoch   1071 Step:      0 Loss:   0.4615 Training Acc:    77.24    77.24    77.24%\n",
      "Epoch   1072 Step:      0 Loss:   0.4616 Training Acc:    77.23    77.23    77.23%\n",
      "Epoch   1073 Step:      0 Loss:   0.4619 Training Acc:    77.18    77.18    77.18%\n",
      "Epoch   1074 Step:      0 Loss:   0.4625 Training Acc:    77.22    77.22    77.22%\n",
      "Epoch   1075 Step:      0 Loss:   0.4637 Training Acc:    76.99    76.99    76.99%\n",
      "Epoch   1076 Step:      0 Loss:   0.4652 Training Acc:    77.11    77.11    77.11%\n",
      "Epoch   1077 Step:      0 Loss:   0.4662 Training Acc:    76.75    76.75    76.75%\n",
      "Epoch   1078 Step:      0 Loss:   0.4649 Training Acc:    77.15    77.15    77.15%\n",
      "Epoch   1079 Step:      0 Loss:   0.4619 Training Acc:    77.09    77.09    77.09%\n",
      "Epoch   1080 Step:      0 Loss:   0.4601 Training Acc:    77.34    77.34    77.34%\n",
      "Epoch   1081 Step:      0 Loss:   0.4609 Training Acc:    77.32    77.32    77.32%\n",
      "Epoch   1082 Step:      0 Loss:   0.4624 Training Acc:    77.04    77.04    77.04%\n",
      "Epoch   1083 Step:      0 Loss:   0.4620 Training Acc:    77.32    77.32    77.32%\n",
      "Epoch   1084 Step:      0 Loss:   0.4601 Training Acc:    77.26    77.26    77.26%\n",
      "Epoch   1085 Step:      0 Loss:   0.4592 Training Acc:    77.40    77.40    77.40%\n",
      "Epoch   1086 Step:      0 Loss:   0.4600 Training Acc:    77.39    77.39    77.39%\n",
      "Epoch   1087 Step:      0 Loss:   0.4607 Training Acc:    77.17    77.17    77.17%\n",
      "Epoch   1088 Step:      0 Loss:   0.4598 Training Acc:    77.44    77.44    77.44%\n",
      "Epoch   1089 Step:      0 Loss:   0.4586 Training Acc:    77.40    77.40    77.40%\n",
      "Epoch   1090 Step:      0 Loss:   0.4586 Training Acc:    77.42    77.42    77.42%\n",
      "Epoch   1091 Step:      0 Loss:   0.4592 Training Acc:    77.45    77.45    77.45%\n",
      "Epoch   1092 Step:      0 Loss:   0.4591 Training Acc:    77.33    77.33    77.33%\n",
      "Epoch   1093 Step:      0 Loss:   0.4582 Training Acc:    77.52    77.52    77.52%\n",
      "Epoch   1094 Step:      0 Loss:   0.4576 Training Acc:    77.50    77.50    77.50%\n",
      "Epoch   1095 Step:      0 Loss:   0.4579 Training Acc:    77.43    77.43    77.43%\n",
      "Epoch   1096 Step:      0 Loss:   0.4581 Training Acc:    77.53    77.53    77.53%\n",
      "Epoch   1097 Step:      0 Loss:   0.4577 Training Acc:    77.44    77.44    77.44%\n",
      "Epoch   1098 Step:      0 Loss:   0.4571 Training Acc:    77.57    77.57    77.57%\n",
      "Epoch   1099 Step:      0 Loss:   0.4569 Training Acc:    77.56    77.56    77.56%\n",
      "Epoch   1100 Step:      0 Loss:   0.4571 Training Acc:    77.50    77.50    77.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1101 Step:      0 Loss:   0.4571 Training Acc:    77.60    77.60    77.60%\n",
      "Epoch   1102 Step:      0 Loss:   0.4566 Training Acc:    77.55    77.55    77.55%\n",
      "Epoch   1103 Step:      0 Loss:   0.4562 Training Acc:    77.64    77.64    77.64%\n",
      "Epoch   1104 Step:      0 Loss:   0.4561 Training Acc:    77.64    77.64    77.64%\n",
      "Epoch   1105 Step:      0 Loss:   0.4562 Training Acc:    77.58    77.58    77.58%\n",
      "Epoch   1106 Step:      0 Loss:   0.4561 Training Acc:    77.66    77.66    77.66%\n",
      "Epoch   1107 Step:      0 Loss:   0.4557 Training Acc:    77.63    77.63    77.63%\n",
      "Epoch   1108 Step:      0 Loss:   0.4554 Training Acc:    77.68    77.68    77.68%\n",
      "Epoch   1109 Step:      0 Loss:   0.4553 Training Acc:    77.69    77.69    77.69%\n",
      "Epoch   1110 Step:      0 Loss:   0.4553 Training Acc:    77.66    77.66    77.66%\n",
      "Epoch   1111 Step:      0 Loss:   0.4552 Training Acc:    77.70    77.70    77.70%\n",
      "Epoch   1112 Step:      0 Loss:   0.4549 Training Acc:    77.69    77.69    77.69%\n",
      "Epoch   1113 Step:      0 Loss:   0.4546 Training Acc:    77.72    77.72    77.72%\n",
      "Epoch   1114 Step:      0 Loss:   0.4545 Training Acc:    77.74    77.74    77.74%\n",
      "Epoch   1115 Step:      0 Loss:   0.4544 Training Acc:    77.72    77.72    77.72%\n",
      "Epoch   1116 Step:      0 Loss:   0.4543 Training Acc:    77.77    77.77    77.77%\n",
      "Epoch   1117 Step:      0 Loss:   0.4541 Training Acc:    77.76    77.76    77.76%\n",
      "Epoch   1118 Step:      0 Loss:   0.4539 Training Acc:    77.80    77.80    77.80%\n",
      "Epoch   1119 Step:      0 Loss:   0.4537 Training Acc:    77.80    77.80    77.80%\n",
      "Epoch   1120 Step:      0 Loss:   0.4536 Training Acc:    77.78    77.78    77.78%\n",
      "Epoch   1121 Step:      0 Loss:   0.4534 Training Acc:    77.83    77.83    77.83%\n",
      "Epoch   1122 Step:      0 Loss:   0.4533 Training Acc:    77.82    77.82    77.82%\n",
      "Epoch   1123 Step:      0 Loss:   0.4531 Training Acc:    77.84    77.84    77.84%\n",
      "Epoch   1124 Step:      0 Loss:   0.4529 Training Acc:    77.84    77.84    77.84%\n",
      "Epoch   1125 Step:      0 Loss:   0.4527 Training Acc:    77.85    77.85    77.85%\n",
      "Epoch   1126 Step:      0 Loss:   0.4526 Training Acc:    77.88    77.88    77.88%\n",
      "Epoch   1127 Step:      0 Loss:   0.4525 Training Acc:    77.87    77.87    77.87%\n",
      "Epoch   1128 Step:      0 Loss:   0.4523 Training Acc:    77.89    77.89    77.89%\n",
      "Epoch   1129 Step:      0 Loss:   0.4521 Training Acc:    77.90    77.90    77.90%\n",
      "Epoch   1130 Step:      0 Loss:   0.4520 Training Acc:    77.91    77.91    77.91%\n",
      "Epoch   1131 Step:      0 Loss:   0.4518 Training Acc:    77.93    77.93    77.93%\n",
      "Epoch   1132 Step:      0 Loss:   0.4517 Training Acc:    77.93    77.93    77.93%\n",
      "Epoch   1133 Step:      0 Loss:   0.4515 Training Acc:    77.95    77.95    77.95%\n",
      "Epoch   1134 Step:      0 Loss:   0.4513 Training Acc:    77.94    77.94    77.94%\n",
      "Epoch   1135 Step:      0 Loss:   0.4512 Training Acc:    77.97    77.97    77.97%\n",
      "Epoch   1136 Step:      0 Loss:   0.4510 Training Acc:    77.95    77.95    77.95%\n",
      "Epoch   1137 Step:      0 Loss:   0.4508 Training Acc:    77.99    77.99    77.99%\n",
      "Epoch   1138 Step:      0 Loss:   0.4507 Training Acc:    77.99    77.99    77.99%\n",
      "Epoch   1139 Step:      0 Loss:   0.4505 Training Acc:    78.01    78.01    78.01%\n",
      "Epoch   1140 Step:      0 Loss:   0.4504 Training Acc:    78.01    78.01    78.01%\n",
      "Epoch   1141 Step:      0 Loss:   0.4502 Training Acc:    78.02    78.02    78.02%\n",
      "Epoch   1142 Step:      0 Loss:   0.4501 Training Acc:    78.03    78.03    78.03%\n",
      "Epoch   1143 Step:      0 Loss:   0.4499 Training Acc:    78.05    78.05    78.05%\n",
      "Epoch   1144 Step:      0 Loss:   0.4498 Training Acc:    78.04    78.04    78.04%\n",
      "Epoch   1145 Step:      0 Loss:   0.4496 Training Acc:    78.05    78.05    78.05%\n",
      "Epoch   1146 Step:      0 Loss:   0.4494 Training Acc:    78.06    78.06    78.06%\n",
      "Epoch   1147 Step:      0 Loss:   0.4493 Training Acc:    78.07    78.07    78.07%\n",
      "Epoch   1148 Step:      0 Loss:   0.4491 Training Acc:    78.08    78.08    78.08%\n",
      "Epoch   1149 Step:      0 Loss:   0.4490 Training Acc:    78.09    78.09    78.09%\n",
      "Epoch   1150 Step:      0 Loss:   0.4488 Training Acc:    78.08    78.08    78.08%\n",
      "Epoch   1151 Step:      0 Loss:   0.4487 Training Acc:    78.10    78.10    78.10%\n",
      "Epoch   1152 Step:      0 Loss:   0.4486 Training Acc:    78.09    78.09    78.09%\n",
      "Epoch   1153 Step:      0 Loss:   0.4485 Training Acc:    78.11    78.11    78.11%\n",
      "Epoch   1154 Step:      0 Loss:   0.4484 Training Acc:    78.09    78.09    78.09%\n",
      "Epoch   1155 Step:      0 Loss:   0.4485 Training Acc:    78.12    78.12    78.12%\n",
      "Epoch   1156 Step:      0 Loss:   0.4486 Training Acc:    78.06    78.06    78.06%\n",
      "Epoch   1157 Step:      0 Loss:   0.4489 Training Acc:    78.08    78.08    78.08%\n",
      "Epoch   1158 Step:      0 Loss:   0.4493 Training Acc:    77.96    77.96    77.96%\n",
      "Epoch   1159 Step:      0 Loss:   0.4496 Training Acc:    78.03    78.03    78.03%\n",
      "Epoch   1160 Step:      0 Loss:   0.4497 Training Acc:    77.92    77.92    77.92%\n",
      "Epoch   1161 Step:      0 Loss:   0.4491 Training Acc:    78.06    78.06    78.06%\n",
      "Epoch   1162 Step:      0 Loss:   0.4481 Training Acc:    78.06    78.06    78.06%\n",
      "Epoch   1163 Step:      0 Loss:   0.4470 Training Acc:    78.20    78.20    78.20%\n",
      "Epoch   1164 Step:      0 Loss:   0.4466 Training Acc:    78.24    78.24    78.24%\n",
      "Epoch   1165 Step:      0 Loss:   0.4467 Training Acc:    78.21    78.21    78.21%\n",
      "Epoch   1166 Step:      0 Loss:   0.4471 Training Acc:    78.19    78.19    78.19%\n",
      "Epoch   1167 Step:      0 Loss:   0.4474 Training Acc:    78.11    78.11    78.11%\n",
      "Epoch   1168 Step:      0 Loss:   0.4471 Training Acc:    78.19    78.19    78.19%\n",
      "Epoch   1169 Step:      0 Loss:   0.4465 Training Acc:    78.18    78.18    78.18%\n",
      "Epoch   1170 Step:      0 Loss:   0.4458 Training Acc:    78.28    78.28    78.28%\n",
      "Epoch   1171 Step:      0 Loss:   0.4455 Training Acc:    78.29    78.29    78.29%\n",
      "Epoch   1172 Step:      0 Loss:   0.4455 Training Acc:    78.27    78.27    78.27%\n",
      "Epoch   1173 Step:      0 Loss:   0.4457 Training Acc:    78.27    78.27    78.27%\n",
      "Epoch   1174 Step:      0 Loss:   0.4457 Training Acc:    78.24    78.24    78.24%\n",
      "Epoch   1175 Step:      0 Loss:   0.4455 Training Acc:    78.28    78.28    78.28%\n",
      "Epoch   1176 Step:      0 Loss:   0.4451 Training Acc:    78.30    78.30    78.30%\n",
      "Epoch   1177 Step:      0 Loss:   0.4447 Training Acc:    78.33    78.33    78.33%\n",
      "Epoch   1178 Step:      0 Loss:   0.4444 Training Acc:    78.36    78.36    78.36%\n",
      "Epoch   1179 Step:      0 Loss:   0.4443 Training Acc:    78.37    78.37    78.37%\n",
      "Epoch   1180 Step:      0 Loss:   0.4443 Training Acc:    78.35    78.35    78.35%\n",
      "Epoch   1181 Step:      0 Loss:   0.4443 Training Acc:    78.36    78.36    78.36%\n",
      "Epoch   1182 Step:      0 Loss:   0.4442 Training Acc:    78.35    78.35    78.35%\n",
      "Epoch   1183 Step:      0 Loss:   0.4440 Training Acc:    78.38    78.38    78.38%\n",
      "Epoch   1184 Step:      0 Loss:   0.4437 Training Acc:    78.41    78.41    78.41%\n",
      "Epoch   1185 Step:      0 Loss:   0.4434 Training Acc:    78.42    78.42    78.42%\n",
      "Epoch   1186 Step:      0 Loss:   0.4432 Training Acc:    78.43    78.43    78.43%\n",
      "Epoch   1187 Step:      0 Loss:   0.4431 Training Acc:    78.43    78.43    78.43%\n",
      "Epoch   1188 Step:      0 Loss:   0.4430 Training Acc:    78.43    78.43    78.43%\n",
      "Epoch   1189 Step:      0 Loss:   0.4429 Training Acc:    78.45    78.45    78.45%\n",
      "Epoch   1190 Step:      0 Loss:   0.4428 Training Acc:    78.44    78.44    78.44%\n",
      "Epoch   1191 Step:      0 Loss:   0.4427 Training Acc:    78.45    78.45    78.45%\n",
      "Epoch   1192 Step:      0 Loss:   0.4425 Training Acc:    78.45    78.45    78.45%\n",
      "Epoch   1193 Step:      0 Loss:   0.4424 Training Acc:    78.48    78.48    78.48%\n",
      "Epoch   1194 Step:      0 Loss:   0.4423 Training Acc:    78.45    78.45    78.45%\n",
      "Epoch   1195 Step:      0 Loss:   0.4422 Training Acc:    78.51    78.51    78.51%\n",
      "Epoch   1196 Step:      0 Loss:   0.4423 Training Acc:    78.43    78.43    78.43%\n",
      "Epoch   1197 Step:      0 Loss:   0.4425 Training Acc:    78.50    78.50    78.50%\n",
      "Epoch   1198 Step:      0 Loss:   0.4430 Training Acc:    78.35    78.35    78.35%\n",
      "Epoch   1199 Step:      0 Loss:   0.4435 Training Acc:    78.44    78.44    78.44%\n",
      "Epoch   1200 Step:      0 Loss:   0.4442 Training Acc:    78.21    78.21    78.21%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1201 Step:      0 Loss:   0.4442 Training Acc:    78.40    78.40    78.40%\n",
      "Epoch   1202 Step:      0 Loss:   0.4435 Training Acc:    78.27    78.27    78.27%\n",
      "Epoch   1203 Step:      0 Loss:   0.4422 Training Acc:    78.54    78.54    78.54%\n",
      "Epoch   1204 Step:      0 Loss:   0.4413 Training Acc:    78.48    78.48    78.48%\n",
      "Epoch   1205 Step:      0 Loss:   0.4411 Training Acc:    78.58    78.58    78.58%\n",
      "Epoch   1206 Step:      0 Loss:   0.4415 Training Acc:    78.49    78.49    78.49%\n",
      "Epoch   1207 Step:      0 Loss:   0.4417 Training Acc:    78.47    78.47    78.47%\n",
      "Epoch   1208 Step:      0 Loss:   0.4413 Training Acc:    78.52    78.52    78.52%\n",
      "Epoch   1209 Step:      0 Loss:   0.4404 Training Acc:    78.56    78.56    78.56%\n",
      "Epoch   1210 Step:      0 Loss:   0.4398 Training Acc:    78.66    78.66    78.66%\n",
      "Epoch   1211 Step:      0 Loss:   0.4398 Training Acc:    78.60    78.60    78.60%\n",
      "Epoch   1212 Step:      0 Loss:   0.4402 Training Acc:    78.59    78.59    78.59%\n",
      "Epoch   1213 Step:      0 Loss:   0.4404 Training Acc:    78.56    78.56    78.56%\n",
      "Epoch   1214 Step:      0 Loss:   0.4401 Training Acc:    78.59    78.59    78.59%\n",
      "Epoch   1215 Step:      0 Loss:   0.4394 Training Acc:    78.62    78.62    78.62%\n",
      "Epoch   1216 Step:      0 Loss:   0.4389 Training Acc:    78.70    78.70    78.70%\n",
      "Epoch   1217 Step:      0 Loss:   0.4388 Training Acc:    78.67    78.67    78.67%\n",
      "Epoch   1218 Step:      0 Loss:   0.4389 Training Acc:    78.69    78.69    78.69%\n",
      "Epoch   1219 Step:      0 Loss:   0.4389 Training Acc:    78.66    78.66    78.66%\n",
      "Epoch   1220 Step:      0 Loss:   0.4386 Training Acc:    78.68    78.68    78.68%\n",
      "Epoch   1221 Step:      0 Loss:   0.4382 Training Acc:    78.72    78.72    78.72%\n",
      "Epoch   1222 Step:      0 Loss:   0.4379 Training Acc:    78.75    78.75    78.75%\n",
      "Epoch   1223 Step:      0 Loss:   0.4378 Training Acc:    78.75    78.75    78.75%\n",
      "Epoch   1224 Step:      0 Loss:   0.4379 Training Acc:    78.72    78.72    78.72%\n",
      "Epoch   1225 Step:      0 Loss:   0.4379 Training Acc:    78.78    78.78    78.78%\n",
      "Epoch   1226 Step:      0 Loss:   0.4377 Training Acc:    78.71    78.71    78.71%\n",
      "Epoch   1227 Step:      0 Loss:   0.4375 Training Acc:    78.79    78.79    78.79%\n",
      "Epoch   1228 Step:      0 Loss:   0.4372 Training Acc:    78.76    78.76    78.76%\n",
      "Epoch   1229 Step:      0 Loss:   0.4371 Training Acc:    78.80    78.80    78.80%\n",
      "Epoch   1230 Step:      0 Loss:   0.4370 Training Acc:    78.75    78.75    78.75%\n",
      "Epoch   1231 Step:      0 Loss:   0.4370 Training Acc:    78.82    78.82    78.82%\n",
      "Epoch   1232 Step:      0 Loss:   0.4370 Training Acc:    78.74    78.74    78.74%\n",
      "Epoch   1233 Step:      0 Loss:   0.4369 Training Acc:    78.81    78.81    78.81%\n",
      "Epoch   1234 Step:      0 Loss:   0.4368 Training Acc:    78.76    78.76    78.76%\n",
      "Epoch   1235 Step:      0 Loss:   0.4367 Training Acc:    78.84    78.84    78.84%\n",
      "Epoch   1236 Step:      0 Loss:   0.4366 Training Acc:    78.76    78.76    78.76%\n",
      "Epoch   1237 Step:      0 Loss:   0.4367 Training Acc:    78.86    78.86    78.86%\n",
      "Epoch   1238 Step:      0 Loss:   0.4367 Training Acc:    78.74    78.74    78.74%\n",
      "Epoch   1239 Step:      0 Loss:   0.4366 Training Acc:    78.86    78.86    78.86%\n",
      "Epoch   1240 Step:      0 Loss:   0.4365 Training Acc:    78.74    78.74    78.74%\n",
      "Epoch   1241 Step:      0 Loss:   0.4362 Training Acc:    78.90    78.90    78.90%\n",
      "Epoch   1242 Step:      0 Loss:   0.4359 Training Acc:    78.79    78.79    78.79%\n",
      "Epoch   1243 Step:      0 Loss:   0.4355 Training Acc:    78.91    78.91    78.91%\n",
      "Epoch   1244 Step:      0 Loss:   0.4352 Training Acc:    78.84    78.84    78.84%\n",
      "Epoch   1245 Step:      0 Loss:   0.4348 Training Acc:    78.92    78.92    78.92%\n",
      "Epoch   1246 Step:      0 Loss:   0.4346 Training Acc:    78.93    78.93    78.93%\n",
      "Epoch   1247 Step:      0 Loss:   0.4344 Training Acc:    78.94    78.94    78.94%\n",
      "Epoch   1248 Step:      0 Loss:   0.4342 Training Acc:    78.97    78.97    78.97%\n",
      "Epoch   1249 Step:      0 Loss:   0.4341 Training Acc:    78.94    78.94    78.94%\n",
      "Epoch   1250 Step:      0 Loss:   0.4341 Training Acc:    78.98    78.98    78.98%\n",
      "Epoch   1251 Step:      0 Loss:   0.4340 Training Acc:    78.93    78.93    78.93%\n",
      "Epoch   1252 Step:      0 Loss:   0.4340 Training Acc:    78.98    78.98    78.98%\n",
      "Epoch   1253 Step:      0 Loss:   0.4340 Training Acc:    78.92    78.92    78.92%\n",
      "Epoch   1254 Step:      0 Loss:   0.4339 Training Acc:    78.99    78.99    78.99%\n",
      "Epoch   1255 Step:      0 Loss:   0.4339 Training Acc:    78.92    78.92    78.92%\n",
      "Epoch   1256 Step:      0 Loss:   0.4337 Training Acc:    79.02    79.02    79.02%\n",
      "Epoch   1257 Step:      0 Loss:   0.4336 Training Acc:    78.93    78.93    78.93%\n",
      "Epoch   1258 Step:      0 Loss:   0.4334 Training Acc:    79.03    79.03    79.03%\n",
      "Epoch   1259 Step:      0 Loss:   0.4332 Training Acc:    78.96    78.96    78.96%\n",
      "Epoch   1260 Step:      0 Loss:   0.4329 Training Acc:    79.06    79.06    79.06%\n",
      "Epoch   1261 Step:      0 Loss:   0.4327 Training Acc:    78.98    78.98    78.98%\n",
      "Epoch   1262 Step:      0 Loss:   0.4325 Training Acc:    79.07    79.07    79.07%\n",
      "Epoch   1263 Step:      0 Loss:   0.4323 Training Acc:    79.02    79.02    79.02%\n",
      "Epoch   1264 Step:      0 Loss:   0.4321 Training Acc:    79.09    79.09    79.09%\n",
      "Epoch   1265 Step:      0 Loss:   0.4319 Training Acc:    79.05    79.05    79.05%\n",
      "Epoch   1266 Step:      0 Loss:   0.4317 Training Acc:    79.11    79.11    79.11%\n",
      "Epoch   1267 Step:      0 Loss:   0.4315 Training Acc:    79.09    79.09    79.09%\n",
      "Epoch   1268 Step:      0 Loss:   0.4314 Training Acc:    79.12    79.12    79.12%\n",
      "Epoch   1269 Step:      0 Loss:   0.4312 Training Acc:    79.12    79.12    79.12%\n",
      "Epoch   1270 Step:      0 Loss:   0.4311 Training Acc:    79.13    79.13    79.13%\n",
      "Epoch   1271 Step:      0 Loss:   0.4309 Training Acc:    79.15    79.15    79.15%\n",
      "Epoch   1272 Step:      0 Loss:   0.4308 Training Acc:    79.14    79.14    79.14%\n",
      "Epoch   1273 Step:      0 Loss:   0.4307 Training Acc:    79.17    79.17    79.17%\n",
      "Epoch   1274 Step:      0 Loss:   0.4306 Training Acc:    79.14    79.14    79.14%\n",
      "Epoch   1275 Step:      0 Loss:   0.4305 Training Acc:    79.18    79.18    79.18%\n",
      "Epoch   1276 Step:      0 Loss:   0.4304 Training Acc:    79.12    79.12    79.12%\n",
      "Epoch   1277 Step:      0 Loss:   0.4304 Training Acc:    79.19    79.19    79.19%\n",
      "Epoch   1278 Step:      0 Loss:   0.4305 Training Acc:    79.12    79.12    79.12%\n",
      "Epoch   1279 Step:      0 Loss:   0.4306 Training Acc:    79.21    79.21    79.21%\n",
      "Epoch   1280 Step:      0 Loss:   0.4310 Training Acc:    79.05    79.05    79.05%\n",
      "Epoch   1281 Step:      0 Loss:   0.4316 Training Acc:    79.19    79.19    79.19%\n",
      "Epoch   1282 Step:      0 Loss:   0.4324 Training Acc:    78.92    78.92    78.92%\n",
      "Epoch   1283 Step:      0 Loss:   0.4333 Training Acc:    79.10    79.10    79.10%\n",
      "Epoch   1284 Step:      0 Loss:   0.4337 Training Acc:    78.80    78.80    78.80%\n",
      "Epoch   1285 Step:      0 Loss:   0.4330 Training Acc:    79.12    79.12    79.12%\n",
      "Epoch   1286 Step:      0 Loss:   0.4312 Training Acc:    79.02    79.02    79.02%\n",
      "Epoch   1287 Step:      0 Loss:   0.4293 Training Acc:    79.30    79.30    79.30%\n",
      "Epoch   1288 Step:      0 Loss:   0.4286 Training Acc:    79.28    79.28    79.28%\n",
      "Epoch   1289 Step:      0 Loss:   0.4292 Training Acc:    79.18    79.18    79.18%\n",
      "Epoch   1290 Step:      0 Loss:   0.4301 Training Acc:    79.28    79.28    79.28%\n",
      "Epoch   1291 Step:      0 Loss:   0.4304 Training Acc:    79.07    79.07    79.07%\n",
      "Epoch   1292 Step:      0 Loss:   0.4297 Training Acc:    79.31    79.31    79.31%\n",
      "Epoch   1293 Step:      0 Loss:   0.4285 Training Acc:    79.22    79.22    79.22%\n",
      "Epoch   1294 Step:      0 Loss:   0.4279 Training Acc:    79.32    79.32    79.32%\n",
      "Epoch   1295 Step:      0 Loss:   0.4280 Training Acc:    79.34    79.34    79.34%\n",
      "Epoch   1296 Step:      0 Loss:   0.4285 Training Acc:    79.21    79.21    79.21%\n",
      "Epoch   1297 Step:      0 Loss:   0.4287 Training Acc:    79.37    79.37    79.37%\n",
      "Epoch   1298 Step:      0 Loss:   0.4283 Training Acc:    79.22    79.22    79.22%\n",
      "Epoch   1299 Step:      0 Loss:   0.4275 Training Acc:    79.36    79.36    79.36%\n",
      "Epoch   1300 Step:      0 Loss:   0.4270 Training Acc:    79.35    79.35    79.35%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1301 Step:      0 Loss:   0.4270 Training Acc:    79.34    79.34    79.34%\n",
      "Epoch   1302 Step:      0 Loss:   0.4272 Training Acc:    79.41    79.41    79.41%\n",
      "Epoch   1303 Step:      0 Loss:   0.4273 Training Acc:    79.30    79.30    79.30%\n",
      "Epoch   1304 Step:      0 Loss:   0.4271 Training Acc:    79.43    79.43    79.43%\n",
      "Epoch   1305 Step:      0 Loss:   0.4266 Training Acc:    79.36    79.36    79.36%\n",
      "Epoch   1306 Step:      0 Loss:   0.4262 Training Acc:    79.44    79.44    79.44%\n",
      "Epoch   1307 Step:      0 Loss:   0.4261 Training Acc:    79.44    79.44    79.44%\n",
      "Epoch   1308 Step:      0 Loss:   0.4261 Training Acc:    79.39    79.39    79.39%\n",
      "Epoch   1309 Step:      0 Loss:   0.4262 Training Acc:    79.46    79.46    79.46%\n",
      "Epoch   1310 Step:      0 Loss:   0.4261 Training Acc:    79.39    79.39    79.39%\n",
      "Epoch   1311 Step:      0 Loss:   0.4258 Training Acc:    79.47    79.47    79.47%\n",
      "Epoch   1312 Step:      0 Loss:   0.4255 Training Acc:    79.43    79.43    79.43%\n",
      "Epoch   1313 Step:      0 Loss:   0.4253 Training Acc:    79.48    79.48    79.48%\n",
      "Epoch   1314 Step:      0 Loss:   0.4252 Training Acc:    79.50    79.50    79.50%\n",
      "Epoch   1315 Step:      0 Loss:   0.4251 Training Acc:    79.46    79.46    79.46%\n",
      "Epoch   1316 Step:      0 Loss:   0.4251 Training Acc:    79.51    79.51    79.51%\n",
      "Epoch   1317 Step:      0 Loss:   0.4250 Training Acc:    79.45    79.45    79.45%\n",
      "Epoch   1318 Step:      0 Loss:   0.4249 Training Acc:    79.51    79.51    79.51%\n",
      "Epoch   1319 Step:      0 Loss:   0.4247 Training Acc:    79.49    79.49    79.49%\n",
      "Epoch   1320 Step:      0 Loss:   0.4246 Training Acc:    79.54    79.54    79.54%\n",
      "Epoch   1321 Step:      0 Loss:   0.4245 Training Acc:    79.53    79.53    79.53%\n",
      "Epoch   1322 Step:      0 Loss:   0.4246 Training Acc:    79.48    79.48    79.48%\n",
      "Epoch   1323 Step:      0 Loss:   0.4248 Training Acc:    79.53    79.53    79.53%\n",
      "Epoch   1324 Step:      0 Loss:   0.4251 Training Acc:    79.43    79.43    79.43%\n",
      "Epoch   1325 Step:      0 Loss:   0.4256 Training Acc:    79.53    79.53    79.53%\n",
      "Epoch   1326 Step:      0 Loss:   0.4261 Training Acc:    79.35    79.35    79.35%\n",
      "Epoch   1327 Step:      0 Loss:   0.4262 Training Acc:    79.48    79.48    79.48%\n",
      "Epoch   1328 Step:      0 Loss:   0.4260 Training Acc:    79.36    79.36    79.36%\n",
      "Epoch   1329 Step:      0 Loss:   0.4249 Training Acc:    79.52    79.52    79.52%\n",
      "Epoch   1330 Step:      0 Loss:   0.4238 Training Acc:    79.56    79.56    79.56%\n",
      "Epoch   1331 Step:      0 Loss:   0.4231 Training Acc:    79.58    79.58    79.58%\n",
      "Epoch   1332 Step:      0 Loss:   0.4231 Training Acc:    79.63    79.63    79.63%\n",
      "Epoch   1333 Step:      0 Loss:   0.4235 Training Acc:    79.53    79.53    79.53%\n",
      "Epoch   1334 Step:      0 Loss:   0.4238 Training Acc:    79.61    79.61    79.61%\n",
      "Epoch   1335 Step:      0 Loss:   0.4238 Training Acc:    79.52    79.52    79.52%\n",
      "Epoch   1336 Step:      0 Loss:   0.4233 Training Acc:    79.61    79.61    79.61%\n",
      "Epoch   1337 Step:      0 Loss:   0.4226 Training Acc:    79.63    79.63    79.63%\n",
      "Epoch   1338 Step:      0 Loss:   0.4221 Training Acc:    79.65    79.65    79.65%\n",
      "Epoch   1339 Step:      0 Loss:   0.4220 Training Acc:    79.70    79.70    79.70%\n",
      "Epoch   1340 Step:      0 Loss:   0.4222 Training Acc:    79.64    79.64    79.64%\n",
      "Epoch   1341 Step:      0 Loss:   0.4224 Training Acc:    79.68    79.68    79.68%\n",
      "Epoch   1342 Step:      0 Loss:   0.4223 Training Acc:    79.63    79.63    79.63%\n",
      "Epoch   1343 Step:      0 Loss:   0.4219 Training Acc:    79.69    79.69    79.69%\n",
      "Epoch   1344 Step:      0 Loss:   0.4215 Training Acc:    79.67    79.67    79.67%\n",
      "Epoch   1345 Step:      0 Loss:   0.4211 Training Acc:    79.73    79.73    79.73%\n",
      "Epoch   1346 Step:      0 Loss:   0.4210 Training Acc:    79.73    79.73    79.73%\n",
      "Epoch   1347 Step:      0 Loss:   0.4210 Training Acc:    79.73    79.73    79.73%\n",
      "Epoch   1348 Step:      0 Loss:   0.4211 Training Acc:    79.75    79.75    79.75%\n",
      "Epoch   1349 Step:      0 Loss:   0.4210 Training Acc:    79.72    79.72    79.72%\n",
      "Epoch   1350 Step:      0 Loss:   0.4209 Training Acc:    79.77    79.77    79.77%\n",
      "Epoch   1351 Step:      0 Loss:   0.4206 Training Acc:    79.75    79.75    79.75%\n",
      "Epoch   1352 Step:      0 Loss:   0.4203 Training Acc:    79.80    79.80    79.80%\n",
      "Epoch   1353 Step:      0 Loss:   0.4201 Training Acc:    79.77    79.77    79.77%\n",
      "Epoch   1354 Step:      0 Loss:   0.4200 Training Acc:    79.81    79.81    79.81%\n",
      "Epoch   1355 Step:      0 Loss:   0.4199 Training Acc:    79.78    79.78    79.78%\n",
      "Epoch   1356 Step:      0 Loss:   0.4199 Training Acc:    79.81    79.81    79.81%\n",
      "Epoch   1357 Step:      0 Loss:   0.4198 Training Acc:    79.80    79.80    79.80%\n",
      "Epoch   1358 Step:      0 Loss:   0.4197 Training Acc:    79.82    79.82    79.82%\n",
      "Epoch   1359 Step:      0 Loss:   0.4195 Training Acc:    79.82    79.82    79.82%\n",
      "Epoch   1360 Step:      0 Loss:   0.4193 Training Acc:    79.84    79.84    79.84%\n",
      "Epoch   1361 Step:      0 Loss:   0.4191 Training Acc:    79.84    79.84    79.84%\n",
      "Epoch   1362 Step:      0 Loss:   0.4190 Training Acc:    79.86    79.86    79.86%\n",
      "Epoch   1363 Step:      0 Loss:   0.4188 Training Acc:    79.85    79.85    79.85%\n",
      "Epoch   1364 Step:      0 Loss:   0.4187 Training Acc:    79.88    79.88    79.88%\n",
      "Epoch   1365 Step:      0 Loss:   0.4186 Training Acc:    79.87    79.87    79.87%\n",
      "Epoch   1366 Step:      0 Loss:   0.4186 Training Acc:    79.90    79.90    79.90%\n",
      "Epoch   1367 Step:      0 Loss:   0.4185 Training Acc:    79.87    79.87    79.87%\n",
      "Epoch   1368 Step:      0 Loss:   0.4184 Training Acc:    79.92    79.92    79.92%\n",
      "Epoch   1369 Step:      0 Loss:   0.4183 Training Acc:    79.87    79.87    79.87%\n",
      "Epoch   1370 Step:      0 Loss:   0.4183 Training Acc:    79.93    79.93    79.93%\n",
      "Epoch   1371 Step:      0 Loss:   0.4183 Training Acc:    79.87    79.87    79.87%\n",
      "Epoch   1372 Step:      0 Loss:   0.4184 Training Acc:    79.92    79.92    79.92%\n",
      "Epoch   1373 Step:      0 Loss:   0.4186 Training Acc:    79.83    79.83    79.83%\n",
      "Epoch   1374 Step:      0 Loss:   0.4190 Training Acc:    79.92    79.92    79.92%\n",
      "Epoch   1375 Step:      0 Loss:   0.4196 Training Acc:    79.76    79.76    79.76%\n",
      "Epoch   1376 Step:      0 Loss:   0.4202 Training Acc:    79.83    79.83    79.83%\n",
      "Epoch   1377 Step:      0 Loss:   0.4206 Training Acc:    79.69    79.69    79.69%\n",
      "Epoch   1378 Step:      0 Loss:   0.4202 Training Acc:    79.83    79.83    79.83%\n",
      "Epoch   1379 Step:      0 Loss:   0.4191 Training Acc:    79.80    79.80    79.80%\n",
      "Epoch   1380 Step:      0 Loss:   0.4176 Training Acc:    79.96    79.96    79.96%\n",
      "Epoch   1381 Step:      0 Loss:   0.4166 Training Acc:    79.98    79.98    79.98%\n",
      "Epoch   1382 Step:      0 Loss:   0.4166 Training Acc:    79.98    79.98    79.98%\n",
      "Epoch   1383 Step:      0 Loss:   0.4171 Training Acc:    80.00    80.00    80.00%\n",
      "Epoch   1384 Step:      0 Loss:   0.4177 Training Acc:    79.89    79.89    79.89%\n",
      "Epoch   1385 Step:      0 Loss:   0.4178 Training Acc:    79.97    79.97    79.97%\n",
      "Epoch   1386 Step:      0 Loss:   0.4172 Training Acc:    79.92    79.92    79.92%\n",
      "Epoch   1387 Step:      0 Loss:   0.4164 Training Acc:    80.03    80.03    80.03%\n",
      "Epoch   1388 Step:      0 Loss:   0.4158 Training Acc:    80.03    80.03    80.03%\n",
      "Epoch   1389 Step:      0 Loss:   0.4157 Training Acc:    80.03    80.03    80.03%\n",
      "Epoch   1390 Step:      0 Loss:   0.4159 Training Acc:    80.07    80.07    80.07%\n",
      "Epoch   1391 Step:      0 Loss:   0.4162 Training Acc:    79.99    79.99    79.99%\n",
      "Epoch   1392 Step:      0 Loss:   0.4161 Training Acc:    80.07    80.07    80.07%\n",
      "Epoch   1393 Step:      0 Loss:   0.4157 Training Acc:    80.02    80.02    80.02%\n",
      "Epoch   1394 Step:      0 Loss:   0.4152 Training Acc:    80.10    80.10    80.10%\n",
      "Epoch   1395 Step:      0 Loss:   0.4148 Training Acc:    80.09    80.09    80.09%\n",
      "Epoch   1396 Step:      0 Loss:   0.4148 Training Acc:    80.09    80.09    80.09%\n",
      "Epoch   1397 Step:      0 Loss:   0.4148 Training Acc:    80.13    80.13    80.13%\n",
      "Epoch   1398 Step:      0 Loss:   0.4149 Training Acc:    80.07    80.07    80.07%\n",
      "Epoch   1399 Step:      0 Loss:   0.4149 Training Acc:    80.15    80.15    80.15%\n",
      "Epoch   1400 Step:      0 Loss:   0.4146 Training Acc:    80.08    80.08    80.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1401 Step:      0 Loss:   0.4143 Training Acc:    80.16    80.16    80.16%\n",
      "Epoch   1402 Step:      0 Loss:   0.4140 Training Acc:    80.14    80.14    80.14%\n",
      "Epoch   1403 Step:      0 Loss:   0.4139 Training Acc:    80.16    80.16    80.16%\n",
      "Epoch   1404 Step:      0 Loss:   0.4138 Training Acc:    80.19    80.19    80.19%\n",
      "Epoch   1405 Step:      0 Loss:   0.4138 Training Acc:    80.13    80.13    80.13%\n",
      "Epoch   1406 Step:      0 Loss:   0.4138 Training Acc:    80.20    80.20    80.20%\n",
      "Epoch   1407 Step:      0 Loss:   0.4136 Training Acc:    80.14    80.14    80.14%\n",
      "Epoch   1408 Step:      0 Loss:   0.4134 Training Acc:    80.22    80.22    80.22%\n",
      "Epoch   1409 Step:      0 Loss:   0.4132 Training Acc:    80.18    80.18    80.18%\n",
      "Epoch   1410 Step:      0 Loss:   0.4130 Training Acc:    80.23    80.23    80.23%\n",
      "Epoch   1411 Step:      0 Loss:   0.4129 Training Acc:    80.23    80.23    80.23%\n",
      "Epoch   1412 Step:      0 Loss:   0.4128 Training Acc:    80.23    80.23    80.23%\n",
      "Epoch   1413 Step:      0 Loss:   0.4127 Training Acc:    80.26    80.26    80.26%\n",
      "Epoch   1414 Step:      0 Loss:   0.4127 Training Acc:    80.23    80.23    80.23%\n",
      "Epoch   1415 Step:      0 Loss:   0.4126 Training Acc:    80.27    80.27    80.27%\n",
      "Epoch   1416 Step:      0 Loss:   0.4125 Training Acc:    80.25    80.25    80.25%\n",
      "Epoch   1417 Step:      0 Loss:   0.4125 Training Acc:    80.25    80.25    80.25%\n",
      "Epoch   1418 Step:      0 Loss:   0.4125 Training Acc:    80.28    80.28    80.28%\n",
      "Epoch   1419 Step:      0 Loss:   0.4127 Training Acc:    80.18    80.18    80.18%\n",
      "Epoch   1420 Step:      0 Loss:   0.4132 Training Acc:    80.26    80.26    80.26%\n",
      "Epoch   1421 Step:      0 Loss:   0.4143 Training Acc:    80.07    80.07    80.07%\n",
      "Epoch   1422 Step:      0 Loss:   0.4159 Training Acc:    80.11    80.11    80.11%\n",
      "Epoch   1423 Step:      0 Loss:   0.4178 Training Acc:    79.79    79.79    79.79%\n",
      "Epoch   1424 Step:      0 Loss:   0.4185 Training Acc:    79.94    79.94    79.94%\n",
      "Epoch   1425 Step:      0 Loss:   0.4171 Training Acc:    79.80    79.80    79.80%\n",
      "Epoch   1426 Step:      0 Loss:   0.4141 Training Acc:    80.25    80.25    80.25%\n",
      "Epoch   1427 Step:      0 Loss:   0.4122 Training Acc:    80.22    80.22    80.22%\n",
      "Epoch   1428 Step:      0 Loss:   0.4126 Training Acc:    80.22    80.22    80.22%\n",
      "Epoch   1429 Step:      0 Loss:   0.4137 Training Acc:    80.20    80.20    80.20%\n",
      "Epoch   1430 Step:      0 Loss:   0.4138 Training Acc:    80.08    80.08    80.08%\n",
      "Epoch   1431 Step:      0 Loss:   0.4123 Training Acc:    80.37    80.37    80.37%\n",
      "Epoch   1432 Step:      0 Loss:   0.4112 Training Acc:    80.29    80.29    80.29%\n",
      "Epoch   1433 Step:      0 Loss:   0.4117 Training Acc:    80.30    80.30    80.30%\n",
      "Epoch   1434 Step:      0 Loss:   0.4125 Training Acc:    80.26    80.26    80.26%\n",
      "Epoch   1435 Step:      0 Loss:   0.4122 Training Acc:    80.22    80.22    80.22%\n",
      "Epoch   1436 Step:      0 Loss:   0.4107 Training Acc:    80.40    80.40    80.40%\n",
      "Epoch   1437 Step:      0 Loss:   0.4100 Training Acc:    80.40    80.40    80.40%\n",
      "Epoch   1438 Step:      0 Loss:   0.4105 Training Acc:    80.38    80.38    80.38%\n",
      "Epoch   1439 Step:      0 Loss:   0.4112 Training Acc:    80.32    80.32    80.32%\n",
      "Epoch   1440 Step:      0 Loss:   0.4110 Training Acc:    80.34    80.34    80.34%\n",
      "Epoch   1441 Step:      0 Loss:   0.4099 Training Acc:    80.41    80.41    80.41%\n",
      "Epoch   1442 Step:      0 Loss:   0.4093 Training Acc:    80.47    80.47    80.47%\n",
      "Epoch   1443 Step:      0 Loss:   0.4096 Training Acc:    80.40    80.40    80.40%\n",
      "Epoch   1444 Step:      0 Loss:   0.4100 Training Acc:    80.42    80.42    80.42%\n",
      "Epoch   1445 Step:      0 Loss:   0.4098 Training Acc:    80.40    80.40    80.40%\n",
      "Epoch   1446 Step:      0 Loss:   0.4092 Training Acc:    80.46    80.46    80.46%\n",
      "Epoch   1447 Step:      0 Loss:   0.4088 Training Acc:    80.50    80.50    80.50%\n",
      "Epoch   1448 Step:      0 Loss:   0.4089 Training Acc:    80.43    80.43    80.43%\n",
      "Epoch   1449 Step:      0 Loss:   0.4091 Training Acc:    80.51    80.51    80.51%\n",
      "Epoch   1450 Step:      0 Loss:   0.4089 Training Acc:    80.46    80.46    80.46%\n",
      "Epoch   1451 Step:      0 Loss:   0.4084 Training Acc:    80.52    80.52    80.52%\n",
      "Epoch   1452 Step:      0 Loss:   0.4082 Training Acc:    80.56    80.56    80.56%\n",
      "Epoch   1453 Step:      0 Loss:   0.4082 Training Acc:    80.50    80.50    80.50%\n",
      "Epoch   1454 Step:      0 Loss:   0.4083 Training Acc:    80.55    80.55    80.55%\n",
      "Epoch   1455 Step:      0 Loss:   0.4082 Training Acc:    80.49    80.49    80.49%\n",
      "Epoch   1456 Step:      0 Loss:   0.4079 Training Acc:    80.57    80.57    80.57%\n",
      "Epoch   1457 Step:      0 Loss:   0.4076 Training Acc:    80.53    80.53    80.53%\n",
      "Epoch   1458 Step:      0 Loss:   0.4075 Training Acc:    80.58    80.58    80.58%\n",
      "Epoch   1459 Step:      0 Loss:   0.4075 Training Acc:    80.57    80.57    80.57%\n",
      "Epoch   1460 Step:      0 Loss:   0.4075 Training Acc:    80.57    80.57    80.57%\n",
      "Epoch   1461 Step:      0 Loss:   0.4073 Training Acc:    80.61    80.61    80.61%\n",
      "Epoch   1462 Step:      0 Loss:   0.4071 Training Acc:    80.58    80.58    80.58%\n",
      "Epoch   1463 Step:      0 Loss:   0.4069 Training Acc:    80.64    80.64    80.64%\n",
      "Epoch   1464 Step:      0 Loss:   0.4069 Training Acc:    80.60    80.60    80.60%\n",
      "Epoch   1465 Step:      0 Loss:   0.4068 Training Acc:    80.61    80.61    80.61%\n",
      "Epoch   1466 Step:      0 Loss:   0.4067 Training Acc:    80.61    80.61    80.61%\n",
      "Epoch   1467 Step:      0 Loss:   0.4065 Training Acc:    80.64    80.64    80.64%\n",
      "Epoch   1468 Step:      0 Loss:   0.4064 Training Acc:    80.62    80.62    80.62%\n",
      "Epoch   1469 Step:      0 Loss:   0.4063 Training Acc:    80.68    80.68    80.68%\n",
      "Epoch   1470 Step:      0 Loss:   0.4062 Training Acc:    80.63    80.63    80.63%\n",
      "Epoch   1471 Step:      0 Loss:   0.4061 Training Acc:    80.69    80.69    80.69%\n",
      "Epoch   1472 Step:      0 Loss:   0.4060 Training Acc:    80.65    80.65    80.65%\n",
      "Epoch   1473 Step:      0 Loss:   0.4058 Training Acc:    80.70    80.70    80.70%\n",
      "Epoch   1474 Step:      0 Loss:   0.4057 Training Acc:    80.68    80.68    80.68%\n",
      "Epoch   1475 Step:      0 Loss:   0.4056 Training Acc:    80.68    80.68    80.68%\n",
      "Epoch   1476 Step:      0 Loss:   0.4055 Training Acc:    80.71    80.71    80.71%\n",
      "Epoch   1477 Step:      0 Loss:   0.4054 Training Acc:    80.68    80.68    80.68%\n",
      "Epoch   1478 Step:      0 Loss:   0.4053 Training Acc:    80.72    80.72    80.72%\n",
      "Epoch   1479 Step:      0 Loss:   0.4051 Training Acc:    80.70    80.70    80.70%\n",
      "Epoch   1480 Step:      0 Loss:   0.4050 Training Acc:    80.73    80.73    80.73%\n",
      "Epoch   1481 Step:      0 Loss:   0.4049 Training Acc:    80.72    80.72    80.72%\n",
      "Epoch   1482 Step:      0 Loss:   0.4048 Training Acc:    80.73    80.73    80.73%\n",
      "Epoch   1483 Step:      0 Loss:   0.4047 Training Acc:    80.74    80.74    80.74%\n",
      "Epoch   1484 Step:      0 Loss:   0.4046 Training Acc:    80.76    80.76    80.76%\n",
      "Epoch   1485 Step:      0 Loss:   0.4045 Training Acc:    80.75    80.75    80.75%\n",
      "Epoch   1486 Step:      0 Loss:   0.4043 Training Acc:    80.77    80.77    80.77%\n",
      "Epoch   1487 Step:      0 Loss:   0.4042 Training Acc:    80.75    80.75    80.75%\n",
      "Epoch   1488 Step:      0 Loss:   0.4041 Training Acc:    80.80    80.80    80.80%\n",
      "Epoch   1489 Step:      0 Loss:   0.4040 Training Acc:    80.75    80.75    80.75%\n",
      "Epoch   1490 Step:      0 Loss:   0.4040 Training Acc:    80.81    80.81    80.81%\n",
      "Epoch   1491 Step:      0 Loss:   0.4039 Training Acc:    80.74    80.74    80.74%\n",
      "Epoch   1492 Step:      0 Loss:   0.4039 Training Acc:    80.80    80.80    80.80%\n",
      "Epoch   1493 Step:      0 Loss:   0.4039 Training Acc:    80.74    80.74    80.74%\n",
      "Epoch   1494 Step:      0 Loss:   0.4041 Training Acc:    80.80    80.80    80.80%\n",
      "Epoch   1495 Step:      0 Loss:   0.4044 Training Acc:    80.68    80.68    80.68%\n",
      "Epoch   1496 Step:      0 Loss:   0.4050 Training Acc:    80.75    80.75    80.75%\n",
      "Epoch   1497 Step:      0 Loss:   0.4060 Training Acc:    80.56    80.56    80.56%\n",
      "Epoch   1498 Step:      0 Loss:   0.4072 Training Acc:    80.62    80.62    80.62%\n",
      "Epoch   1499 Step:      0 Loss:   0.4085 Training Acc:    80.36    80.36    80.36%\n",
      "Epoch   1500 Step:      0 Loss:   0.4086 Training Acc:    80.54    80.54    80.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1501 Step:      0 Loss:   0.4074 Training Acc:    80.43    80.43    80.43%\n",
      "Epoch   1502 Step:      0 Loss:   0.4050 Training Acc:    80.76    80.76    80.76%\n",
      "Epoch   1503 Step:      0 Loss:   0.4031 Training Acc:    80.75    80.75    80.75%\n",
      "Epoch   1504 Step:      0 Loss:   0.4028 Training Acc:    80.84    80.84    80.84%\n",
      "Epoch   1505 Step:      0 Loss:   0.4037 Training Acc:    80.79    80.79    80.79%\n",
      "Epoch   1506 Step:      0 Loss:   0.4046 Training Acc:    80.66    80.66    80.66%\n",
      "Epoch   1507 Step:      0 Loss:   0.4044 Training Acc:    80.79    80.79    80.79%\n",
      "Epoch   1508 Step:      0 Loss:   0.4035 Training Acc:    80.71    80.71    80.71%\n",
      "Epoch   1509 Step:      0 Loss:   0.4025 Training Acc:    80.88    80.88    80.88%\n",
      "Epoch   1510 Step:      0 Loss:   0.4023 Training Acc:    80.83    80.83    80.83%\n",
      "Epoch   1511 Step:      0 Loss:   0.4026 Training Acc:    80.82    80.82    80.82%\n",
      "Epoch   1512 Step:      0 Loss:   0.4027 Training Acc:    80.88    80.88    80.88%\n",
      "Epoch   1513 Step:      0 Loss:   0.4024 Training Acc:    80.81    80.81    80.81%\n",
      "Epoch   1514 Step:      0 Loss:   0.4019 Training Acc:    80.92    80.92    80.92%\n",
      "Epoch   1515 Step:      0 Loss:   0.4016 Training Acc:    80.86    80.86    80.86%\n",
      "Epoch   1516 Step:      0 Loss:   0.4016 Training Acc:    80.93    80.93    80.93%\n",
      "Epoch   1517 Step:      0 Loss:   0.4017 Training Acc:    80.88    80.88    80.88%\n",
      "Epoch   1518 Step:      0 Loss:   0.4015 Training Acc:    80.88    80.88    80.88%\n",
      "Epoch   1519 Step:      0 Loss:   0.4011 Training Acc:    80.96    80.96    80.96%\n",
      "Epoch   1520 Step:      0 Loss:   0.4008 Training Acc:    80.91    80.91    80.91%\n",
      "Epoch   1521 Step:      0 Loss:   0.4007 Training Acc:    80.97    80.97    80.97%\n",
      "Epoch   1522 Step:      0 Loss:   0.4008 Training Acc:    80.92    80.92    80.92%\n",
      "Epoch   1523 Step:      0 Loss:   0.4008 Training Acc:    80.96    80.96    80.96%\n",
      "Epoch   1524 Step:      0 Loss:   0.4007 Training Acc:    80.93    80.93    80.93%\n",
      "Epoch   1525 Step:      0 Loss:   0.4003 Training Acc:    80.98    80.98    80.98%\n",
      "Epoch   1526 Step:      0 Loss:   0.4000 Training Acc:    81.00    81.00    81.00%\n",
      "Epoch   1527 Step:      0 Loss:   0.3999 Training Acc:    80.98    80.98    80.98%\n",
      "Epoch   1528 Step:      0 Loss:   0.3999 Training Acc:    81.00    81.00    81.00%\n",
      "Epoch   1529 Step:      0 Loss:   0.4000 Training Acc:    80.98    80.98    80.98%\n",
      "Epoch   1530 Step:      0 Loss:   0.3999 Training Acc:    81.01    81.01    81.01%\n",
      "Epoch   1531 Step:      0 Loss:   0.3997 Training Acc:    80.99    80.99    80.99%\n",
      "Epoch   1532 Step:      0 Loss:   0.3994 Training Acc:    81.04    81.04    81.04%\n",
      "Epoch   1533 Step:      0 Loss:   0.3992 Training Acc:    81.02    81.02    81.02%\n",
      "Epoch   1534 Step:      0 Loss:   0.3991 Training Acc:    81.05    81.05    81.05%\n",
      "Epoch   1535 Step:      0 Loss:   0.3991 Training Acc:    81.04    81.04    81.04%\n",
      "Epoch   1536 Step:      0 Loss:   0.3990 Training Acc:    81.05    81.05    81.05%\n",
      "Epoch   1537 Step:      0 Loss:   0.3989 Training Acc:    81.07    81.07    81.07%\n",
      "Epoch   1538 Step:      0 Loss:   0.3988 Training Acc:    81.04    81.04    81.04%\n",
      "Epoch   1539 Step:      0 Loss:   0.3986 Training Acc:    81.09    81.09    81.09%\n",
      "Epoch   1540 Step:      0 Loss:   0.3985 Training Acc:    81.07    81.07    81.07%\n",
      "Epoch   1541 Step:      0 Loss:   0.3984 Training Acc:    81.12    81.12    81.12%\n",
      "Epoch   1542 Step:      0 Loss:   0.3983 Training Acc:    81.07    81.07    81.07%\n",
      "Epoch   1543 Step:      0 Loss:   0.3982 Training Acc:    81.11    81.11    81.11%\n",
      "Epoch   1544 Step:      0 Loss:   0.3981 Training Acc:    81.10    81.10    81.10%\n",
      "Epoch   1545 Step:      0 Loss:   0.3980 Training Acc:    81.12    81.12    81.12%\n",
      "Epoch   1546 Step:      0 Loss:   0.3978 Training Acc:    81.13    81.13    81.13%\n",
      "Epoch   1547 Step:      0 Loss:   0.3977 Training Acc:    81.13    81.13    81.13%\n",
      "Epoch   1548 Step:      0 Loss:   0.3976 Training Acc:    81.14    81.14    81.14%\n",
      "Epoch   1549 Step:      0 Loss:   0.3975 Training Acc:    81.15    81.15    81.15%\n",
      "Epoch   1550 Step:      0 Loss:   0.3974 Training Acc:    81.15    81.15    81.15%\n",
      "Epoch   1551 Step:      0 Loss:   0.3973 Training Acc:    81.15    81.15    81.15%\n",
      "Epoch   1552 Step:      0 Loss:   0.3972 Training Acc:    81.18    81.18    81.18%\n",
      "Epoch   1553 Step:      0 Loss:   0.3971 Training Acc:    81.16    81.16    81.16%\n",
      "Epoch   1554 Step:      0 Loss:   0.3970 Training Acc:    81.19    81.19    81.19%\n",
      "Epoch   1555 Step:      0 Loss:   0.3969 Training Acc:    81.17    81.17    81.17%\n",
      "Epoch   1556 Step:      0 Loss:   0.3967 Training Acc:    81.20    81.20    81.20%\n",
      "Epoch   1557 Step:      0 Loss:   0.3966 Training Acc:    81.18    81.18    81.18%\n",
      "Epoch   1558 Step:      0 Loss:   0.3965 Training Acc:    81.21    81.21    81.21%\n",
      "Epoch   1559 Step:      0 Loss:   0.3964 Training Acc:    81.19    81.19    81.19%\n",
      "Epoch   1560 Step:      0 Loss:   0.3963 Training Acc:    81.23    81.23    81.23%\n",
      "Epoch   1561 Step:      0 Loss:   0.3963 Training Acc:    81.20    81.20    81.20%\n",
      "Epoch   1562 Step:      0 Loss:   0.3962 Training Acc:    81.24    81.24    81.24%\n",
      "Epoch   1563 Step:      0 Loss:   0.3961 Training Acc:    81.20    81.20    81.20%\n",
      "Epoch   1564 Step:      0 Loss:   0.3960 Training Acc:    81.24    81.24    81.24%\n",
      "Epoch   1565 Step:      0 Loss:   0.3959 Training Acc:    81.19    81.19    81.19%\n",
      "Epoch   1566 Step:      0 Loss:   0.3959 Training Acc:    81.24    81.24    81.24%\n",
      "Epoch   1567 Step:      0 Loss:   0.3958 Training Acc:    81.19    81.19    81.19%\n",
      "Epoch   1568 Step:      0 Loss:   0.3959 Training Acc:    81.25    81.25    81.25%\n",
      "Epoch   1569 Step:      0 Loss:   0.3960 Training Acc:    81.16    81.16    81.16%\n",
      "Epoch   1570 Step:      0 Loss:   0.3962 Training Acc:    81.24    81.24    81.24%\n",
      "Epoch   1571 Step:      0 Loss:   0.3965 Training Acc:    81.13    81.13    81.13%\n",
      "Epoch   1572 Step:      0 Loss:   0.3971 Training Acc:    81.23    81.23    81.23%\n",
      "Epoch   1573 Step:      0 Loss:   0.3978 Training Acc:    81.01    81.01    81.01%\n",
      "Epoch   1574 Step:      0 Loss:   0.3986 Training Acc:    81.11    81.11    81.11%\n",
      "Epoch   1575 Step:      0 Loss:   0.3992 Training Acc:    80.90    80.90    80.90%\n",
      "Epoch   1576 Step:      0 Loss:   0.3992 Training Acc:    81.08    81.08    81.08%\n",
      "Epoch   1577 Step:      0 Loss:   0.3983 Training Acc:    80.96    80.96    80.96%\n",
      "Epoch   1578 Step:      0 Loss:   0.3967 Training Acc:    81.23    81.23    81.23%\n",
      "Epoch   1579 Step:      0 Loss:   0.3951 Training Acc:    81.19    81.19    81.19%\n",
      "Epoch   1580 Step:      0 Loss:   0.3943 Training Acc:    81.34    81.34    81.34%\n",
      "Epoch   1581 Step:      0 Loss:   0.3944 Training Acc:    81.33    81.33    81.33%\n",
      "Epoch   1582 Step:      0 Loss:   0.3951 Training Acc:    81.20    81.20    81.20%\n",
      "Epoch   1583 Step:      0 Loss:   0.3957 Training Acc:    81.29    81.29    81.29%\n",
      "Epoch   1584 Step:      0 Loss:   0.3957 Training Acc:    81.14    81.14    81.14%\n",
      "Epoch   1585 Step:      0 Loss:   0.3952 Training Acc:    81.32    81.32    81.32%\n",
      "Epoch   1586 Step:      0 Loss:   0.3943 Training Acc:    81.26    81.26    81.26%\n",
      "Epoch   1587 Step:      0 Loss:   0.3936 Training Acc:    81.37    81.37    81.37%\n",
      "Epoch   1588 Step:      0 Loss:   0.3934 Training Acc:    81.38    81.38    81.38%\n",
      "Epoch   1589 Step:      0 Loss:   0.3936 Training Acc:    81.33    81.33    81.33%\n",
      "Epoch   1590 Step:      0 Loss:   0.3939 Training Acc:    81.38    81.38    81.38%\n",
      "Epoch   1591 Step:      0 Loss:   0.3940 Training Acc:    81.27    81.27    81.27%\n",
      "Epoch   1592 Step:      0 Loss:   0.3938 Training Acc:    81.38    81.38    81.38%\n",
      "Epoch   1593 Step:      0 Loss:   0.3934 Training Acc:    81.32    81.32    81.32%\n",
      "Epoch   1594 Step:      0 Loss:   0.3929 Training Acc:    81.41    81.41    81.41%\n",
      "Epoch   1595 Step:      0 Loss:   0.3927 Training Acc:    81.40    81.40    81.40%\n",
      "Epoch   1596 Step:      0 Loss:   0.3926 Training Acc:    81.40    81.40    81.40%\n",
      "Epoch   1597 Step:      0 Loss:   0.3927 Training Acc:    81.42    81.42    81.42%\n",
      "Epoch   1598 Step:      0 Loss:   0.3928 Training Acc:    81.38    81.38    81.38%\n",
      "Epoch   1599 Step:      0 Loss:   0.3927 Training Acc:    81.43    81.43    81.43%\n",
      "Epoch   1600 Step:      0 Loss:   0.3925 Training Acc:    81.39    81.39    81.39%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1601 Step:      0 Loss:   0.3922 Training Acc:    81.45    81.45    81.45%\n",
      "Epoch   1602 Step:      0 Loss:   0.3920 Training Acc:    81.43    81.43    81.43%\n",
      "Epoch   1603 Step:      0 Loss:   0.3918 Training Acc:    81.47    81.47    81.47%\n",
      "Epoch   1604 Step:      0 Loss:   0.3917 Training Acc:    81.47    81.47    81.47%\n",
      "Epoch   1605 Step:      0 Loss:   0.3917 Training Acc:    81.45    81.45    81.45%\n",
      "Epoch   1606 Step:      0 Loss:   0.3917 Training Acc:    81.48    81.48    81.48%\n",
      "Epoch   1607 Step:      0 Loss:   0.3916 Training Acc:    81.45    81.45    81.45%\n",
      "Epoch   1608 Step:      0 Loss:   0.3915 Training Acc:    81.49    81.49    81.49%\n",
      "Epoch   1609 Step:      0 Loss:   0.3914 Training Acc:    81.47    81.47    81.47%\n",
      "Epoch   1610 Step:      0 Loss:   0.3912 Training Acc:    81.50    81.50    81.50%\n",
      "Epoch   1611 Step:      0 Loss:   0.3910 Training Acc:    81.49    81.49    81.49%\n",
      "Epoch   1612 Step:      0 Loss:   0.3909 Training Acc:    81.51    81.51    81.51%\n",
      "Epoch   1613 Step:      0 Loss:   0.3907 Training Acc:    81.52    81.52    81.52%\n",
      "Epoch   1614 Step:      0 Loss:   0.3906 Training Acc:    81.53    81.53    81.53%\n",
      "Epoch   1615 Step:      0 Loss:   0.3906 Training Acc:    81.53    81.53    81.53%\n",
      "Epoch   1616 Step:      0 Loss:   0.3905 Training Acc:    81.52    81.52    81.52%\n",
      "Epoch   1617 Step:      0 Loss:   0.3904 Training Acc:    81.54    81.54    81.54%\n",
      "Epoch   1618 Step:      0 Loss:   0.3903 Training Acc:    81.53    81.53    81.53%\n",
      "Epoch   1619 Step:      0 Loss:   0.3902 Training Acc:    81.56    81.56    81.56%\n",
      "Epoch   1620 Step:      0 Loss:   0.3901 Training Acc:    81.55    81.55    81.55%\n",
      "Epoch   1621 Step:      0 Loss:   0.3900 Training Acc:    81.57    81.57    81.57%\n",
      "Epoch   1622 Step:      0 Loss:   0.3899 Training Acc:    81.56    81.56    81.56%\n",
      "Epoch   1623 Step:      0 Loss:   0.3897 Training Acc:    81.58    81.58    81.58%\n",
      "Epoch   1624 Step:      0 Loss:   0.3896 Training Acc:    81.58    81.58    81.58%\n",
      "Epoch   1625 Step:      0 Loss:   0.3895 Training Acc:    81.60    81.60    81.60%\n",
      "Epoch   1626 Step:      0 Loss:   0.3894 Training Acc:    81.60    81.60    81.60%\n",
      "Epoch   1627 Step:      0 Loss:   0.3893 Training Acc:    81.62    81.62    81.62%\n",
      "Epoch   1628 Step:      0 Loss:   0.3892 Training Acc:    81.62    81.62    81.62%\n",
      "Epoch   1629 Step:      0 Loss:   0.3891 Training Acc:    81.61    81.61    81.61%\n",
      "Epoch   1630 Step:      0 Loss:   0.3890 Training Acc:    81.64    81.64    81.64%\n",
      "Epoch   1631 Step:      0 Loss:   0.3890 Training Acc:    81.60    81.60    81.60%\n",
      "Epoch   1632 Step:      0 Loss:   0.3889 Training Acc:    81.64    81.64    81.64%\n",
      "Epoch   1633 Step:      0 Loss:   0.3889 Training Acc:    81.59    81.59    81.59%\n",
      "Epoch   1634 Step:      0 Loss:   0.3890 Training Acc:    81.65    81.65    81.65%\n",
      "Epoch   1635 Step:      0 Loss:   0.3891 Training Acc:    81.56    81.56    81.56%\n",
      "Epoch   1636 Step:      0 Loss:   0.3894 Training Acc:    81.62    81.62    81.62%\n",
      "Epoch   1637 Step:      0 Loss:   0.3899 Training Acc:    81.49    81.49    81.49%\n",
      "Epoch   1638 Step:      0 Loss:   0.3907 Training Acc:    81.54    81.54    81.54%\n",
      "Epoch   1639 Step:      0 Loss:   0.3918 Training Acc:    81.33    81.33    81.33%\n",
      "Epoch   1640 Step:      0 Loss:   0.3928 Training Acc:    81.41    81.41    81.41%\n",
      "Epoch   1641 Step:      0 Loss:   0.3932 Training Acc:    81.23    81.23    81.23%\n",
      "Epoch   1642 Step:      0 Loss:   0.3921 Training Acc:    81.44    81.44    81.44%\n",
      "Epoch   1643 Step:      0 Loss:   0.3900 Training Acc:    81.47    81.47    81.47%\n",
      "Epoch   1644 Step:      0 Loss:   0.3881 Training Acc:    81.69    81.69    81.69%\n",
      "Epoch   1645 Step:      0 Loss:   0.3875 Training Acc:    81.71    81.71    81.71%\n",
      "Epoch   1646 Step:      0 Loss:   0.3882 Training Acc:    81.62    81.62    81.62%\n",
      "Epoch   1647 Step:      0 Loss:   0.3894 Training Acc:    81.62    81.62    81.62%\n",
      "Epoch   1648 Step:      0 Loss:   0.3898 Training Acc:    81.48    81.48    81.48%\n",
      "Epoch   1649 Step:      0 Loss:   0.3890 Training Acc:    81.64    81.64    81.64%\n",
      "Epoch   1650 Step:      0 Loss:   0.3877 Training Acc:    81.65    81.65    81.65%\n",
      "Epoch   1651 Step:      0 Loss:   0.3869 Training Acc:    81.75    81.75    81.75%\n",
      "Epoch   1652 Step:      0 Loss:   0.3870 Training Acc:    81.75    81.75    81.75%\n",
      "Epoch   1653 Step:      0 Loss:   0.3876 Training Acc:    81.66    81.66    81.66%\n",
      "Epoch   1654 Step:      0 Loss:   0.3880 Training Acc:    81.70    81.70    81.70%\n",
      "Epoch   1655 Step:      0 Loss:   0.3877 Training Acc:    81.66    81.66    81.66%\n",
      "Epoch   1656 Step:      0 Loss:   0.3869 Training Acc:    81.76    81.76    81.76%\n",
      "Epoch   1657 Step:      0 Loss:   0.3863 Training Acc:    81.78    81.78    81.78%\n",
      "Epoch   1658 Step:      0 Loss:   0.3862 Training Acc:    81.77    81.77    81.77%\n",
      "Epoch   1659 Step:      0 Loss:   0.3865 Training Acc:    81.78    81.78    81.78%\n",
      "Epoch   1660 Step:      0 Loss:   0.3867 Training Acc:    81.71    81.71    81.71%\n",
      "Epoch   1661 Step:      0 Loss:   0.3866 Training Acc:    81.78    81.78    81.78%\n",
      "Epoch   1662 Step:      0 Loss:   0.3862 Training Acc:    81.72    81.72    81.72%\n",
      "Epoch   1663 Step:      0 Loss:   0.3857 Training Acc:    81.83    81.83    81.83%\n",
      "Epoch   1664 Step:      0 Loss:   0.3855 Training Acc:    81.81    81.81    81.81%\n",
      "Epoch   1665 Step:      0 Loss:   0.3856 Training Acc:    81.82    81.82    81.82%\n",
      "Epoch   1666 Step:      0 Loss:   0.3858 Training Acc:    81.82    81.82    81.82%\n",
      "Epoch   1667 Step:      0 Loss:   0.3858 Training Acc:    81.80    81.80    81.80%\n",
      "Epoch   1668 Step:      0 Loss:   0.3857 Training Acc:    81.80    81.80    81.80%\n",
      "Epoch   1669 Step:      0 Loss:   0.3854 Training Acc:    81.84    81.84    81.84%\n",
      "Epoch   1670 Step:      0 Loss:   0.3854 Training Acc:    81.80    81.80    81.80%\n",
      "Epoch   1671 Step:      0 Loss:   0.3855 Training Acc:    81.86    81.86    81.86%\n",
      "Epoch   1672 Step:      0 Loss:   0.3859 Training Acc:    81.71    81.71    81.71%\n",
      "Epoch   1673 Step:      0 Loss:   0.3864 Training Acc:    81.80    81.80    81.80%\n",
      "Epoch   1674 Step:      0 Loss:   0.3872 Training Acc:    81.59    81.59    81.59%\n",
      "Epoch   1675 Step:      0 Loss:   0.3879 Training Acc:    81.72    81.72    81.72%\n",
      "Epoch   1676 Step:      0 Loss:   0.3886 Training Acc:    81.50    81.50    81.50%\n",
      "Epoch   1677 Step:      0 Loss:   0.3886 Training Acc:    81.68    81.68    81.68%\n",
      "Epoch   1678 Step:      0 Loss:   0.3878 Training Acc:    81.56    81.56    81.56%\n",
      "Epoch   1679 Step:      0 Loss:   0.3860 Training Acc:    81.83    81.83    81.83%\n",
      "Epoch   1680 Step:      0 Loss:   0.3844 Training Acc:    81.84    81.84    81.84%\n",
      "Epoch   1681 Step:      0 Loss:   0.3839 Training Acc:    81.91    81.91    81.91%\n",
      "Epoch   1682 Step:      0 Loss:   0.3843 Training Acc:    81.92    81.92    81.92%\n",
      "Epoch   1683 Step:      0 Loss:   0.3852 Training Acc:    81.74    81.74    81.74%\n",
      "Epoch   1684 Step:      0 Loss:   0.3857 Training Acc:    81.84    81.84    81.84%\n",
      "Epoch   1685 Step:      0 Loss:   0.3854 Training Acc:    81.70    81.70    81.70%\n",
      "Epoch   1686 Step:      0 Loss:   0.3844 Training Acc:    81.92    81.92    81.92%\n",
      "Epoch   1687 Step:      0 Loss:   0.3835 Training Acc:    81.90    81.90    81.90%\n",
      "Epoch   1688 Step:      0 Loss:   0.3831 Training Acc:    81.97    81.97    81.97%\n",
      "Epoch   1689 Step:      0 Loss:   0.3834 Training Acc:    81.96    81.96    81.96%\n",
      "Epoch   1690 Step:      0 Loss:   0.3839 Training Acc:    81.85    81.85    81.85%\n",
      "Epoch   1691 Step:      0 Loss:   0.3840 Training Acc:    81.94    81.94    81.94%\n",
      "Epoch   1692 Step:      0 Loss:   0.3837 Training Acc:    81.85    81.85    81.85%\n",
      "Epoch   1693 Step:      0 Loss:   0.3831 Training Acc:    81.99    81.99    81.99%\n",
      "Epoch   1694 Step:      0 Loss:   0.3826 Training Acc:    81.96    81.96    81.96%\n",
      "Epoch   1695 Step:      0 Loss:   0.3825 Training Acc:    81.99    81.99    81.99%\n",
      "Epoch   1696 Step:      0 Loss:   0.3826 Training Acc:    82.00    82.00    82.00%\n",
      "Epoch   1697 Step:      0 Loss:   0.3827 Training Acc:    81.94    81.94    81.94%\n",
      "Epoch   1698 Step:      0 Loss:   0.3827 Training Acc:    82.01    82.01    82.01%\n",
      "Epoch   1699 Step:      0 Loss:   0.3825 Training Acc:    81.95    81.95    81.95%\n",
      "Epoch   1700 Step:      0 Loss:   0.3822 Training Acc:    82.04    82.04    82.04%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1701 Step:      0 Loss:   0.3819 Training Acc:    82.01    82.01    82.01%\n",
      "Epoch   1702 Step:      0 Loss:   0.3819 Training Acc:    82.03    82.03    82.03%\n",
      "Epoch   1703 Step:      0 Loss:   0.3818 Training Acc:    82.03    82.03    82.03%\n",
      "Epoch   1704 Step:      0 Loss:   0.3818 Training Acc:    82.01    82.01    82.01%\n",
      "Epoch   1705 Step:      0 Loss:   0.3817 Training Acc:    82.05    82.05    82.05%\n",
      "Epoch   1706 Step:      0 Loss:   0.3815 Training Acc:    82.03    82.03    82.03%\n",
      "Epoch   1707 Step:      0 Loss:   0.3813 Training Acc:    82.08    82.08    82.08%\n",
      "Epoch   1708 Step:      0 Loss:   0.3812 Training Acc:    82.06    82.06    82.06%\n",
      "Epoch   1709 Step:      0 Loss:   0.3811 Training Acc:    82.07    82.07    82.07%\n",
      "Epoch   1710 Step:      0 Loss:   0.3811 Training Acc:    82.07    82.07    82.07%\n",
      "Epoch   1711 Step:      0 Loss:   0.3810 Training Acc:    82.07    82.07    82.07%\n",
      "Epoch   1712 Step:      0 Loss:   0.3809 Training Acc:    82.09    82.09    82.09%\n",
      "Epoch   1713 Step:      0 Loss:   0.3808 Training Acc:    82.08    82.08    82.08%\n",
      "Epoch   1714 Step:      0 Loss:   0.3806 Training Acc:    82.11    82.11    82.11%\n",
      "Epoch   1715 Step:      0 Loss:   0.3805 Training Acc:    82.10    82.10    82.10%\n",
      "Epoch   1716 Step:      0 Loss:   0.3803 Training Acc:    82.12    82.12    82.12%\n",
      "Epoch   1717 Step:      0 Loss:   0.3803 Training Acc:    82.12    82.12    82.12%\n",
      "Epoch   1718 Step:      0 Loss:   0.3802 Training Acc:    82.13    82.13    82.13%\n",
      "Epoch   1719 Step:      0 Loss:   0.3801 Training Acc:    82.13    82.13    82.13%\n",
      "Epoch   1720 Step:      0 Loss:   0.3800 Training Acc:    82.13    82.13    82.13%\n",
      "Epoch   1721 Step:      0 Loss:   0.3799 Training Acc:    82.14    82.14    82.14%\n",
      "Epoch   1722 Step:      0 Loss:   0.3798 Training Acc:    82.15    82.15    82.15%\n",
      "Epoch   1723 Step:      0 Loss:   0.3797 Training Acc:    82.15    82.15    82.15%\n",
      "Epoch   1724 Step:      0 Loss:   0.3796 Training Acc:    82.18    82.18    82.18%\n",
      "Epoch   1725 Step:      0 Loss:   0.3795 Training Acc:    82.15    82.15    82.15%\n",
      "Epoch   1726 Step:      0 Loss:   0.3794 Training Acc:    82.19    82.19    82.19%\n",
      "Epoch   1727 Step:      0 Loss:   0.3793 Training Acc:    82.16    82.16    82.16%\n",
      "Epoch   1728 Step:      0 Loss:   0.3792 Training Acc:    82.19    82.19    82.19%\n",
      "Epoch   1729 Step:      0 Loss:   0.3791 Training Acc:    82.16    82.16    82.16%\n",
      "Epoch   1730 Step:      0 Loss:   0.3790 Training Acc:    82.20    82.20    82.20%\n",
      "Epoch   1731 Step:      0 Loss:   0.3789 Training Acc:    82.16    82.16    82.16%\n",
      "Epoch   1732 Step:      0 Loss:   0.3788 Training Acc:    82.21    82.21    82.21%\n",
      "Epoch   1733 Step:      0 Loss:   0.3787 Training Acc:    82.18    82.18    82.18%\n",
      "Epoch   1734 Step:      0 Loss:   0.3786 Training Acc:    82.23    82.23    82.23%\n",
      "Epoch   1735 Step:      0 Loss:   0.3785 Training Acc:    82.19    82.19    82.19%\n",
      "Epoch   1736 Step:      0 Loss:   0.3784 Training Acc:    82.23    82.23    82.23%\n",
      "Epoch   1737 Step:      0 Loss:   0.3783 Training Acc:    82.20    82.20    82.20%\n",
      "Epoch   1738 Step:      0 Loss:   0.3783 Training Acc:    82.24    82.24    82.24%\n",
      "Epoch   1739 Step:      0 Loss:   0.3782 Training Acc:    82.20    82.20    82.20%\n",
      "Epoch   1740 Step:      0 Loss:   0.3782 Training Acc:    82.25    82.25    82.25%\n",
      "Epoch   1741 Step:      0 Loss:   0.3782 Training Acc:    82.20    82.20    82.20%\n",
      "Epoch   1742 Step:      0 Loss:   0.3782 Training Acc:    82.26    82.26    82.26%\n",
      "Epoch   1743 Step:      0 Loss:   0.3783 Training Acc:    82.18    82.18    82.18%\n",
      "Epoch   1744 Step:      0 Loss:   0.3786 Training Acc:    82.24    82.24    82.24%\n",
      "Epoch   1745 Step:      0 Loss:   0.3790 Training Acc:    82.10    82.10    82.10%\n",
      "Epoch   1746 Step:      0 Loss:   0.3795 Training Acc:    82.17    82.17    82.17%\n",
      "Epoch   1747 Step:      0 Loss:   0.3803 Training Acc:    81.98    81.98    81.98%\n",
      "Epoch   1748 Step:      0 Loss:   0.3812 Training Acc:    82.06    82.06    82.06%\n",
      "Epoch   1749 Step:      0 Loss:   0.3820 Training Acc:    81.88    81.88    81.88%\n",
      "Epoch   1750 Step:      0 Loss:   0.3821 Training Acc:    82.02    82.02    82.02%\n",
      "Epoch   1751 Step:      0 Loss:   0.3815 Training Acc:    81.90    81.90    81.90%\n",
      "Epoch   1752 Step:      0 Loss:   0.3800 Training Acc:    82.15    82.15    82.15%\n",
      "Epoch   1753 Step:      0 Loss:   0.3783 Training Acc:    82.15    82.15    82.15%\n",
      "Epoch   1754 Step:      0 Loss:   0.3772 Training Acc:    82.32    82.32    82.32%\n",
      "Epoch   1755 Step:      0 Loss:   0.3771 Training Acc:    82.27    82.27    82.27%\n",
      "Epoch   1756 Step:      0 Loss:   0.3776 Training Acc:    82.22    82.22    82.22%\n",
      "Epoch   1757 Step:      0 Loss:   0.3781 Training Acc:    82.25    82.25    82.25%\n",
      "Epoch   1758 Step:      0 Loss:   0.3782 Training Acc:    82.13    82.13    82.13%\n",
      "Epoch   1759 Step:      0 Loss:   0.3777 Training Acc:    82.27    82.27    82.27%\n",
      "Epoch   1760 Step:      0 Loss:   0.3769 Training Acc:    82.24    82.24    82.24%\n",
      "Epoch   1761 Step:      0 Loss:   0.3764 Training Acc:    82.36    82.36    82.36%\n",
      "Epoch   1762 Step:      0 Loss:   0.3763 Training Acc:    82.32    82.32    82.32%\n",
      "Epoch   1763 Step:      0 Loss:   0.3765 Training Acc:    82.30    82.30    82.30%\n",
      "Epoch   1764 Step:      0 Loss:   0.3767 Training Acc:    82.31    82.31    82.31%\n",
      "Epoch   1765 Step:      0 Loss:   0.3766 Training Acc:    82.28    82.28    82.28%\n",
      "Epoch   1766 Step:      0 Loss:   0.3763 Training Acc:    82.36    82.36    82.36%\n",
      "Epoch   1767 Step:      0 Loss:   0.3757 Training Acc:    82.34    82.34    82.34%\n",
      "Epoch   1768 Step:      0 Loss:   0.3753 Training Acc:    82.40    82.40    82.40%\n",
      "Epoch   1769 Step:      0 Loss:   0.3752 Training Acc:    82.38    82.38    82.38%\n",
      "Epoch   1770 Step:      0 Loss:   0.3754 Training Acc:    82.39    82.39    82.39%\n",
      "Epoch   1771 Step:      0 Loss:   0.3755 Training Acc:    82.37    82.37    82.37%\n",
      "Epoch   1772 Step:      0 Loss:   0.3755 Training Acc:    82.35    82.35    82.35%\n",
      "Epoch   1773 Step:      0 Loss:   0.3754 Training Acc:    82.39    82.39    82.39%\n",
      "Epoch   1774 Step:      0 Loss:   0.3750 Training Acc:    82.40    82.40    82.40%\n",
      "Epoch   1775 Step:      0 Loss:   0.3747 Training Acc:    82.41    82.41    82.41%\n",
      "Epoch   1776 Step:      0 Loss:   0.3744 Training Acc:    82.45    82.45    82.45%\n",
      "Epoch   1777 Step:      0 Loss:   0.3744 Training Acc:    82.42    82.42    82.42%\n",
      "Epoch   1778 Step:      0 Loss:   0.3744 Training Acc:    82.44    82.44    82.44%\n",
      "Epoch   1779 Step:      0 Loss:   0.3744 Training Acc:    82.42    82.42    82.42%\n",
      "Epoch   1780 Step:      0 Loss:   0.3743 Training Acc:    82.44    82.44    82.44%\n",
      "Epoch   1781 Step:      0 Loss:   0.3742 Training Acc:    82.45    82.45    82.45%\n",
      "Epoch   1782 Step:      0 Loss:   0.3740 Training Acc:    82.44    82.44    82.44%\n",
      "Epoch   1783 Step:      0 Loss:   0.3738 Training Acc:    82.49    82.49    82.49%\n",
      "Epoch   1784 Step:      0 Loss:   0.3737 Training Acc:    82.47    82.47    82.47%\n",
      "Epoch   1785 Step:      0 Loss:   0.3736 Training Acc:    82.49    82.49    82.49%\n",
      "Epoch   1786 Step:      0 Loss:   0.3735 Training Acc:    82.48    82.48    82.48%\n",
      "Epoch   1787 Step:      0 Loss:   0.3735 Training Acc:    82.49    82.49    82.49%\n",
      "Epoch   1788 Step:      0 Loss:   0.3734 Training Acc:    82.48    82.48    82.48%\n",
      "Epoch   1789 Step:      0 Loss:   0.3734 Training Acc:    82.50    82.50    82.50%\n",
      "Epoch   1790 Step:      0 Loss:   0.3733 Training Acc:    82.48    82.48    82.48%\n",
      "Epoch   1791 Step:      0 Loss:   0.3731 Training Acc:    82.51    82.51    82.51%\n",
      "Epoch   1792 Step:      0 Loss:   0.3730 Training Acc:    82.51    82.51    82.51%\n",
      "Epoch   1793 Step:      0 Loss:   0.3729 Training Acc:    82.51    82.51    82.51%\n",
      "Epoch   1794 Step:      0 Loss:   0.3728 Training Acc:    82.51    82.51    82.51%\n",
      "Epoch   1795 Step:      0 Loss:   0.3727 Training Acc:    82.54    82.54    82.54%\n",
      "Epoch   1796 Step:      0 Loss:   0.3727 Training Acc:    82.52    82.52    82.52%\n",
      "Epoch   1797 Step:      0 Loss:   0.3726 Training Acc:    82.57    82.57    82.57%\n",
      "Epoch   1798 Step:      0 Loss:   0.3726 Training Acc:    82.52    82.52    82.52%\n",
      "Epoch   1799 Step:      0 Loss:   0.3726 Training Acc:    82.57    82.57    82.57%\n",
      "Epoch   1800 Step:      0 Loss:   0.3726 Training Acc:    82.52    82.52    82.52%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1801 Step:      0 Loss:   0.3727 Training Acc:    82.58    82.58    82.58%\n",
      "Epoch   1802 Step:      0 Loss:   0.3727 Training Acc:    82.51    82.51    82.51%\n",
      "Epoch   1803 Step:      0 Loss:   0.3728 Training Acc:    82.56    82.56    82.56%\n",
      "Epoch   1804 Step:      0 Loss:   0.3729 Training Acc:    82.50    82.50    82.50%\n",
      "Epoch   1805 Step:      0 Loss:   0.3730 Training Acc:    82.56    82.56    82.56%\n",
      "Epoch   1806 Step:      0 Loss:   0.3730 Training Acc:    82.48    82.48    82.48%\n",
      "Epoch   1807 Step:      0 Loss:   0.3730 Training Acc:    82.57    82.57    82.57%\n",
      "Epoch   1808 Step:      0 Loss:   0.3728 Training Acc:    82.48    82.48    82.48%\n",
      "Epoch   1809 Step:      0 Loss:   0.3726 Training Acc:    82.59    82.59    82.59%\n",
      "Epoch   1810 Step:      0 Loss:   0.3723 Training Acc:    82.53    82.53    82.53%\n",
      "Epoch   1811 Step:      0 Loss:   0.3719 Training Acc:    82.62    82.62    82.62%\n",
      "Epoch   1812 Step:      0 Loss:   0.3716 Training Acc:    82.57    82.57    82.57%\n",
      "Epoch   1813 Step:      0 Loss:   0.3713 Training Acc:    82.63    82.63    82.63%\n",
      "Epoch   1814 Step:      0 Loss:   0.3712 Training Acc:    82.61    82.61    82.61%\n",
      "Epoch   1815 Step:      0 Loss:   0.3710 Training Acc:    82.63    82.63    82.63%\n",
      "Epoch   1816 Step:      0 Loss:   0.3709 Training Acc:    82.64    82.64    82.64%\n",
      "Epoch   1817 Step:      0 Loss:   0.3709 Training Acc:    82.63    82.63    82.63%\n",
      "Epoch   1818 Step:      0 Loss:   0.3707 Training Acc:    82.65    82.65    82.65%\n",
      "Epoch   1819 Step:      0 Loss:   0.3706 Training Acc:    82.63    82.63    82.63%\n",
      "Epoch   1820 Step:      0 Loss:   0.3705 Training Acc:    82.69    82.69    82.69%\n",
      "Epoch   1821 Step:      0 Loss:   0.3704 Training Acc:    82.66    82.66    82.66%\n",
      "Epoch   1822 Step:      0 Loss:   0.3703 Training Acc:    82.69    82.69    82.69%\n",
      "Epoch   1823 Step:      0 Loss:   0.3701 Training Acc:    82.68    82.68    82.68%\n",
      "Epoch   1824 Step:      0 Loss:   0.3700 Training Acc:    82.71    82.71    82.71%\n",
      "Epoch   1825 Step:      0 Loss:   0.3700 Training Acc:    82.68    82.68    82.68%\n",
      "Epoch   1826 Step:      0 Loss:   0.3699 Training Acc:    82.71    82.71    82.71%\n",
      "Epoch   1827 Step:      0 Loss:   0.3699 Training Acc:    82.71    82.71    82.71%\n",
      "Epoch   1828 Step:      0 Loss:   0.3699 Training Acc:    82.74    82.74    82.74%\n",
      "Epoch   1829 Step:      0 Loss:   0.3698 Training Acc:    82.70    82.70    82.70%\n",
      "Epoch   1830 Step:      0 Loss:   0.3698 Training Acc:    82.73    82.73    82.73%\n",
      "Epoch   1831 Step:      0 Loss:   0.3698 Training Acc:    82.70    82.70    82.70%\n",
      "Epoch   1832 Step:      0 Loss:   0.3698 Training Acc:    82.73    82.73    82.73%\n",
      "Epoch   1833 Step:      0 Loss:   0.3698 Training Acc:    82.71    82.71    82.71%\n",
      "Epoch   1834 Step:      0 Loss:   0.3697 Training Acc:    82.73    82.73    82.73%\n",
      "Epoch   1835 Step:      0 Loss:   0.3696 Training Acc:    82.71    82.71    82.71%\n",
      "Epoch   1836 Step:      0 Loss:   0.3695 Training Acc:    82.74    82.74    82.74%\n",
      "Epoch   1837 Step:      0 Loss:   0.3693 Training Acc:    82.73    82.73    82.73%\n",
      "Epoch   1838 Step:      0 Loss:   0.3692 Training Acc:    82.76    82.76    82.76%\n",
      "Epoch   1839 Step:      0 Loss:   0.3690 Training Acc:    82.73    82.73    82.73%\n",
      "Epoch   1840 Step:      0 Loss:   0.3689 Training Acc:    82.78    82.78    82.78%\n",
      "Epoch   1841 Step:      0 Loss:   0.3688 Training Acc:    82.74    82.74    82.74%\n",
      "Epoch   1842 Step:      0 Loss:   0.3687 Training Acc:    82.79    82.79    82.79%\n",
      "Epoch   1843 Step:      0 Loss:   0.3687 Training Acc:    82.77    82.77    82.77%\n",
      "Epoch   1844 Step:      0 Loss:   0.3687 Training Acc:    82.78    82.78    82.78%\n",
      "Epoch   1845 Step:      0 Loss:   0.3687 Training Acc:    82.74    82.74    82.74%\n",
      "Epoch   1846 Step:      0 Loss:   0.3687 Training Acc:    82.79    82.79    82.79%\n",
      "Epoch   1847 Step:      0 Loss:   0.3687 Training Acc:    82.75    82.75    82.75%\n",
      "Epoch   1848 Step:      0 Loss:   0.3686 Training Acc:    82.80    82.80    82.80%\n",
      "Epoch   1849 Step:      0 Loss:   0.3684 Training Acc:    82.77    82.77    82.77%\n",
      "Epoch   1850 Step:      0 Loss:   0.3682 Training Acc:    82.81    82.81    82.81%\n",
      "Epoch   1851 Step:      0 Loss:   0.3678 Training Acc:    82.80    82.80    82.80%\n",
      "Epoch   1852 Step:      0 Loss:   0.3675 Training Acc:    82.84    82.84    82.84%\n",
      "Epoch   1853 Step:      0 Loss:   0.3672 Training Acc:    82.86    82.86    82.86%\n",
      "Epoch   1854 Step:      0 Loss:   0.3670 Training Acc:    82.90    82.90    82.90%\n",
      "Epoch   1855 Step:      0 Loss:   0.3668 Training Acc:    82.90    82.90    82.90%\n",
      "Epoch   1856 Step:      0 Loss:   0.3667 Training Acc:    82.91    82.91    82.91%\n",
      "Epoch   1857 Step:      0 Loss:   0.3666 Training Acc:    82.91    82.91    82.91%\n",
      "Epoch   1858 Step:      0 Loss:   0.3666 Training Acc:    82.90    82.90    82.90%\n",
      "Epoch   1859 Step:      0 Loss:   0.3666 Training Acc:    82.93    82.93    82.93%\n",
      "Epoch   1860 Step:      0 Loss:   0.3666 Training Acc:    82.90    82.90    82.90%\n",
      "Epoch   1861 Step:      0 Loss:   0.3666 Training Acc:    82.90    82.90    82.90%\n",
      "Epoch   1862 Step:      0 Loss:   0.3666 Training Acc:    82.89    82.89    82.89%\n",
      "Epoch   1863 Step:      0 Loss:   0.3665 Training Acc:    82.90    82.90    82.90%\n",
      "Epoch   1864 Step:      0 Loss:   0.3665 Training Acc:    82.89    82.89    82.89%\n",
      "Epoch   1865 Step:      0 Loss:   0.3664 Training Acc:    82.91    82.91    82.91%\n",
      "Epoch   1866 Step:      0 Loss:   0.3663 Training Acc:    82.90    82.90    82.90%\n",
      "Epoch   1867 Step:      0 Loss:   0.3662 Training Acc:    82.92    82.92    82.92%\n",
      "Epoch   1868 Step:      0 Loss:   0.3662 Training Acc:    82.90    82.90    82.90%\n",
      "Epoch   1869 Step:      0 Loss:   0.3661 Training Acc:    82.93    82.93    82.93%\n",
      "Epoch   1870 Step:      0 Loss:   0.3661 Training Acc:    82.89    82.89    82.89%\n",
      "Epoch   1871 Step:      0 Loss:   0.3662 Training Acc:    82.94    82.94    82.94%\n",
      "Epoch   1872 Step:      0 Loss:   0.3665 Training Acc:    82.86    82.86    82.86%\n",
      "Epoch   1873 Step:      0 Loss:   0.3670 Training Acc:    82.89    82.89    82.89%\n",
      "Epoch   1874 Step:      0 Loss:   0.3678 Training Acc:    82.75    82.75    82.75%\n",
      "Epoch   1875 Step:      0 Loss:   0.3688 Training Acc:    82.76    82.76    82.76%\n",
      "Epoch   1876 Step:      0 Loss:   0.3700 Training Acc:    82.59    82.59    82.59%\n",
      "Epoch   1877 Step:      0 Loss:   0.3707 Training Acc:    82.61    82.61    82.61%\n",
      "Epoch   1878 Step:      0 Loss:   0.3703 Training Acc:    82.57    82.57    82.57%\n",
      "Epoch   1879 Step:      0 Loss:   0.3684 Training Acc:    82.78    82.78    82.78%\n",
      "Epoch   1880 Step:      0 Loss:   0.3660 Training Acc:    82.89    82.89    82.89%\n",
      "Epoch   1881 Step:      0 Loss:   0.3645 Training Acc:    83.06    83.06    83.06%\n",
      "Epoch   1882 Step:      0 Loss:   0.3646 Training Acc:    83.03    83.03    83.03%\n",
      "Epoch   1883 Step:      0 Loss:   0.3657 Training Acc:    82.90    82.90    82.90%\n",
      "Epoch   1884 Step:      0 Loss:   0.3667 Training Acc:    82.92    82.92    82.92%\n",
      "Epoch   1885 Step:      0 Loss:   0.3666 Training Acc:    82.82    82.82    82.82%\n",
      "Epoch   1886 Step:      0 Loss:   0.3655 Training Acc:    82.98    82.98    82.98%\n",
      "Epoch   1887 Step:      0 Loss:   0.3642 Training Acc:    83.03    83.03    83.03%\n",
      "Epoch   1888 Step:      0 Loss:   0.3637 Training Acc:    83.09    83.09    83.09%\n",
      "Epoch   1889 Step:      0 Loss:   0.3641 Training Acc:    83.06    83.06    83.06%\n",
      "Epoch   1890 Step:      0 Loss:   0.3648 Training Acc:    82.97    82.97    82.97%\n",
      "Epoch   1891 Step:      0 Loss:   0.3650 Training Acc:    83.01    83.01    83.01%\n",
      "Epoch   1892 Step:      0 Loss:   0.3645 Training Acc:    82.98    82.98    82.98%\n",
      "Epoch   1893 Step:      0 Loss:   0.3637 Training Acc:    83.08    83.08    83.08%\n",
      "Epoch   1894 Step:      0 Loss:   0.3632 Training Acc:    83.12    83.12    83.12%\n",
      "Epoch   1895 Step:      0 Loss:   0.3632 Training Acc:    83.12    83.12    83.12%\n",
      "Epoch   1896 Step:      0 Loss:   0.3635 Training Acc:    83.11    83.11    83.11%\n",
      "Epoch   1897 Step:      0 Loss:   0.3637 Training Acc:    83.04    83.04    83.04%\n",
      "Epoch   1898 Step:      0 Loss:   0.3635 Training Acc:    83.10    83.10    83.10%\n",
      "Epoch   1899 Step:      0 Loss:   0.3631 Training Acc:    83.10    83.10    83.10%\n",
      "Epoch   1900 Step:      0 Loss:   0.3627 Training Acc:    83.16    83.16    83.16%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1901 Step:      0 Loss:   0.3625 Training Acc:    83.16    83.16    83.16%\n",
      "Epoch   1902 Step:      0 Loss:   0.3625 Training Acc:    83.14    83.14    83.14%\n",
      "Epoch   1903 Step:      0 Loss:   0.3626 Training Acc:    83.15    83.15    83.15%\n",
      "Epoch   1904 Step:      0 Loss:   0.3627 Training Acc:    83.12    83.12    83.12%\n",
      "Epoch   1905 Step:      0 Loss:   0.3625 Training Acc:    83.16    83.16    83.16%\n",
      "Epoch   1906 Step:      0 Loss:   0.3622 Training Acc:    83.15    83.15    83.15%\n",
      "Epoch   1907 Step:      0 Loss:   0.3620 Training Acc:    83.18    83.18    83.18%\n",
      "Epoch   1908 Step:      0 Loss:   0.3618 Training Acc:    83.20    83.20    83.20%\n",
      "Epoch   1909 Step:      0 Loss:   0.3618 Training Acc:    83.18    83.18    83.18%\n",
      "Epoch   1910 Step:      0 Loss:   0.3618 Training Acc:    83.19    83.19    83.19%\n",
      "Epoch   1911 Step:      0 Loss:   0.3618 Training Acc:    83.17    83.17    83.17%\n",
      "Epoch   1912 Step:      0 Loss:   0.3617 Training Acc:    83.20    83.20    83.20%\n",
      "Epoch   1913 Step:      0 Loss:   0.3615 Training Acc:    83.19    83.19    83.19%\n",
      "Epoch   1914 Step:      0 Loss:   0.3613 Training Acc:    83.24    83.24    83.24%\n",
      "Epoch   1915 Step:      0 Loss:   0.3612 Training Acc:    83.24    83.24    83.24%\n",
      "Epoch   1916 Step:      0 Loss:   0.3611 Training Acc:    83.26    83.26    83.26%\n",
      "Epoch   1917 Step:      0 Loss:   0.3610 Training Acc:    83.23    83.23    83.23%\n",
      "Epoch   1918 Step:      0 Loss:   0.3610 Training Acc:    83.24    83.24    83.24%\n",
      "Epoch   1919 Step:      0 Loss:   0.3609 Training Acc:    83.23    83.23    83.23%\n",
      "Epoch   1920 Step:      0 Loss:   0.3608 Training Acc:    83.25    83.25    83.25%\n",
      "Epoch   1921 Step:      0 Loss:   0.3607 Training Acc:    83.24    83.24    83.24%\n",
      "Epoch   1922 Step:      0 Loss:   0.3606 Training Acc:    83.27    83.27    83.27%\n",
      "Epoch   1923 Step:      0 Loss:   0.3605 Training Acc:    83.24    83.24    83.24%\n",
      "Epoch   1924 Step:      0 Loss:   0.3604 Training Acc:    83.30    83.30    83.30%\n",
      "Epoch   1925 Step:      0 Loss:   0.3604 Training Acc:    83.22    83.22    83.22%\n",
      "Epoch   1926 Step:      0 Loss:   0.3605 Training Acc:    83.27    83.27    83.27%\n",
      "Epoch   1927 Step:      0 Loss:   0.3607 Training Acc:    83.20    83.20    83.20%\n",
      "Epoch   1928 Step:      0 Loss:   0.3611 Training Acc:    83.23    83.23    83.23%\n",
      "Epoch   1929 Step:      0 Loss:   0.3618 Training Acc:    83.12    83.12    83.12%\n",
      "Epoch   1930 Step:      0 Loss:   0.3629 Training Acc:    83.12    83.12    83.12%\n",
      "Epoch   1931 Step:      0 Loss:   0.3645 Training Acc:    82.90    82.90    82.90%\n",
      "Epoch   1932 Step:      0 Loss:   0.3661 Training Acc:    82.88    82.88    82.88%\n",
      "Epoch   1933 Step:      0 Loss:   0.3670 Training Acc:    82.72    82.72    82.72%\n",
      "Epoch   1934 Step:      0 Loss:   0.3654 Training Acc:    82.92    82.92    82.92%\n",
      "Epoch   1935 Step:      0 Loss:   0.3624 Training Acc:    83.08    83.08    83.08%\n",
      "Epoch   1936 Step:      0 Loss:   0.3599 Training Acc:    83.30    83.30    83.30%\n",
      "Epoch   1937 Step:      0 Loss:   0.3596 Training Acc:    83.31    83.31    83.31%\n",
      "Epoch   1938 Step:      0 Loss:   0.3611 Training Acc:    83.14    83.14    83.14%\n",
      "Epoch   1939 Step:      0 Loss:   0.3624 Training Acc:    83.10    83.10    83.10%\n",
      "Epoch   1940 Step:      0 Loss:   0.3623 Training Acc:    83.07    83.07    83.07%\n",
      "Epoch   1941 Step:      0 Loss:   0.3606 Training Acc:    83.25    83.25    83.25%\n",
      "Epoch   1942 Step:      0 Loss:   0.3591 Training Acc:    83.31    83.31    83.31%\n",
      "Epoch   1943 Step:      0 Loss:   0.3591 Training Acc:    83.32    83.32    83.32%\n",
      "Epoch   1944 Step:      0 Loss:   0.3601 Training Acc:    83.27    83.27    83.27%\n",
      "Epoch   1945 Step:      0 Loss:   0.3608 Training Acc:    83.16    83.16    83.16%\n",
      "Epoch   1946 Step:      0 Loss:   0.3601 Training Acc:    83.28    83.28    83.28%\n",
      "Epoch   1947 Step:      0 Loss:   0.3589 Training Acc:    83.31    83.31    83.31%\n",
      "Epoch   1948 Step:      0 Loss:   0.3582 Training Acc:    83.39    83.39    83.39%\n",
      "Epoch   1949 Step:      0 Loss:   0.3585 Training Acc:    83.39    83.39    83.39%\n",
      "Epoch   1950 Step:      0 Loss:   0.3591 Training Acc:    83.29    83.29    83.29%\n",
      "Epoch   1951 Step:      0 Loss:   0.3592 Training Acc:    83.34    83.34    83.34%\n",
      "Epoch   1952 Step:      0 Loss:   0.3587 Training Acc:    83.31    83.31    83.31%\n",
      "Epoch   1953 Step:      0 Loss:   0.3580 Training Acc:    83.41    83.41    83.41%\n",
      "Epoch   1954 Step:      0 Loss:   0.3577 Training Acc:    83.40    83.40    83.40%\n",
      "Epoch   1955 Step:      0 Loss:   0.3579 Training Acc:    83.37    83.37    83.37%\n",
      "Epoch   1956 Step:      0 Loss:   0.3582 Training Acc:    83.39    83.39    83.39%\n",
      "Epoch   1957 Step:      0 Loss:   0.3581 Training Acc:    83.35    83.35    83.35%\n",
      "Epoch   1958 Step:      0 Loss:   0.3577 Training Acc:    83.41    83.41    83.41%\n",
      "Epoch   1959 Step:      0 Loss:   0.3573 Training Acc:    83.42    83.42    83.42%\n",
      "Epoch   1960 Step:      0 Loss:   0.3571 Training Acc:    83.45    83.45    83.45%\n",
      "Epoch   1961 Step:      0 Loss:   0.3572 Training Acc:    83.45    83.45    83.45%\n",
      "Epoch   1962 Step:      0 Loss:   0.3573 Training Acc:    83.40    83.40    83.40%\n",
      "Epoch   1963 Step:      0 Loss:   0.3573 Training Acc:    83.43    83.43    83.43%\n",
      "Epoch   1964 Step:      0 Loss:   0.3570 Training Acc:    83.42    83.42    83.42%\n",
      "Epoch   1965 Step:      0 Loss:   0.3567 Training Acc:    83.49    83.49    83.49%\n",
      "Epoch   1966 Step:      0 Loss:   0.3566 Training Acc:    83.49    83.49    83.49%\n",
      "Epoch   1967 Step:      0 Loss:   0.3566 Training Acc:    83.46    83.46    83.46%\n",
      "Epoch   1968 Step:      0 Loss:   0.3566 Training Acc:    83.48    83.48    83.48%\n",
      "Epoch   1969 Step:      0 Loss:   0.3565 Training Acc:    83.44    83.44    83.44%\n",
      "Epoch   1970 Step:      0 Loss:   0.3564 Training Acc:    83.50    83.50    83.50%\n",
      "Epoch   1971 Step:      0 Loss:   0.3562 Training Acc:    83.48    83.48    83.48%\n",
      "Epoch   1972 Step:      0 Loss:   0.3560 Training Acc:    83.52    83.52    83.52%\n",
      "Epoch   1973 Step:      0 Loss:   0.3560 Training Acc:    83.54    83.54    83.54%\n",
      "Epoch   1974 Step:      0 Loss:   0.3559 Training Acc:    83.51    83.51    83.51%\n",
      "Epoch   1975 Step:      0 Loss:   0.3559 Training Acc:    83.52    83.52    83.52%\n",
      "Epoch   1976 Step:      0 Loss:   0.3558 Training Acc:    83.50    83.50    83.50%\n",
      "Epoch   1977 Step:      0 Loss:   0.3556 Training Acc:    83.55    83.55    83.55%\n",
      "Epoch   1978 Step:      0 Loss:   0.3555 Training Acc:    83.54    83.54    83.54%\n",
      "Epoch   1979 Step:      0 Loss:   0.3554 Training Acc:    83.54    83.54    83.54%\n",
      "Epoch   1980 Step:      0 Loss:   0.3553 Training Acc:    83.56    83.56    83.56%\n",
      "Epoch   1981 Step:      0 Loss:   0.3553 Training Acc:    83.54    83.54    83.54%\n",
      "Epoch   1982 Step:      0 Loss:   0.3552 Training Acc:    83.56    83.56    83.56%\n",
      "Epoch   1983 Step:      0 Loss:   0.3551 Training Acc:    83.55    83.55    83.55%\n",
      "Epoch   1984 Step:      0 Loss:   0.3550 Training Acc:    83.58    83.58    83.58%\n",
      "Epoch   1985 Step:      0 Loss:   0.3549 Training Acc:    83.56    83.56    83.56%\n",
      "Epoch   1986 Step:      0 Loss:   0.3548 Training Acc:    83.58    83.58    83.58%\n",
      "Epoch   1987 Step:      0 Loss:   0.3547 Training Acc:    83.58    83.58    83.58%\n",
      "Epoch   1988 Step:      0 Loss:   0.3547 Training Acc:    83.58    83.58    83.58%\n",
      "Epoch   1989 Step:      0 Loss:   0.3546 Training Acc:    83.58    83.58    83.58%\n",
      "Epoch   1990 Step:      0 Loss:   0.3546 Training Acc:    83.59    83.59    83.59%\n",
      "Epoch   1991 Step:      0 Loss:   0.3545 Training Acc:    83.57    83.57    83.57%\n",
      "Epoch   1992 Step:      0 Loss:   0.3545 Training Acc:    83.61    83.61    83.61%\n",
      "Epoch   1993 Step:      0 Loss:   0.3546 Training Acc:    83.57    83.57    83.57%\n",
      "Epoch   1994 Step:      0 Loss:   0.3547 Training Acc:    83.59    83.59    83.59%\n",
      "Epoch   1995 Step:      0 Loss:   0.3550 Training Acc:    83.54    83.54    83.54%\n",
      "Epoch   1996 Step:      0 Loss:   0.3555 Training Acc:    83.54    83.54    83.54%\n",
      "Epoch   1997 Step:      0 Loss:   0.3562 Training Acc:    83.42    83.42    83.42%\n",
      "Epoch   1998 Step:      0 Loss:   0.3572 Training Acc:    83.43    83.43    83.43%\n",
      "Epoch   1999 Step:      0 Loss:   0.3584 Training Acc:    83.26    83.26    83.26%\n",
      "Epoch   2000 Step:      0 Loss:   0.3593 Training Acc:    83.28    83.28    83.28%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2001 Step:      0 Loss:   0.3596 Training Acc:    83.16    83.16    83.16%\n",
      "Epoch   2002 Step:      0 Loss:   0.3584 Training Acc:    83.35    83.35    83.35%\n",
      "Epoch   2003 Step:      0 Loss:   0.3563 Training Acc:    83.40    83.40    83.40%\n",
      "Epoch   2004 Step:      0 Loss:   0.3542 Training Acc:    83.61    83.61    83.61%\n",
      "Epoch   2005 Step:      0 Loss:   0.3532 Training Acc:    83.67    83.67    83.67%\n",
      "Epoch   2006 Step:      0 Loss:   0.3537 Training Acc:    83.64    83.64    83.64%\n",
      "Epoch   2007 Step:      0 Loss:   0.3547 Training Acc:    83.57    83.57    83.57%\n",
      "Epoch   2008 Step:      0 Loss:   0.3554 Training Acc:    83.48    83.48    83.48%\n",
      "Epoch   2009 Step:      0 Loss:   0.3552 Training Acc:    83.55    83.55    83.55%\n",
      "Epoch   2010 Step:      0 Loss:   0.3542 Training Acc:    83.56    83.56    83.56%\n",
      "Epoch   2011 Step:      0 Loss:   0.3532 Training Acc:    83.66    83.66    83.66%\n",
      "Epoch   2012 Step:      0 Loss:   0.3528 Training Acc:    83.67    83.67    83.67%\n",
      "Epoch   2013 Step:      0 Loss:   0.3529 Training Acc:    83.69    83.69    83.69%\n",
      "Epoch   2014 Step:      0 Loss:   0.3533 Training Acc:    83.65    83.65    83.65%\n",
      "Epoch   2015 Step:      0 Loss:   0.3535 Training Acc:    83.63    83.63    83.63%\n",
      "Epoch   2016 Step:      0 Loss:   0.3532 Training Acc:    83.68    83.68    83.68%\n",
      "Epoch   2017 Step:      0 Loss:   0.3528 Training Acc:    83.65    83.65    83.65%\n",
      "Epoch   2018 Step:      0 Loss:   0.3524 Training Acc:    83.71    83.71    83.71%\n",
      "Epoch   2019 Step:      0 Loss:   0.3522 Training Acc:    83.69    83.69    83.69%\n",
      "Epoch   2020 Step:      0 Loss:   0.3522 Training Acc:    83.72    83.72    83.72%\n",
      "Epoch   2021 Step:      0 Loss:   0.3523 Training Acc:    83.71    83.71    83.71%\n",
      "Epoch   2022 Step:      0 Loss:   0.3522 Training Acc:    83.72    83.72    83.72%\n",
      "Epoch   2023 Step:      0 Loss:   0.3520 Training Acc:    83.74    83.74    83.74%\n",
      "Epoch   2024 Step:      0 Loss:   0.3517 Training Acc:    83.72    83.72    83.72%\n",
      "Epoch   2025 Step:      0 Loss:   0.3516 Training Acc:    83.75    83.75    83.75%\n",
      "Epoch   2026 Step:      0 Loss:   0.3515 Training Acc:    83.73    83.73    83.73%\n",
      "Epoch   2027 Step:      0 Loss:   0.3515 Training Acc:    83.75    83.75    83.75%\n",
      "Epoch   2028 Step:      0 Loss:   0.3515 Training Acc:    83.76    83.76    83.76%\n",
      "Epoch   2029 Step:      0 Loss:   0.3514 Training Acc:    83.76    83.76    83.76%\n",
      "Epoch   2030 Step:      0 Loss:   0.3512 Training Acc:    83.77    83.77    83.77%\n",
      "Epoch   2031 Step:      0 Loss:   0.3510 Training Acc:    83.77    83.77    83.77%\n",
      "Epoch   2032 Step:      0 Loss:   0.3508 Training Acc:    83.81    83.81    83.81%\n",
      "Epoch   2033 Step:      0 Loss:   0.3507 Training Acc:    83.81    83.81    83.81%\n",
      "Epoch   2034 Step:      0 Loss:   0.3507 Training Acc:    83.80    83.80    83.80%\n",
      "Epoch   2035 Step:      0 Loss:   0.3507 Training Acc:    83.80    83.80    83.80%\n",
      "Epoch   2036 Step:      0 Loss:   0.3507 Training Acc:    83.79    83.79    83.79%\n",
      "Epoch   2037 Step:      0 Loss:   0.3506 Training Acc:    83.81    83.81    83.81%\n",
      "Epoch   2038 Step:      0 Loss:   0.3504 Training Acc:    83.82    83.82    83.82%\n",
      "Epoch   2039 Step:      0 Loss:   0.3503 Training Acc:    83.84    83.84    83.84%\n",
      "Epoch   2040 Step:      0 Loss:   0.3501 Training Acc:    83.82    83.82    83.82%\n",
      "Epoch   2041 Step:      0 Loss:   0.3500 Training Acc:    83.85    83.85    83.85%\n",
      "Epoch   2042 Step:      0 Loss:   0.3499 Training Acc:    83.86    83.86    83.86%\n",
      "Epoch   2043 Step:      0 Loss:   0.3499 Training Acc:    83.84    83.84    83.84%\n",
      "Epoch   2044 Step:      0 Loss:   0.3498 Training Acc:    83.87    83.87    83.87%\n",
      "Epoch   2045 Step:      0 Loss:   0.3498 Training Acc:    83.85    83.85    83.85%\n",
      "Epoch   2046 Step:      0 Loss:   0.3497 Training Acc:    83.87    83.87    83.87%\n",
      "Epoch   2047 Step:      0 Loss:   0.3496 Training Acc:    83.85    83.85    83.85%\n",
      "Epoch   2048 Step:      0 Loss:   0.3495 Training Acc:    83.86    83.86    83.86%\n",
      "Epoch   2049 Step:      0 Loss:   0.3494 Training Acc:    83.86    83.86    83.86%\n",
      "Epoch   2050 Step:      0 Loss:   0.3493 Training Acc:    83.87    83.87    83.87%\n",
      "Epoch   2051 Step:      0 Loss:   0.3492 Training Acc:    83.88    83.88    83.88%\n",
      "Epoch   2052 Step:      0 Loss:   0.3491 Training Acc:    83.88    83.88    83.88%\n",
      "Epoch   2053 Step:      0 Loss:   0.3490 Training Acc:    83.90    83.90    83.90%\n",
      "Epoch   2054 Step:      0 Loss:   0.3489 Training Acc:    83.89    83.89    83.89%\n",
      "Epoch   2055 Step:      0 Loss:   0.3488 Training Acc:    83.90    83.90    83.90%\n",
      "Epoch   2056 Step:      0 Loss:   0.3488 Training Acc:    83.90    83.90    83.90%\n",
      "Epoch   2057 Step:      0 Loss:   0.3487 Training Acc:    83.91    83.91    83.91%\n",
      "Epoch   2058 Step:      0 Loss:   0.3486 Training Acc:    83.91    83.91    83.91%\n",
      "Epoch   2059 Step:      0 Loss:   0.3485 Training Acc:    83.92    83.92    83.92%\n",
      "Epoch   2060 Step:      0 Loss:   0.3484 Training Acc:    83.91    83.91    83.91%\n",
      "Epoch   2061 Step:      0 Loss:   0.3483 Training Acc:    83.93    83.93    83.93%\n",
      "Epoch   2062 Step:      0 Loss:   0.3482 Training Acc:    83.92    83.92    83.92%\n",
      "Epoch   2063 Step:      0 Loss:   0.3481 Training Acc:    83.93    83.93    83.93%\n",
      "Epoch   2064 Step:      0 Loss:   0.3480 Training Acc:    83.94    83.94    83.94%\n",
      "Epoch   2065 Step:      0 Loss:   0.3480 Training Acc:    83.94    83.94    83.94%\n",
      "Epoch   2066 Step:      0 Loss:   0.3479 Training Acc:    83.93    83.93    83.93%\n",
      "Epoch   2067 Step:      0 Loss:   0.3478 Training Acc:    83.96    83.96    83.96%\n",
      "Epoch   2068 Step:      0 Loss:   0.3478 Training Acc:    83.94    83.94    83.94%\n",
      "Epoch   2069 Step:      0 Loss:   0.3477 Training Acc:    83.96    83.96    83.96%\n",
      "Epoch   2070 Step:      0 Loss:   0.3477 Training Acc:    83.94    83.94    83.94%\n",
      "Epoch   2071 Step:      0 Loss:   0.3477 Training Acc:    83.95    83.95    83.95%\n",
      "Epoch   2072 Step:      0 Loss:   0.3478 Training Acc:    83.92    83.92    83.92%\n",
      "Epoch   2073 Step:      0 Loss:   0.3479 Training Acc:    83.95    83.95    83.95%\n",
      "Epoch   2074 Step:      0 Loss:   0.3481 Training Acc:    83.89    83.89    83.89%\n",
      "Epoch   2075 Step:      0 Loss:   0.3485 Training Acc:    83.93    83.93    83.93%\n",
      "Epoch   2076 Step:      0 Loss:   0.3491 Training Acc:    83.79    83.79    83.79%\n",
      "Epoch   2077 Step:      0 Loss:   0.3500 Training Acc:    83.84    83.84    83.84%\n",
      "Epoch   2078 Step:      0 Loss:   0.3510 Training Acc:    83.62    83.62    83.62%\n",
      "Epoch   2079 Step:      0 Loss:   0.3521 Training Acc:    83.70    83.70    83.70%\n",
      "Epoch   2080 Step:      0 Loss:   0.3529 Training Acc:    83.49    83.49    83.49%\n",
      "Epoch   2081 Step:      0 Loss:   0.3528 Training Acc:    83.64    83.64    83.64%\n",
      "Epoch   2082 Step:      0 Loss:   0.3518 Training Acc:    83.62    83.62    83.62%\n",
      "Epoch   2083 Step:      0 Loss:   0.3499 Training Acc:    83.83    83.83    83.83%\n",
      "Epoch   2084 Step:      0 Loss:   0.3482 Training Acc:    83.90    83.90    83.90%\n",
      "Epoch   2085 Step:      0 Loss:   0.3473 Training Acc:    83.94    83.94    83.94%\n",
      "Epoch   2086 Step:      0 Loss:   0.3474 Training Acc:    83.98    83.98    83.98%\n",
      "Epoch   2087 Step:      0 Loss:   0.3481 Training Acc:    83.82    83.82    83.82%\n",
      "Epoch   2088 Step:      0 Loss:   0.3485 Training Acc:    83.91    83.91    83.91%\n",
      "Epoch   2089 Step:      0 Loss:   0.3482 Training Acc:    83.84    83.84    83.84%\n",
      "Epoch   2090 Step:      0 Loss:   0.3474 Training Acc:    83.99    83.99    83.99%\n",
      "Epoch   2091 Step:      0 Loss:   0.3466 Training Acc:    83.99    83.99    83.99%\n",
      "Epoch   2092 Step:      0 Loss:   0.3463 Training Acc:    84.02    84.02    84.02%\n",
      "Epoch   2093 Step:      0 Loss:   0.3465 Training Acc:    84.02    84.02    84.02%\n",
      "Epoch   2094 Step:      0 Loss:   0.3469 Training Acc:    83.94    83.94    83.94%\n",
      "Epoch   2095 Step:      0 Loss:   0.3469 Training Acc:    84.01    84.01    84.01%\n",
      "Epoch   2096 Step:      0 Loss:   0.3466 Training Acc:    83.95    83.95    83.95%\n",
      "Epoch   2097 Step:      0 Loss:   0.3459 Training Acc:    84.07    84.07    84.07%\n",
      "Epoch   2098 Step:      0 Loss:   0.3454 Training Acc:    84.06    84.06    84.06%\n",
      "Epoch   2099 Step:      0 Loss:   0.3452 Training Acc:    84.09    84.09    84.09%\n",
      "Epoch   2100 Step:      0 Loss:   0.3454 Training Acc:    84.07    84.07    84.07%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2101 Step:      0 Loss:   0.3457 Training Acc:    84.02    84.02    84.02%\n",
      "Epoch   2102 Step:      0 Loss:   0.3458 Training Acc:    84.06    84.06    84.06%\n",
      "Epoch   2103 Step:      0 Loss:   0.3455 Training Acc:    84.03    84.03    84.03%\n",
      "Epoch   2104 Step:      0 Loss:   0.3451 Training Acc:    84.10    84.10    84.10%\n",
      "Epoch   2105 Step:      0 Loss:   0.3447 Training Acc:    84.12    84.12    84.12%\n",
      "Epoch   2106 Step:      0 Loss:   0.3445 Training Acc:    84.13    84.13    84.13%\n",
      "Epoch   2107 Step:      0 Loss:   0.3445 Training Acc:    84.14    84.14    84.14%\n",
      "Epoch   2108 Step:      0 Loss:   0.3446 Training Acc:    84.11    84.11    84.11%\n",
      "Epoch   2109 Step:      0 Loss:   0.3447 Training Acc:    84.12    84.12    84.12%\n",
      "Epoch   2110 Step:      0 Loss:   0.3446 Training Acc:    84.09    84.09    84.09%\n",
      "Epoch   2111 Step:      0 Loss:   0.3444 Training Acc:    84.14    84.14    84.14%\n",
      "Epoch   2112 Step:      0 Loss:   0.3442 Training Acc:    84.15    84.15    84.15%\n",
      "Epoch   2113 Step:      0 Loss:   0.3440 Training Acc:    84.17    84.17    84.17%\n",
      "Epoch   2114 Step:      0 Loss:   0.3439 Training Acc:    84.15    84.15    84.15%\n",
      "Epoch   2115 Step:      0 Loss:   0.3438 Training Acc:    84.16    84.16    84.16%\n",
      "Epoch   2116 Step:      0 Loss:   0.3438 Training Acc:    84.17    84.17    84.17%\n",
      "Epoch   2117 Step:      0 Loss:   0.3438 Training Acc:    84.15    84.15    84.15%\n",
      "Epoch   2118 Step:      0 Loss:   0.3437 Training Acc:    84.18    84.18    84.18%\n",
      "Epoch   2119 Step:      0 Loss:   0.3435 Training Acc:    84.17    84.17    84.17%\n",
      "Epoch   2120 Step:      0 Loss:   0.3434 Training Acc:    84.19    84.19    84.19%\n",
      "Epoch   2121 Step:      0 Loss:   0.3432 Training Acc:    84.19    84.19    84.19%\n",
      "Epoch   2122 Step:      0 Loss:   0.3431 Training Acc:    84.20    84.20    84.20%\n",
      "Epoch   2123 Step:      0 Loss:   0.3431 Training Acc:    84.20    84.20    84.20%\n",
      "Epoch   2124 Step:      0 Loss:   0.3430 Training Acc:    84.19    84.19    84.19%\n",
      "Epoch   2125 Step:      0 Loss:   0.3430 Training Acc:    84.21    84.21    84.21%\n",
      "Epoch   2126 Step:      0 Loss:   0.3429 Training Acc:    84.21    84.21    84.21%\n",
      "Epoch   2127 Step:      0 Loss:   0.3428 Training Acc:    84.22    84.22    84.22%\n",
      "Epoch   2128 Step:      0 Loss:   0.3427 Training Acc:    84.23    84.23    84.23%\n",
      "Epoch   2129 Step:      0 Loss:   0.3426 Training Acc:    84.24    84.24    84.24%\n",
      "Epoch   2130 Step:      0 Loss:   0.3425 Training Acc:    84.24    84.24    84.24%\n",
      "Epoch   2131 Step:      0 Loss:   0.3424 Training Acc:    84.24    84.24    84.24%\n",
      "Epoch   2132 Step:      0 Loss:   0.3423 Training Acc:    84.25    84.25    84.25%\n",
      "Epoch   2133 Step:      0 Loss:   0.3423 Training Acc:    84.25    84.25    84.25%\n",
      "Epoch   2134 Step:      0 Loss:   0.3422 Training Acc:    84.26    84.26    84.26%\n",
      "Epoch   2135 Step:      0 Loss:   0.3422 Training Acc:    84.25    84.25    84.25%\n",
      "Epoch   2136 Step:      0 Loss:   0.3421 Training Acc:    84.26    84.26    84.26%\n",
      "Epoch   2137 Step:      0 Loss:   0.3421 Training Acc:    84.26    84.26    84.26%\n",
      "Epoch   2138 Step:      0 Loss:   0.3420 Training Acc:    84.26    84.26    84.26%\n",
      "Epoch   2139 Step:      0 Loss:   0.3420 Training Acc:    84.26    84.26    84.26%\n",
      "Epoch   2140 Step:      0 Loss:   0.3420 Training Acc:    84.26    84.26    84.26%\n",
      "Epoch   2141 Step:      0 Loss:   0.3420 Training Acc:    84.26    84.26    84.26%\n",
      "Epoch   2142 Step:      0 Loss:   0.3422 Training Acc:    84.23    84.23    84.23%\n",
      "Epoch   2143 Step:      0 Loss:   0.3424 Training Acc:    84.24    84.24    84.24%\n",
      "Epoch   2144 Step:      0 Loss:   0.3427 Training Acc:    84.19    84.19    84.19%\n",
      "Epoch   2145 Step:      0 Loss:   0.3433 Training Acc:    84.19    84.19    84.19%\n",
      "Epoch   2146 Step:      0 Loss:   0.3439 Training Acc:    84.12    84.12    84.12%\n",
      "Epoch   2147 Step:      0 Loss:   0.3446 Training Acc:    84.10    84.10    84.10%\n",
      "Epoch   2148 Step:      0 Loss:   0.3453 Training Acc:    84.02    84.02    84.02%\n",
      "Epoch   2149 Step:      0 Loss:   0.3453 Training Acc:    84.03    84.03    84.03%\n",
      "Epoch   2150 Step:      0 Loss:   0.3448 Training Acc:    84.04    84.04    84.04%\n",
      "Epoch   2151 Step:      0 Loss:   0.3435 Training Acc:    84.18    84.18    84.18%\n",
      "Epoch   2152 Step:      0 Loss:   0.3420 Training Acc:    84.21    84.21    84.21%\n",
      "Epoch   2153 Step:      0 Loss:   0.3410 Training Acc:    84.33    84.33    84.33%\n",
      "Epoch   2154 Step:      0 Loss:   0.3407 Training Acc:    84.33    84.33    84.33%\n",
      "Epoch   2155 Step:      0 Loss:   0.3411 Training Acc:    84.31    84.31    84.31%\n",
      "Epoch   2156 Step:      0 Loss:   0.3417 Training Acc:    84.24    84.24    84.24%\n",
      "Epoch   2157 Step:      0 Loss:   0.3421 Training Acc:    84.21    84.21    84.21%\n",
      "Epoch   2158 Step:      0 Loss:   0.3419 Training Acc:    84.24    84.24    84.24%\n",
      "Epoch   2159 Step:      0 Loss:   0.3413 Training Acc:    84.25    84.25    84.25%\n",
      "Epoch   2160 Step:      0 Loss:   0.3406 Training Acc:    84.34    84.34    84.34%\n",
      "Epoch   2161 Step:      0 Loss:   0.3401 Training Acc:    84.35    84.35    84.35%\n",
      "Epoch   2162 Step:      0 Loss:   0.3400 Training Acc:    84.39    84.39    84.39%\n",
      "Epoch   2163 Step:      0 Loss:   0.3402 Training Acc:    84.35    84.35    84.35%\n",
      "Epoch   2164 Step:      0 Loss:   0.3404 Training Acc:    84.35    84.35    84.35%\n",
      "Epoch   2165 Step:      0 Loss:   0.3406 Training Acc:    84.32    84.32    84.32%\n",
      "Epoch   2166 Step:      0 Loss:   0.3404 Training Acc:    84.34    84.34    84.34%\n",
      "Epoch   2167 Step:      0 Loss:   0.3401 Training Acc:    84.37    84.37    84.37%\n",
      "Epoch   2168 Step:      0 Loss:   0.3396 Training Acc:    84.40    84.40    84.40%\n",
      "Epoch   2169 Step:      0 Loss:   0.3393 Training Acc:    84.42    84.42    84.42%\n",
      "Epoch   2170 Step:      0 Loss:   0.3392 Training Acc:    84.43    84.43    84.43%\n",
      "Epoch   2171 Step:      0 Loss:   0.3392 Training Acc:    84.40    84.40    84.40%\n",
      "Epoch   2172 Step:      0 Loss:   0.3393 Training Acc:    84.41    84.41    84.41%\n",
      "Epoch   2173 Step:      0 Loss:   0.3394 Training Acc:    84.40    84.40    84.40%\n",
      "Epoch   2174 Step:      0 Loss:   0.3393 Training Acc:    84.42    84.42    84.42%\n",
      "Epoch   2175 Step:      0 Loss:   0.3391 Training Acc:    84.42    84.42    84.42%\n",
      "Epoch   2176 Step:      0 Loss:   0.3389 Training Acc:    84.44    84.44    84.44%\n",
      "Epoch   2177 Step:      0 Loss:   0.3387 Training Acc:    84.44    84.44    84.44%\n",
      "Epoch   2178 Step:      0 Loss:   0.3386 Training Acc:    84.47    84.47    84.47%\n",
      "Epoch   2179 Step:      0 Loss:   0.3385 Training Acc:    84.48    84.48    84.48%\n",
      "Epoch   2180 Step:      0 Loss:   0.3384 Training Acc:    84.45    84.45    84.45%\n",
      "Epoch   2181 Step:      0 Loss:   0.3384 Training Acc:    84.45    84.45    84.45%\n",
      "Epoch   2182 Step:      0 Loss:   0.3384 Training Acc:    84.44    84.44    84.44%\n",
      "Epoch   2183 Step:      0 Loss:   0.3384 Training Acc:    84.46    84.46    84.46%\n",
      "Epoch   2184 Step:      0 Loss:   0.3383 Training Acc:    84.45    84.45    84.45%\n",
      "Epoch   2185 Step:      0 Loss:   0.3381 Training Acc:    84.48    84.48    84.48%\n",
      "Epoch   2186 Step:      0 Loss:   0.3380 Training Acc:    84.47    84.47    84.47%\n",
      "Epoch   2187 Step:      0 Loss:   0.3379 Training Acc:    84.51    84.51    84.51%\n",
      "Epoch   2188 Step:      0 Loss:   0.3378 Training Acc:    84.50    84.50    84.50%\n",
      "Epoch   2189 Step:      0 Loss:   0.3377 Training Acc:    84.52    84.52    84.52%\n",
      "Epoch   2190 Step:      0 Loss:   0.3377 Training Acc:    84.49    84.49    84.49%\n",
      "Epoch   2191 Step:      0 Loss:   0.3377 Training Acc:    84.52    84.52    84.52%\n",
      "Epoch   2192 Step:      0 Loss:   0.3377 Training Acc:    84.48    84.48    84.48%\n",
      "Epoch   2193 Step:      0 Loss:   0.3377 Training Acc:    84.52    84.52    84.52%\n",
      "Epoch   2194 Step:      0 Loss:   0.3378 Training Acc:    84.47    84.47    84.47%\n",
      "Epoch   2195 Step:      0 Loss:   0.3379 Training Acc:    84.51    84.51    84.51%\n",
      "Epoch   2196 Step:      0 Loss:   0.3381 Training Acc:    84.44    84.44    84.44%\n",
      "Epoch   2197 Step:      0 Loss:   0.3383 Training Acc:    84.48    84.48    84.48%\n",
      "Epoch   2198 Step:      0 Loss:   0.3387 Training Acc:    84.37    84.37    84.37%\n",
      "Epoch   2199 Step:      0 Loss:   0.3392 Training Acc:    84.44    84.44    84.44%\n",
      "Epoch   2200 Step:      0 Loss:   0.3397 Training Acc:    84.29    84.29    84.29%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2201 Step:      0 Loss:   0.3401 Training Acc:    84.38    84.38    84.38%\n",
      "Epoch   2202 Step:      0 Loss:   0.3404 Training Acc:    84.23    84.23    84.23%\n",
      "Epoch   2203 Step:      0 Loss:   0.3401 Training Acc:    84.39    84.39    84.39%\n",
      "Epoch   2204 Step:      0 Loss:   0.3396 Training Acc:    84.30    84.30    84.30%\n",
      "Epoch   2205 Step:      0 Loss:   0.3388 Training Acc:    84.46    84.46    84.46%\n",
      "Epoch   2206 Step:      0 Loss:   0.3380 Training Acc:    84.44    84.44    84.44%\n",
      "Epoch   2207 Step:      0 Loss:   0.3374 Training Acc:    84.50    84.50    84.50%\n",
      "Epoch   2208 Step:      0 Loss:   0.3371 Training Acc:    84.53    84.53    84.53%\n",
      "Epoch   2209 Step:      0 Loss:   0.3370 Training Acc:    84.50    84.50    84.50%\n",
      "Epoch   2210 Step:      0 Loss:   0.3369 Training Acc:    84.55    84.55    84.55%\n",
      "Epoch   2211 Step:      0 Loss:   0.3369 Training Acc:    84.50    84.50    84.50%\n",
      "Epoch   2212 Step:      0 Loss:   0.3368 Training Acc:    84.57    84.57    84.57%\n",
      "Epoch   2213 Step:      0 Loss:   0.3366 Training Acc:    84.50    84.50    84.50%\n",
      "Epoch   2214 Step:      0 Loss:   0.3365 Training Acc:    84.59    84.59    84.59%\n",
      "Epoch   2215 Step:      0 Loss:   0.3364 Training Acc:    84.55    84.55    84.55%\n",
      "Epoch   2216 Step:      0 Loss:   0.3363 Training Acc:    84.58    84.58    84.58%\n",
      "Epoch   2217 Step:      0 Loss:   0.3362 Training Acc:    84.58    84.58    84.58%\n",
      "Epoch   2218 Step:      0 Loss:   0.3361 Training Acc:    84.57    84.57    84.57%\n",
      "Epoch   2219 Step:      0 Loss:   0.3359 Training Acc:    84.62    84.62    84.62%\n",
      "Epoch   2220 Step:      0 Loss:   0.3357 Training Acc:    84.59    84.59    84.59%\n",
      "Epoch   2221 Step:      0 Loss:   0.3354 Training Acc:    84.64    84.64    84.64%\n",
      "Epoch   2222 Step:      0 Loss:   0.3352 Training Acc:    84.63    84.63    84.63%\n",
      "Epoch   2223 Step:      0 Loss:   0.3351 Training Acc:    84.64    84.64    84.64%\n",
      "Epoch   2224 Step:      0 Loss:   0.3351 Training Acc:    84.64    84.64    84.64%\n",
      "Epoch   2225 Step:      0 Loss:   0.3351 Training Acc:    84.66    84.66    84.66%\n",
      "Epoch   2226 Step:      0 Loss:   0.3351 Training Acc:    84.65    84.65    84.65%\n",
      "Epoch   2227 Step:      0 Loss:   0.3351 Training Acc:    84.64    84.64    84.64%\n",
      "Epoch   2228 Step:      0 Loss:   0.3350 Training Acc:    84.65    84.65    84.65%\n",
      "Epoch   2229 Step:      0 Loss:   0.3349 Training Acc:    84.65    84.65    84.65%\n",
      "Epoch   2230 Step:      0 Loss:   0.3347 Training Acc:    84.67    84.67    84.67%\n",
      "Epoch   2231 Step:      0 Loss:   0.3345 Training Acc:    84.66    84.66    84.66%\n",
      "Epoch   2232 Step:      0 Loss:   0.3344 Training Acc:    84.68    84.68    84.68%\n",
      "Epoch   2233 Step:      0 Loss:   0.3342 Training Acc:    84.68    84.68    84.68%\n",
      "Epoch   2234 Step:      0 Loss:   0.3341 Training Acc:    84.70    84.70    84.70%\n",
      "Epoch   2235 Step:      0 Loss:   0.3340 Training Acc:    84.71    84.71    84.71%\n",
      "Epoch   2236 Step:      0 Loss:   0.3339 Training Acc:    84.70    84.70    84.70%\n",
      "Epoch   2237 Step:      0 Loss:   0.3339 Training Acc:    84.70    84.70    84.70%\n",
      "Epoch   2238 Step:      0 Loss:   0.3339 Training Acc:    84.70    84.70    84.70%\n",
      "Epoch   2239 Step:      0 Loss:   0.3338 Training Acc:    84.71    84.71    84.71%\n",
      "Epoch   2240 Step:      0 Loss:   0.3338 Training Acc:    84.71    84.71    84.71%\n",
      "Epoch   2241 Step:      0 Loss:   0.3337 Training Acc:    84.73    84.73    84.73%\n",
      "Epoch   2242 Step:      0 Loss:   0.3336 Training Acc:    84.72    84.72    84.72%\n",
      "Epoch   2243 Step:      0 Loss:   0.3335 Training Acc:    84.73    84.73    84.73%\n",
      "Epoch   2244 Step:      0 Loss:   0.3334 Training Acc:    84.74    84.74    84.74%\n",
      "Epoch   2245 Step:      0 Loss:   0.3334 Training Acc:    84.74    84.74    84.74%\n",
      "Epoch   2246 Step:      0 Loss:   0.3333 Training Acc:    84.75    84.75    84.75%\n",
      "Epoch   2247 Step:      0 Loss:   0.3332 Training Acc:    84.75    84.75    84.75%\n",
      "Epoch   2248 Step:      0 Loss:   0.3332 Training Acc:    84.75    84.75    84.75%\n",
      "Epoch   2249 Step:      0 Loss:   0.3331 Training Acc:    84.76    84.76    84.76%\n",
      "Epoch   2250 Step:      0 Loss:   0.3331 Training Acc:    84.76    84.76    84.76%\n",
      "Epoch   2251 Step:      0 Loss:   0.3331 Training Acc:    84.73    84.73    84.73%\n",
      "Epoch   2252 Step:      0 Loss:   0.3331 Training Acc:    84.76    84.76    84.76%\n",
      "Epoch   2253 Step:      0 Loss:   0.3332 Training Acc:    84.71    84.71    84.71%\n",
      "Epoch   2254 Step:      0 Loss:   0.3333 Training Acc:    84.76    84.76    84.76%\n",
      "Epoch   2255 Step:      0 Loss:   0.3335 Training Acc:    84.69    84.69    84.69%\n",
      "Epoch   2256 Step:      0 Loss:   0.3338 Training Acc:    84.72    84.72    84.72%\n",
      "Epoch   2257 Step:      0 Loss:   0.3341 Training Acc:    84.63    84.63    84.63%\n",
      "Epoch   2258 Step:      0 Loss:   0.3345 Training Acc:    84.70    84.70    84.70%\n",
      "Epoch   2259 Step:      0 Loss:   0.3349 Training Acc:    84.55    84.55    84.55%\n",
      "Epoch   2260 Step:      0 Loss:   0.3352 Training Acc:    84.66    84.66    84.66%\n",
      "Epoch   2261 Step:      0 Loss:   0.3354 Training Acc:    84.51    84.51    84.51%\n",
      "Epoch   2262 Step:      0 Loss:   0.3353 Training Acc:    84.64    84.64    84.64%\n",
      "Epoch   2263 Step:      0 Loss:   0.3349 Training Acc:    84.57    84.57    84.57%\n",
      "Epoch   2264 Step:      0 Loss:   0.3341 Training Acc:    84.71    84.71    84.71%\n",
      "Epoch   2265 Step:      0 Loss:   0.3333 Training Acc:    84.69    84.69    84.69%\n",
      "Epoch   2266 Step:      0 Loss:   0.3325 Training Acc:    84.78    84.78    84.78%\n",
      "Epoch   2267 Step:      0 Loss:   0.3320 Training Acc:    84.84    84.84    84.84%\n",
      "Epoch   2268 Step:      0 Loss:   0.3318 Training Acc:    84.80    84.80    84.80%\n",
      "Epoch   2269 Step:      0 Loss:   0.3318 Training Acc:    84.83    84.83    84.83%\n",
      "Epoch   2270 Step:      0 Loss:   0.3321 Training Acc:    84.79    84.79    84.79%\n",
      "Epoch   2271 Step:      0 Loss:   0.3322 Training Acc:    84.82    84.82    84.82%\n",
      "Epoch   2272 Step:      0 Loss:   0.3323 Training Acc:    84.76    84.76    84.76%\n",
      "Epoch   2273 Step:      0 Loss:   0.3322 Training Acc:    84.83    84.83    84.83%\n",
      "Epoch   2274 Step:      0 Loss:   0.3319 Training Acc:    84.79    84.79    84.79%\n",
      "Epoch   2275 Step:      0 Loss:   0.3315 Training Acc:    84.84    84.84    84.84%\n",
      "Epoch   2276 Step:      0 Loss:   0.3312 Training Acc:    84.85    84.85    84.85%\n",
      "Epoch   2277 Step:      0 Loss:   0.3309 Training Acc:    84.89    84.89    84.89%\n",
      "Epoch   2278 Step:      0 Loss:   0.3308 Training Acc:    84.89    84.89    84.89%\n",
      "Epoch   2279 Step:      0 Loss:   0.3308 Training Acc:    84.87    84.87    84.87%\n",
      "Epoch   2280 Step:      0 Loss:   0.3308 Training Acc:    84.89    84.89    84.89%\n",
      "Epoch   2281 Step:      0 Loss:   0.3309 Training Acc:    84.86    84.86    84.86%\n",
      "Epoch   2282 Step:      0 Loss:   0.3309 Training Acc:    84.88    84.88    84.88%\n",
      "Epoch   2283 Step:      0 Loss:   0.3308 Training Acc:    84.87    84.87    84.87%\n",
      "Epoch   2284 Step:      0 Loss:   0.3307 Training Acc:    84.89    84.89    84.89%\n",
      "Epoch   2285 Step:      0 Loss:   0.3305 Training Acc:    84.90    84.90    84.90%\n",
      "Epoch   2286 Step:      0 Loss:   0.3303 Training Acc:    84.91    84.91    84.91%\n",
      "Epoch   2287 Step:      0 Loss:   0.3301 Training Acc:    84.92    84.92    84.92%\n",
      "Epoch   2288 Step:      0 Loss:   0.3299 Training Acc:    84.94    84.94    84.94%\n",
      "Epoch   2289 Step:      0 Loss:   0.3298 Training Acc:    84.95    84.95    84.95%\n",
      "Epoch   2290 Step:      0 Loss:   0.3297 Training Acc:    84.95    84.95    84.95%\n",
      "Epoch   2291 Step:      0 Loss:   0.3296 Training Acc:    84.96    84.96    84.96%\n",
      "Epoch   2292 Step:      0 Loss:   0.3296 Training Acc:    84.96    84.96    84.96%\n",
      "Epoch   2293 Step:      0 Loss:   0.3296 Training Acc:    84.97    84.97    84.97%\n",
      "Epoch   2294 Step:      0 Loss:   0.3296 Training Acc:    84.95    84.95    84.95%\n",
      "Epoch   2295 Step:      0 Loss:   0.3295 Training Acc:    84.97    84.97    84.97%\n",
      "Epoch   2296 Step:      0 Loss:   0.3295 Training Acc:    84.94    84.94    84.94%\n",
      "Epoch   2297 Step:      0 Loss:   0.3295 Training Acc:    84.97    84.97    84.97%\n",
      "Epoch   2298 Step:      0 Loss:   0.3294 Training Acc:    84.95    84.95    84.95%\n",
      "Epoch   2299 Step:      0 Loss:   0.3293 Training Acc:    84.97    84.97    84.97%\n",
      "Epoch   2300 Step:      0 Loss:   0.3293 Training Acc:    84.95    84.95    84.95%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2301 Step:      0 Loss:   0.3292 Training Acc:    84.98    84.98    84.98%\n",
      "Epoch   2302 Step:      0 Loss:   0.3292 Training Acc:    84.97    84.97    84.97%\n",
      "Epoch   2303 Step:      0 Loss:   0.3291 Training Acc:    84.99    84.99    84.99%\n",
      "Epoch   2304 Step:      0 Loss:   0.3291 Training Acc:    84.96    84.96    84.96%\n",
      "Epoch   2305 Step:      0 Loss:   0.3291 Training Acc:    84.99    84.99    84.99%\n",
      "Epoch   2306 Step:      0 Loss:   0.3292 Training Acc:    84.94    84.94    84.94%\n",
      "Epoch   2307 Step:      0 Loss:   0.3293 Training Acc:    84.97    84.97    84.97%\n",
      "Epoch   2308 Step:      0 Loss:   0.3296 Training Acc:    84.92    84.92    84.92%\n",
      "Epoch   2309 Step:      0 Loss:   0.3299 Training Acc:    84.91    84.91    84.91%\n",
      "Epoch   2310 Step:      0 Loss:   0.3303 Training Acc:    84.88    84.88    84.88%\n",
      "Epoch   2311 Step:      0 Loss:   0.3307 Training Acc:    84.83    84.83    84.83%\n",
      "Epoch   2312 Step:      0 Loss:   0.3311 Training Acc:    84.83    84.83    84.83%\n",
      "Epoch   2313 Step:      0 Loss:   0.3314 Training Acc:    84.78    84.78    84.78%\n",
      "Epoch   2314 Step:      0 Loss:   0.3312 Training Acc:    84.83    84.83    84.83%\n",
      "Epoch   2315 Step:      0 Loss:   0.3308 Training Acc:    84.79    84.79    84.79%\n",
      "Epoch   2316 Step:      0 Loss:   0.3299 Training Acc:    84.94    84.94    84.94%\n",
      "Epoch   2317 Step:      0 Loss:   0.3290 Training Acc:    84.94    84.94    84.94%\n",
      "Epoch   2318 Step:      0 Loss:   0.3284 Training Acc:    85.04    85.04    85.04%\n",
      "Epoch   2319 Step:      0 Loss:   0.3283 Training Acc:    84.99    84.99    84.99%\n",
      "Epoch   2320 Step:      0 Loss:   0.3285 Training Acc:    85.01    85.01    85.01%\n",
      "Epoch   2321 Step:      0 Loss:   0.3289 Training Acc:    84.94    84.94    84.94%\n",
      "Epoch   2322 Step:      0 Loss:   0.3290 Training Acc:    84.97    84.97    84.97%\n",
      "Epoch   2323 Step:      0 Loss:   0.3290 Training Acc:    84.93    84.93    84.93%\n",
      "Epoch   2324 Step:      0 Loss:   0.3285 Training Acc:    85.00    85.00    85.00%\n",
      "Epoch   2325 Step:      0 Loss:   0.3279 Training Acc:    85.00    85.00    85.00%\n",
      "Epoch   2326 Step:      0 Loss:   0.3273 Training Acc:    85.10    85.10    85.10%\n",
      "Epoch   2327 Step:      0 Loss:   0.3269 Training Acc:    85.13    85.13    85.13%\n",
      "Epoch   2328 Step:      0 Loss:   0.3268 Training Acc:    85.12    85.12    85.12%\n",
      "Epoch   2329 Step:      0 Loss:   0.3270 Training Acc:    85.11    85.11    85.11%\n",
      "Epoch   2330 Step:      0 Loss:   0.3272 Training Acc:    85.07    85.07    85.07%\n",
      "Epoch   2331 Step:      0 Loss:   0.3273 Training Acc:    85.08    85.08    85.08%\n",
      "Epoch   2332 Step:      0 Loss:   0.3273 Training Acc:    85.05    85.05    85.05%\n",
      "Epoch   2333 Step:      0 Loss:   0.3271 Training Acc:    85.10    85.10    85.10%\n",
      "Epoch   2334 Step:      0 Loss:   0.3269 Training Acc:    85.08    85.08    85.08%\n",
      "Epoch   2335 Step:      0 Loss:   0.3266 Training Acc:    85.12    85.12    85.12%\n",
      "Epoch   2336 Step:      0 Loss:   0.3264 Training Acc:    85.12    85.12    85.12%\n",
      "Epoch   2337 Step:      0 Loss:   0.3262 Training Acc:    85.15    85.15    85.15%\n",
      "Epoch   2338 Step:      0 Loss:   0.3262 Training Acc:    85.15    85.15    85.15%\n",
      "Epoch   2339 Step:      0 Loss:   0.3263 Training Acc:    85.17    85.17    85.17%\n",
      "Epoch   2340 Step:      0 Loss:   0.3263 Training Acc:    85.13    85.13    85.13%\n",
      "Epoch   2341 Step:      0 Loss:   0.3263 Training Acc:    85.15    85.15    85.15%\n",
      "Epoch   2342 Step:      0 Loss:   0.3262 Training Acc:    85.13    85.13    85.13%\n",
      "Epoch   2343 Step:      0 Loss:   0.3261 Training Acc:    85.16    85.16    85.16%\n",
      "Epoch   2344 Step:      0 Loss:   0.3260 Training Acc:    85.15    85.15    85.15%\n",
      "Epoch   2345 Step:      0 Loss:   0.3259 Training Acc:    85.19    85.19    85.19%\n",
      "Epoch   2346 Step:      0 Loss:   0.3258 Training Acc:    85.17    85.17    85.17%\n",
      "Epoch   2347 Step:      0 Loss:   0.3257 Training Acc:    85.18    85.18    85.18%\n",
      "Epoch   2348 Step:      0 Loss:   0.3257 Training Acc:    85.16    85.16    85.16%\n",
      "Epoch   2349 Step:      0 Loss:   0.3257 Training Acc:    85.17    85.17    85.17%\n",
      "Epoch   2350 Step:      0 Loss:   0.3258 Training Acc:    85.15    85.15    85.15%\n",
      "Epoch   2351 Step:      0 Loss:   0.3259 Training Acc:    85.16    85.16    85.16%\n",
      "Epoch   2352 Step:      0 Loss:   0.3260 Training Acc:    85.11    85.11    85.11%\n",
      "Epoch   2353 Step:      0 Loss:   0.3261 Training Acc:    85.15    85.15    85.15%\n",
      "Epoch   2354 Step:      0 Loss:   0.3262 Training Acc:    85.09    85.09    85.09%\n",
      "Epoch   2355 Step:      0 Loss:   0.3263 Training Acc:    85.14    85.14    85.14%\n",
      "Epoch   2356 Step:      0 Loss:   0.3264 Training Acc:    85.09    85.09    85.09%\n",
      "Epoch   2357 Step:      0 Loss:   0.3263 Training Acc:    85.14    85.14    85.14%\n",
      "Epoch   2358 Step:      0 Loss:   0.3263 Training Acc:    85.08    85.08    85.08%\n",
      "Epoch   2359 Step:      0 Loss:   0.3262 Training Acc:    85.14    85.14    85.14%\n",
      "Epoch   2360 Step:      0 Loss:   0.3261 Training Acc:    85.09    85.09    85.09%\n",
      "Epoch   2361 Step:      0 Loss:   0.3260 Training Acc:    85.15    85.15    85.15%\n",
      "Epoch   2362 Step:      0 Loss:   0.3258 Training Acc:    85.12    85.12    85.12%\n",
      "Epoch   2363 Step:      0 Loss:   0.3256 Training Acc:    85.17    85.17    85.17%\n",
      "Epoch   2364 Step:      0 Loss:   0.3253 Training Acc:    85.16    85.16    85.16%\n",
      "Epoch   2365 Step:      0 Loss:   0.3249 Training Acc:    85.23    85.23    85.23%\n",
      "Epoch   2366 Step:      0 Loss:   0.3245 Training Acc:    85.24    85.24    85.24%\n",
      "Epoch   2367 Step:      0 Loss:   0.3242 Training Acc:    85.26    85.26    85.26%\n",
      "Epoch   2368 Step:      0 Loss:   0.3239 Training Acc:    85.26    85.26    85.26%\n",
      "Epoch   2369 Step:      0 Loss:   0.3238 Training Acc:    85.28    85.28    85.28%\n",
      "Epoch   2370 Step:      0 Loss:   0.3237 Training Acc:    85.30    85.30    85.30%\n",
      "Epoch   2371 Step:      0 Loss:   0.3237 Training Acc:    85.29    85.29    85.29%\n",
      "Epoch   2372 Step:      0 Loss:   0.3238 Training Acc:    85.31    85.31    85.31%\n",
      "Epoch   2373 Step:      0 Loss:   0.3238 Training Acc:    85.27    85.27    85.27%\n",
      "Epoch   2374 Step:      0 Loss:   0.3239 Training Acc:    85.30    85.30    85.30%\n",
      "Epoch   2375 Step:      0 Loss:   0.3239 Training Acc:    85.26    85.26    85.26%\n",
      "Epoch   2376 Step:      0 Loss:   0.3239 Training Acc:    85.29    85.29    85.29%\n",
      "Epoch   2377 Step:      0 Loss:   0.3239 Training Acc:    85.25    85.25    85.25%\n",
      "Epoch   2378 Step:      0 Loss:   0.3238 Training Acc:    85.29    85.29    85.29%\n",
      "Epoch   2379 Step:      0 Loss:   0.3237 Training Acc:    85.25    85.25    85.25%\n",
      "Epoch   2380 Step:      0 Loss:   0.3236 Training Acc:    85.30    85.30    85.30%\n",
      "Epoch   2381 Step:      0 Loss:   0.3235 Training Acc:    85.27    85.27    85.27%\n",
      "Epoch   2382 Step:      0 Loss:   0.3234 Training Acc:    85.30    85.30    85.30%\n",
      "Epoch   2383 Step:      0 Loss:   0.3233 Training Acc:    85.28    85.28    85.28%\n",
      "Epoch   2384 Step:      0 Loss:   0.3232 Training Acc:    85.30    85.30    85.30%\n",
      "Epoch   2385 Step:      0 Loss:   0.3231 Training Acc:    85.28    85.28    85.28%\n",
      "Epoch   2386 Step:      0 Loss:   0.3231 Training Acc:    85.30    85.30    85.30%\n",
      "Epoch   2387 Step:      0 Loss:   0.3230 Training Acc:    85.30    85.30    85.30%\n",
      "Epoch   2388 Step:      0 Loss:   0.3229 Training Acc:    85.32    85.32    85.32%\n",
      "Epoch   2389 Step:      0 Loss:   0.3228 Training Acc:    85.30    85.30    85.30%\n",
      "Epoch   2390 Step:      0 Loss:   0.3227 Training Acc:    85.32    85.32    85.32%\n",
      "Epoch   2391 Step:      0 Loss:   0.3226 Training Acc:    85.32    85.32    85.32%\n",
      "Epoch   2392 Step:      0 Loss:   0.3224 Training Acc:    85.33    85.33    85.33%\n",
      "Epoch   2393 Step:      0 Loss:   0.3223 Training Acc:    85.34    85.34    85.34%\n",
      "Epoch   2394 Step:      0 Loss:   0.3221 Training Acc:    85.35    85.35    85.35%\n",
      "Epoch   2395 Step:      0 Loss:   0.3220 Training Acc:    85.37    85.37    85.37%\n",
      "Epoch   2396 Step:      0 Loss:   0.3219 Training Acc:    85.39    85.39    85.39%\n",
      "Epoch   2397 Step:      0 Loss:   0.3218 Training Acc:    85.39    85.39    85.39%\n",
      "Epoch   2398 Step:      0 Loss:   0.3217 Training Acc:    85.41    85.41    85.41%\n",
      "Epoch   2399 Step:      0 Loss:   0.3216 Training Acc:    85.40    85.40    85.40%\n",
      "Epoch   2400 Step:      0 Loss:   0.3215 Training Acc:    85.42    85.42    85.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2401 Step:      0 Loss:   0.3214 Training Acc:    85.41    85.41    85.41%\n",
      "Epoch   2402 Step:      0 Loss:   0.3214 Training Acc:    85.43    85.43    85.43%\n",
      "Epoch   2403 Step:      0 Loss:   0.3214 Training Acc:    85.42    85.42    85.42%\n",
      "Epoch   2404 Step:      0 Loss:   0.3213 Training Acc:    85.43    85.43    85.43%\n",
      "Epoch   2405 Step:      0 Loss:   0.3213 Training Acc:    85.42    85.42    85.42%\n",
      "Epoch   2406 Step:      0 Loss:   0.3214 Training Acc:    85.43    85.43    85.43%\n",
      "Epoch   2407 Step:      0 Loss:   0.3215 Training Acc:    85.40    85.40    85.40%\n",
      "Epoch   2408 Step:      0 Loss:   0.3216 Training Acc:    85.41    85.41    85.41%\n",
      "Epoch   2409 Step:      0 Loss:   0.3219 Training Acc:    85.36    85.36    85.36%\n",
      "Epoch   2410 Step:      0 Loss:   0.3222 Training Acc:    85.35    85.35    85.35%\n",
      "Epoch   2411 Step:      0 Loss:   0.3227 Training Acc:    85.29    85.29    85.29%\n",
      "Epoch   2412 Step:      0 Loss:   0.3234 Training Acc:    85.28    85.28    85.28%\n",
      "Epoch   2413 Step:      0 Loss:   0.3242 Training Acc:    85.15    85.15    85.15%\n",
      "Epoch   2414 Step:      0 Loss:   0.3250 Training Acc:    85.18    85.18    85.18%\n",
      "Epoch   2415 Step:      0 Loss:   0.3258 Training Acc:    85.02    85.02    85.02%\n",
      "Epoch   2416 Step:      0 Loss:   0.3258 Training Acc:    85.11    85.11    85.11%\n",
      "Epoch   2417 Step:      0 Loss:   0.3253 Training Acc:    85.04    85.04    85.04%\n",
      "Epoch   2418 Step:      0 Loss:   0.3240 Training Acc:    85.24    85.24    85.24%\n",
      "Epoch   2419 Step:      0 Loss:   0.3226 Training Acc:    85.30    85.30    85.30%\n",
      "Epoch   2420 Step:      0 Loss:   0.3215 Training Acc:    85.40    85.40    85.40%\n",
      "Epoch   2421 Step:      0 Loss:   0.3212 Training Acc:    85.38    85.38    85.38%\n",
      "Epoch   2422 Step:      0 Loss:   0.3215 Training Acc:    85.38    85.38    85.38%\n",
      "Epoch   2423 Step:      0 Loss:   0.3220 Training Acc:    85.35    85.35    85.35%\n",
      "Epoch   2424 Step:      0 Loss:   0.3222 Training Acc:    85.31    85.31    85.31%\n",
      "Epoch   2425 Step:      0 Loss:   0.3218 Training Acc:    85.35    85.35    85.35%\n",
      "Epoch   2426 Step:      0 Loss:   0.3209 Training Acc:    85.42    85.42    85.42%\n",
      "Epoch   2427 Step:      0 Loss:   0.3200 Training Acc:    85.47    85.47    85.47%\n",
      "Epoch   2428 Step:      0 Loss:   0.3196 Training Acc:    85.50    85.50    85.50%\n",
      "Epoch   2429 Step:      0 Loss:   0.3198 Training Acc:    85.51    85.51    85.51%\n",
      "Epoch   2430 Step:      0 Loss:   0.3203 Training Acc:    85.44    85.44    85.44%\n",
      "Epoch   2431 Step:      0 Loss:   0.3206 Training Acc:    85.44    85.44    85.44%\n",
      "Epoch   2432 Step:      0 Loss:   0.3207 Training Acc:    85.42    85.42    85.42%\n",
      "Epoch   2433 Step:      0 Loss:   0.3202 Training Acc:    85.47    85.47    85.47%\n",
      "Epoch   2434 Step:      0 Loss:   0.3196 Training Acc:    85.50    85.50    85.50%\n",
      "Epoch   2435 Step:      0 Loss:   0.3191 Training Acc:    85.55    85.55    85.55%\n",
      "Epoch   2436 Step:      0 Loss:   0.3189 Training Acc:    85.55    85.55    85.55%\n",
      "Epoch   2437 Step:      0 Loss:   0.3189 Training Acc:    85.54    85.54    85.54%\n",
      "Epoch   2438 Step:      0 Loss:   0.3191 Training Acc:    85.54    85.54    85.54%\n",
      "Epoch   2439 Step:      0 Loss:   0.3192 Training Acc:    85.53    85.53    85.53%\n",
      "Epoch   2440 Step:      0 Loss:   0.3192 Training Acc:    85.54    85.54    85.54%\n",
      "Epoch   2441 Step:      0 Loss:   0.3190 Training Acc:    85.54    85.54    85.54%\n",
      "Epoch   2442 Step:      0 Loss:   0.3187 Training Acc:    85.57    85.57    85.57%\n",
      "Epoch   2443 Step:      0 Loss:   0.3184 Training Acc:    85.57    85.57    85.57%\n",
      "Epoch   2444 Step:      0 Loss:   0.3183 Training Acc:    85.59    85.59    85.59%\n",
      "Epoch   2445 Step:      0 Loss:   0.3183 Training Acc:    85.58    85.58    85.58%\n",
      "Epoch   2446 Step:      0 Loss:   0.3184 Training Acc:    85.56    85.56    85.56%\n",
      "Epoch   2447 Step:      0 Loss:   0.3184 Training Acc:    85.60    85.60    85.60%\n",
      "Epoch   2448 Step:      0 Loss:   0.3184 Training Acc:    85.57    85.57    85.57%\n",
      "Epoch   2449 Step:      0 Loss:   0.3182 Training Acc:    85.61    85.61    85.61%\n",
      "Epoch   2450 Step:      0 Loss:   0.3181 Training Acc:    85.58    85.58    85.58%\n",
      "Epoch   2451 Step:      0 Loss:   0.3179 Training Acc:    85.62    85.62    85.62%\n",
      "Epoch   2452 Step:      0 Loss:   0.3178 Training Acc:    85.61    85.61    85.61%\n",
      "Epoch   2453 Step:      0 Loss:   0.3177 Training Acc:    85.61    85.61    85.61%\n",
      "Epoch   2454 Step:      0 Loss:   0.3176 Training Acc:    85.61    85.61    85.61%\n",
      "Epoch   2455 Step:      0 Loss:   0.3176 Training Acc:    85.61    85.61    85.61%\n",
      "Epoch   2456 Step:      0 Loss:   0.3176 Training Acc:    85.62    85.62    85.62%\n",
      "Epoch   2457 Step:      0 Loss:   0.3176 Training Acc:    85.60    85.60    85.60%\n",
      "Epoch   2458 Step:      0 Loss:   0.3175 Training Acc:    85.62    85.62    85.62%\n",
      "Epoch   2459 Step:      0 Loss:   0.3174 Training Acc:    85.62    85.62    85.62%\n",
      "Epoch   2460 Step:      0 Loss:   0.3173 Training Acc:    85.62    85.62    85.62%\n",
      "Epoch   2461 Step:      0 Loss:   0.3172 Training Acc:    85.62    85.62    85.62%\n",
      "Epoch   2462 Step:      0 Loss:   0.3172 Training Acc:    85.63    85.63    85.63%\n",
      "Epoch   2463 Step:      0 Loss:   0.3172 Training Acc:    85.64    85.64    85.64%\n",
      "Epoch   2464 Step:      0 Loss:   0.3172 Training Acc:    85.63    85.63    85.63%\n",
      "Epoch   2465 Step:      0 Loss:   0.3172 Training Acc:    85.63    85.63    85.63%\n",
      "Epoch   2466 Step:      0 Loss:   0.3173 Training Acc:    85.63    85.63    85.63%\n",
      "Epoch   2467 Step:      0 Loss:   0.3174 Training Acc:    85.62    85.62    85.62%\n",
      "Epoch   2468 Step:      0 Loss:   0.3176 Training Acc:    85.62    85.62    85.62%\n",
      "Epoch   2469 Step:      0 Loss:   0.3178 Training Acc:    85.58    85.58    85.58%\n",
      "Epoch   2470 Step:      0 Loss:   0.3181 Training Acc:    85.60    85.60    85.60%\n",
      "Epoch   2471 Step:      0 Loss:   0.3184 Training Acc:    85.54    85.54    85.54%\n",
      "Epoch   2472 Step:      0 Loss:   0.3186 Training Acc:    85.56    85.56    85.56%\n",
      "Epoch   2473 Step:      0 Loss:   0.3188 Training Acc:    85.50    85.50    85.50%\n",
      "Epoch   2474 Step:      0 Loss:   0.3187 Training Acc:    85.55    85.55    85.55%\n",
      "Epoch   2475 Step:      0 Loss:   0.3183 Training Acc:    85.54    85.54    85.54%\n",
      "Epoch   2476 Step:      0 Loss:   0.3177 Training Acc:    85.63    85.63    85.63%\n",
      "Epoch   2477 Step:      0 Loss:   0.3170 Training Acc:    85.64    85.64    85.64%\n",
      "Epoch   2478 Step:      0 Loss:   0.3163 Training Acc:    85.69    85.69    85.69%\n",
      "Epoch   2479 Step:      0 Loss:   0.3159 Training Acc:    85.73    85.73    85.73%\n",
      "Epoch   2480 Step:      0 Loss:   0.3157 Training Acc:    85.72    85.72    85.72%\n",
      "Epoch   2481 Step:      0 Loss:   0.3157 Training Acc:    85.72    85.72    85.72%\n",
      "Epoch   2482 Step:      0 Loss:   0.3159 Training Acc:    85.70    85.70    85.70%\n",
      "Epoch   2483 Step:      0 Loss:   0.3161 Training Acc:    85.69    85.69    85.69%\n",
      "Epoch   2484 Step:      0 Loss:   0.3163 Training Acc:    85.68    85.68    85.68%\n",
      "Epoch   2485 Step:      0 Loss:   0.3163 Training Acc:    85.70    85.70    85.70%\n",
      "Epoch   2486 Step:      0 Loss:   0.3162 Training Acc:    85.69    85.69    85.69%\n",
      "Epoch   2487 Step:      0 Loss:   0.3160 Training Acc:    85.71    85.71    85.71%\n",
      "Epoch   2488 Step:      0 Loss:   0.3157 Training Acc:    85.71    85.71    85.71%\n",
      "Epoch   2489 Step:      0 Loss:   0.3154 Training Acc:    85.73    85.73    85.73%\n",
      "Epoch   2490 Step:      0 Loss:   0.3151 Training Acc:    85.76    85.76    85.76%\n",
      "Epoch   2491 Step:      0 Loss:   0.3150 Training Acc:    85.75    85.75    85.75%\n",
      "Epoch   2492 Step:      0 Loss:   0.3149 Training Acc:    85.78    85.78    85.78%\n",
      "Epoch   2493 Step:      0 Loss:   0.3149 Training Acc:    85.78    85.78    85.78%\n",
      "Epoch   2494 Step:      0 Loss:   0.3149 Training Acc:    85.77    85.77    85.77%\n",
      "Epoch   2495 Step:      0 Loss:   0.3149 Training Acc:    85.77    85.77    85.77%\n",
      "Epoch   2496 Step:      0 Loss:   0.3149 Training Acc:    85.76    85.76    85.76%\n",
      "Epoch   2497 Step:      0 Loss:   0.3149 Training Acc:    85.77    85.77    85.77%\n",
      "Epoch   2498 Step:      0 Loss:   0.3148 Training Acc:    85.77    85.77    85.77%\n",
      "Epoch   2499 Step:      0 Loss:   0.3147 Training Acc:    85.78    85.78    85.78%\n",
      "Epoch   2500 Step:      0 Loss:   0.3146 Training Acc:    85.78    85.78    85.78%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2501 Step:      0 Loss:   0.3144 Training Acc:    85.79    85.79    85.79%\n",
      "Epoch   2502 Step:      0 Loss:   0.3143 Training Acc:    85.81    85.81    85.81%\n",
      "Epoch   2503 Step:      0 Loss:   0.3141 Training Acc:    85.81    85.81    85.81%\n",
      "Epoch   2504 Step:      0 Loss:   0.3140 Training Acc:    85.83    85.83    85.83%\n",
      "Epoch   2505 Step:      0 Loss:   0.3139 Training Acc:    85.82    85.82    85.82%\n",
      "Epoch   2506 Step:      0 Loss:   0.3139 Training Acc:    85.84    85.84    85.84%\n",
      "Epoch   2507 Step:      0 Loss:   0.3138 Training Acc:    85.82    85.82    85.82%\n",
      "Epoch   2508 Step:      0 Loss:   0.3137 Training Acc:    85.85    85.85    85.85%\n",
      "Epoch   2509 Step:      0 Loss:   0.3137 Training Acc:    85.82    85.82    85.82%\n",
      "Epoch   2510 Step:      0 Loss:   0.3137 Training Acc:    85.83    85.83    85.83%\n",
      "Epoch   2511 Step:      0 Loss:   0.3137 Training Acc:    85.83    85.83    85.83%\n",
      "Epoch   2512 Step:      0 Loss:   0.3137 Training Acc:    85.83    85.83    85.83%\n",
      "Epoch   2513 Step:      0 Loss:   0.3137 Training Acc:    85.83    85.83    85.83%\n",
      "Epoch   2514 Step:      0 Loss:   0.3138 Training Acc:    85.82    85.82    85.82%\n",
      "Epoch   2515 Step:      0 Loss:   0.3139 Training Acc:    85.81    85.81    85.81%\n",
      "Epoch   2516 Step:      0 Loss:   0.3142 Training Acc:    85.79    85.79    85.79%\n",
      "Epoch   2517 Step:      0 Loss:   0.3146 Training Acc:    85.78    85.78    85.78%\n",
      "Epoch   2518 Step:      0 Loss:   0.3152 Training Acc:    85.71    85.71    85.71%\n",
      "Epoch   2519 Step:      0 Loss:   0.3160 Training Acc:    85.69    85.69    85.69%\n",
      "Epoch   2520 Step:      0 Loss:   0.3173 Training Acc:    85.54    85.54    85.54%\n",
      "Epoch   2521 Step:      0 Loss:   0.3185 Training Acc:    85.52    85.52    85.52%\n",
      "Epoch   2522 Step:      0 Loss:   0.3199 Training Acc:    85.33    85.33    85.33%\n",
      "Epoch   2523 Step:      0 Loss:   0.3202 Training Acc:    85.37    85.37    85.37%\n",
      "Epoch   2524 Step:      0 Loss:   0.3196 Training Acc:    85.36    85.36    85.36%\n",
      "Epoch   2525 Step:      0 Loss:   0.3171 Training Acc:    85.62    85.62    85.62%\n",
      "Epoch   2526 Step:      0 Loss:   0.3143 Training Acc:    85.78    85.78    85.78%\n",
      "Epoch   2527 Step:      0 Loss:   0.3125 Training Acc:    85.90    85.90    85.90%\n",
      "Epoch   2528 Step:      0 Loss:   0.3127 Training Acc:    85.88    85.88    85.88%\n",
      "Epoch   2529 Step:      0 Loss:   0.3142 Training Acc:    85.80    85.80    85.80%\n",
      "Epoch   2530 Step:      0 Loss:   0.3153 Training Acc:    85.73    85.73    85.73%\n",
      "Epoch   2531 Step:      0 Loss:   0.3154 Training Acc:    85.67    85.67    85.67%\n",
      "Epoch   2532 Step:      0 Loss:   0.3142 Training Acc:    85.77    85.77    85.77%\n",
      "Epoch   2533 Step:      0 Loss:   0.3128 Training Acc:    85.89    85.89    85.89%\n",
      "Epoch   2534 Step:      0 Loss:   0.3120 Training Acc:    85.93    85.93    85.93%\n",
      "Epoch   2535 Step:      0 Loss:   0.3123 Training Acc:    85.91    85.91    85.91%\n",
      "Epoch   2536 Step:      0 Loss:   0.3131 Training Acc:    85.86    85.86    85.86%\n",
      "Epoch   2537 Step:      0 Loss:   0.3135 Training Acc:    85.83    85.83    85.83%\n",
      "Epoch   2538 Step:      0 Loss:   0.3132 Training Acc:    85.85    85.85    85.85%\n",
      "Epoch   2539 Step:      0 Loss:   0.3123 Training Acc:    85.89    85.89    85.89%\n",
      "Epoch   2540 Step:      0 Loss:   0.3116 Training Acc:    85.95    85.95    85.95%\n",
      "Epoch   2541 Step:      0 Loss:   0.3115 Training Acc:    85.95    85.95    85.95%\n",
      "Epoch   2542 Step:      0 Loss:   0.3117 Training Acc:    85.93    85.93    85.93%\n",
      "Epoch   2543 Step:      0 Loss:   0.3121 Training Acc:    85.91    85.91    85.91%\n",
      "Epoch   2544 Step:      0 Loss:   0.3122 Training Acc:    85.90    85.90    85.90%\n",
      "Epoch   2545 Step:      0 Loss:   0.3119 Training Acc:    85.93    85.93    85.93%\n",
      "Epoch   2546 Step:      0 Loss:   0.3114 Training Acc:    85.96    85.96    85.96%\n",
      "Epoch   2547 Step:      0 Loss:   0.3110 Training Acc:    85.99    85.99    85.99%\n",
      "Epoch   2548 Step:      0 Loss:   0.3110 Training Acc:    85.99    85.99    85.99%\n",
      "Epoch   2549 Step:      0 Loss:   0.3111 Training Acc:    85.98    85.98    85.98%\n",
      "Epoch   2550 Step:      0 Loss:   0.3112 Training Acc:    85.97    85.97    85.97%\n",
      "Epoch   2551 Step:      0 Loss:   0.3112 Training Acc:    85.97    85.97    85.97%\n",
      "Epoch   2552 Step:      0 Loss:   0.3110 Training Acc:    85.98    85.98    85.98%\n",
      "Epoch   2553 Step:      0 Loss:   0.3108 Training Acc:    86.00    86.00    86.00%\n",
      "Epoch   2554 Step:      0 Loss:   0.3106 Training Acc:    86.01    86.01    86.01%\n",
      "Epoch   2555 Step:      0 Loss:   0.3105 Training Acc:    86.02    86.02    86.02%\n",
      "Epoch   2556 Step:      0 Loss:   0.3105 Training Acc:    86.02    86.02    86.02%\n",
      "Epoch   2557 Step:      0 Loss:   0.3105 Training Acc:    86.01    86.01    86.01%\n",
      "Epoch   2558 Step:      0 Loss:   0.3105 Training Acc:    86.00    86.00    86.00%\n",
      "Epoch   2559 Step:      0 Loss:   0.3105 Training Acc:    86.02    86.02    86.02%\n",
      "Epoch   2560 Step:      0 Loss:   0.3103 Training Acc:    86.02    86.02    86.02%\n",
      "Epoch   2561 Step:      0 Loss:   0.3102 Training Acc:    86.04    86.04    86.04%\n",
      "Epoch   2562 Step:      0 Loss:   0.3102 Training Acc:    86.06    86.06    86.06%\n",
      "Epoch   2563 Step:      0 Loss:   0.3102 Training Acc:    86.05    86.05    86.05%\n",
      "Epoch   2564 Step:      0 Loss:   0.3102 Training Acc:    86.03    86.03    86.03%\n",
      "Epoch   2565 Step:      0 Loss:   0.3103 Training Acc:    86.03    86.03    86.03%\n",
      "Epoch   2566 Step:      0 Loss:   0.3105 Training Acc:    86.00    86.00    86.00%\n",
      "Epoch   2567 Step:      0 Loss:   0.3107 Training Acc:    86.01    86.01    86.01%\n",
      "Epoch   2568 Step:      0 Loss:   0.3110 Training Acc:    85.95    85.95    85.95%\n",
      "Epoch   2569 Step:      0 Loss:   0.3113 Training Acc:    85.99    85.99    85.99%\n",
      "Epoch   2570 Step:      0 Loss:   0.3118 Training Acc:    85.88    85.88    85.88%\n",
      "Epoch   2571 Step:      0 Loss:   0.3123 Training Acc:    85.91    85.91    85.91%\n",
      "Epoch   2572 Step:      0 Loss:   0.3127 Training Acc:    85.81    85.81    85.81%\n",
      "Epoch   2573 Step:      0 Loss:   0.3125 Training Acc:    85.89    85.89    85.89%\n",
      "Epoch   2574 Step:      0 Loss:   0.3121 Training Acc:    85.87    85.87    85.87%\n",
      "Epoch   2575 Step:      0 Loss:   0.3111 Training Acc:    86.01    86.01    86.01%\n",
      "Epoch   2576 Step:      0 Loss:   0.3100 Training Acc:    86.02    86.02    86.02%\n",
      "Epoch   2577 Step:      0 Loss:   0.3092 Training Acc:    86.10    86.10    86.10%\n",
      "Epoch   2578 Step:      0 Loss:   0.3089 Training Acc:    86.11    86.11    86.11%\n",
      "Epoch   2579 Step:      0 Loss:   0.3090 Training Acc:    86.10    86.10    86.10%\n",
      "Epoch   2580 Step:      0 Loss:   0.3094 Training Acc:    86.09    86.09    86.09%\n",
      "Epoch   2581 Step:      0 Loss:   0.3098 Training Acc:    86.03    86.03    86.03%\n",
      "Epoch   2582 Step:      0 Loss:   0.3100 Training Acc:    86.08    86.08    86.08%\n",
      "Epoch   2583 Step:      0 Loss:   0.3099 Training Acc:    86.01    86.01    86.01%\n",
      "Epoch   2584 Step:      0 Loss:   0.3095 Training Acc:    86.09    86.09    86.09%\n",
      "Epoch   2585 Step:      0 Loss:   0.3090 Training Acc:    86.09    86.09    86.09%\n",
      "Epoch   2586 Step:      0 Loss:   0.3086 Training Acc:    86.14    86.14    86.14%\n",
      "Epoch   2587 Step:      0 Loss:   0.3083 Training Acc:    86.15    86.15    86.15%\n",
      "Epoch   2588 Step:      0 Loss:   0.3082 Training Acc:    86.17    86.17    86.17%\n",
      "Epoch   2589 Step:      0 Loss:   0.3083 Training Acc:    86.16    86.16    86.16%\n",
      "Epoch   2590 Step:      0 Loss:   0.3084 Training Acc:    86.14    86.14    86.14%\n",
      "Epoch   2591 Step:      0 Loss:   0.3085 Training Acc:    86.15    86.15    86.15%\n",
      "Epoch   2592 Step:      0 Loss:   0.3085 Training Acc:    86.11    86.11    86.11%\n",
      "Epoch   2593 Step:      0 Loss:   0.3084 Training Acc:    86.15    86.15    86.15%\n",
      "Epoch   2594 Step:      0 Loss:   0.3082 Training Acc:    86.14    86.14    86.14%\n",
      "Epoch   2595 Step:      0 Loss:   0.3079 Training Acc:    86.16    86.16    86.16%\n",
      "Epoch   2596 Step:      0 Loss:   0.3078 Training Acc:    86.17    86.17    86.17%\n",
      "Epoch   2597 Step:      0 Loss:   0.3076 Training Acc:    86.19    86.19    86.19%\n",
      "Epoch   2598 Step:      0 Loss:   0.3076 Training Acc:    86.19    86.19    86.19%\n",
      "Epoch   2599 Step:      0 Loss:   0.3075 Training Acc:    86.20    86.20    86.20%\n",
      "Epoch   2600 Step:      0 Loss:   0.3075 Training Acc:    86.21    86.21    86.21%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2601 Step:      0 Loss:   0.3075 Training Acc:    86.20    86.20    86.20%\n",
      "Epoch   2602 Step:      0 Loss:   0.3074 Training Acc:    86.21    86.21    86.21%\n",
      "Epoch   2603 Step:      0 Loss:   0.3073 Training Acc:    86.20    86.20    86.20%\n",
      "Epoch   2604 Step:      0 Loss:   0.3072 Training Acc:    86.21    86.21    86.21%\n",
      "Epoch   2605 Step:      0 Loss:   0.3071 Training Acc:    86.20    86.20    86.20%\n",
      "Epoch   2606 Step:      0 Loss:   0.3070 Training Acc:    86.23    86.23    86.23%\n",
      "Epoch   2607 Step:      0 Loss:   0.3070 Training Acc:    86.21    86.21    86.21%\n",
      "Epoch   2608 Step:      0 Loss:   0.3069 Training Acc:    86.24    86.24    86.24%\n",
      "Epoch   2609 Step:      0 Loss:   0.3068 Training Acc:    86.22    86.22    86.22%\n",
      "Epoch   2610 Step:      0 Loss:   0.3068 Training Acc:    86.23    86.23    86.23%\n",
      "Epoch   2611 Step:      0 Loss:   0.3067 Training Acc:    86.22    86.22    86.22%\n",
      "Epoch   2612 Step:      0 Loss:   0.3066 Training Acc:    86.24    86.24    86.24%\n",
      "Epoch   2613 Step:      0 Loss:   0.3066 Training Acc:    86.23    86.23    86.23%\n",
      "Epoch   2614 Step:      0 Loss:   0.3065 Training Acc:    86.25    86.25    86.25%\n",
      "Epoch   2615 Step:      0 Loss:   0.3064 Training Acc:    86.25    86.25    86.25%\n",
      "Epoch   2616 Step:      0 Loss:   0.3064 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2617 Step:      0 Loss:   0.3063 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2618 Step:      0 Loss:   0.3062 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2619 Step:      0 Loss:   0.3061 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2620 Step:      0 Loss:   0.3061 Training Acc:    86.28    86.28    86.28%\n",
      "Epoch   2621 Step:      0 Loss:   0.3060 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2622 Step:      0 Loss:   0.3059 Training Acc:    86.29    86.29    86.29%\n",
      "Epoch   2623 Step:      0 Loss:   0.3058 Training Acc:    86.27    86.27    86.27%\n",
      "Epoch   2624 Step:      0 Loss:   0.3058 Training Acc:    86.30    86.30    86.30%\n",
      "Epoch   2625 Step:      0 Loss:   0.3057 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2626 Step:      0 Loss:   0.3057 Training Acc:    86.30    86.30    86.30%\n",
      "Epoch   2627 Step:      0 Loss:   0.3056 Training Acc:    86.28    86.28    86.28%\n",
      "Epoch   2628 Step:      0 Loss:   0.3056 Training Acc:    86.30    86.30    86.30%\n",
      "Epoch   2629 Step:      0 Loss:   0.3056 Training Acc:    86.28    86.28    86.28%\n",
      "Epoch   2630 Step:      0 Loss:   0.3056 Training Acc:    86.31    86.31    86.31%\n",
      "Epoch   2631 Step:      0 Loss:   0.3057 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2632 Step:      0 Loss:   0.3058 Training Acc:    86.29    86.29    86.29%\n",
      "Epoch   2633 Step:      0 Loss:   0.3060 Training Acc:    86.22    86.22    86.22%\n",
      "Epoch   2634 Step:      0 Loss:   0.3063 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2635 Step:      0 Loss:   0.3069 Training Acc:    86.14    86.14    86.14%\n",
      "Epoch   2636 Step:      0 Loss:   0.3076 Training Acc:    86.18    86.18    86.18%\n",
      "Epoch   2637 Step:      0 Loss:   0.3085 Training Acc:    86.01    86.01    86.01%\n",
      "Epoch   2638 Step:      0 Loss:   0.3096 Training Acc:    86.03    86.03    86.03%\n",
      "Epoch   2639 Step:      0 Loss:   0.3107 Training Acc:    85.84    85.84    85.84%\n",
      "Epoch   2640 Step:      0 Loss:   0.3112 Training Acc:    85.90    85.90    85.90%\n",
      "Epoch   2641 Step:      0 Loss:   0.3112 Training Acc:    85.82    85.82    85.82%\n",
      "Epoch   2642 Step:      0 Loss:   0.3099 Training Acc:    86.00    86.00    86.00%\n",
      "Epoch   2643 Step:      0 Loss:   0.3082 Training Acc:    86.08    86.08    86.08%\n",
      "Epoch   2644 Step:      0 Loss:   0.3064 Training Acc:    86.20    86.20    86.20%\n",
      "Epoch   2645 Step:      0 Loss:   0.3055 Training Acc:    86.30    86.30    86.30%\n",
      "Epoch   2646 Step:      0 Loss:   0.3056 Training Acc:    86.25    86.25    86.25%\n",
      "Epoch   2647 Step:      0 Loss:   0.3063 Training Acc:    86.28    86.28    86.28%\n",
      "Epoch   2648 Step:      0 Loss:   0.3069 Training Acc:    86.13    86.13    86.13%\n",
      "Epoch   2649 Step:      0 Loss:   0.3067 Training Acc:    86.22    86.22    86.22%\n",
      "Epoch   2650 Step:      0 Loss:   0.3059 Training Acc:    86.18    86.18    86.18%\n",
      "Epoch   2651 Step:      0 Loss:   0.3049 Training Acc:    86.33    86.33    86.33%\n",
      "Epoch   2652 Step:      0 Loss:   0.3042 Training Acc:    86.34    86.34    86.34%\n",
      "Epoch   2653 Step:      0 Loss:   0.3042 Training Acc:    86.34    86.34    86.34%\n",
      "Epoch   2654 Step:      0 Loss:   0.3047 Training Acc:    86.37    86.37    86.37%\n",
      "Epoch   2655 Step:      0 Loss:   0.3052 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2656 Step:      0 Loss:   0.3052 Training Acc:    86.33    86.33    86.33%\n",
      "Epoch   2657 Step:      0 Loss:   0.3048 Training Acc:    86.29    86.29    86.29%\n",
      "Epoch   2658 Step:      0 Loss:   0.3041 Training Acc:    86.40    86.40    86.40%\n",
      "Epoch   2659 Step:      0 Loss:   0.3035 Training Acc:    86.38    86.38    86.38%\n",
      "Epoch   2660 Step:      0 Loss:   0.3033 Training Acc:    86.42    86.42    86.42%\n",
      "Epoch   2661 Step:      0 Loss:   0.3035 Training Acc:    86.41    86.41    86.41%\n",
      "Epoch   2662 Step:      0 Loss:   0.3038 Training Acc:    86.36    86.36    86.36%\n",
      "Epoch   2663 Step:      0 Loss:   0.3040 Training Acc:    86.41    86.41    86.41%\n",
      "Epoch   2664 Step:      0 Loss:   0.3039 Training Acc:    86.35    86.35    86.35%\n",
      "Epoch   2665 Step:      0 Loss:   0.3036 Training Acc:    86.43    86.43    86.43%\n",
      "Epoch   2666 Step:      0 Loss:   0.3032 Training Acc:    86.41    86.41    86.41%\n",
      "Epoch   2667 Step:      0 Loss:   0.3029 Training Acc:    86.45    86.45    86.45%\n",
      "Epoch   2668 Step:      0 Loss:   0.3028 Training Acc:    86.44    86.44    86.44%\n",
      "Epoch   2669 Step:      0 Loss:   0.3029 Training Acc:    86.41    86.41    86.41%\n",
      "Epoch   2670 Step:      0 Loss:   0.3030 Training Acc:    86.46    86.46    86.46%\n",
      "Epoch   2671 Step:      0 Loss:   0.3030 Training Acc:    86.40    86.40    86.40%\n",
      "Epoch   2672 Step:      0 Loss:   0.3029 Training Acc:    86.45    86.45    86.45%\n",
      "Epoch   2673 Step:      0 Loss:   0.3027 Training Acc:    86.42    86.42    86.42%\n",
      "Epoch   2674 Step:      0 Loss:   0.3025 Training Acc:    86.48    86.48    86.48%\n",
      "Epoch   2675 Step:      0 Loss:   0.3024 Training Acc:    86.46    86.46    86.46%\n",
      "Epoch   2676 Step:      0 Loss:   0.3023 Training Acc:    86.47    86.47    86.47%\n",
      "Epoch   2677 Step:      0 Loss:   0.3022 Training Acc:    86.48    86.48    86.48%\n",
      "Epoch   2678 Step:      0 Loss:   0.3022 Training Acc:    86.45    86.45    86.45%\n",
      "Epoch   2679 Step:      0 Loss:   0.3022 Training Acc:    86.49    86.49    86.49%\n",
      "Epoch   2680 Step:      0 Loss:   0.3022 Training Acc:    86.45    86.45    86.45%\n",
      "Epoch   2681 Step:      0 Loss:   0.3021 Training Acc:    86.50    86.50    86.50%\n",
      "Epoch   2682 Step:      0 Loss:   0.3020 Training Acc:    86.47    86.47    86.47%\n",
      "Epoch   2683 Step:      0 Loss:   0.3019 Training Acc:    86.51    86.51    86.51%\n",
      "Epoch   2684 Step:      0 Loss:   0.3018 Training Acc:    86.48    86.48    86.48%\n",
      "Epoch   2685 Step:      0 Loss:   0.3017 Training Acc:    86.52    86.52    86.52%\n",
      "Epoch   2686 Step:      0 Loss:   0.3016 Training Acc:    86.52    86.52    86.52%\n",
      "Epoch   2687 Step:      0 Loss:   0.3016 Training Acc:    86.52    86.52    86.52%\n",
      "Epoch   2688 Step:      0 Loss:   0.3016 Training Acc:    86.52    86.52    86.52%\n",
      "Epoch   2689 Step:      0 Loss:   0.3015 Training Acc:    86.53    86.53    86.53%\n",
      "Epoch   2690 Step:      0 Loss:   0.3015 Training Acc:    86.54    86.54    86.54%\n",
      "Epoch   2691 Step:      0 Loss:   0.3014 Training Acc:    86.53    86.53    86.53%\n",
      "Epoch   2692 Step:      0 Loss:   0.3014 Training Acc:    86.54    86.54    86.54%\n",
      "Epoch   2693 Step:      0 Loss:   0.3013 Training Acc:    86.54    86.54    86.54%\n",
      "Epoch   2694 Step:      0 Loss:   0.3012 Training Acc:    86.54    86.54    86.54%\n",
      "Epoch   2695 Step:      0 Loss:   0.3012 Training Acc:    86.56    86.56    86.56%\n",
      "Epoch   2696 Step:      0 Loss:   0.3012 Training Acc:    86.53    86.53    86.53%\n",
      "Epoch   2697 Step:      0 Loss:   0.3012 Training Acc:    86.53    86.53    86.53%\n",
      "Epoch   2698 Step:      0 Loss:   0.3013 Training Acc:    86.51    86.51    86.51%\n",
      "Epoch   2699 Step:      0 Loss:   0.3014 Training Acc:    86.51    86.51    86.51%\n",
      "Epoch   2700 Step:      0 Loss:   0.3016 Training Acc:    86.49    86.49    86.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2701 Step:      0 Loss:   0.3019 Training Acc:    86.47    86.47    86.47%\n",
      "Epoch   2702 Step:      0 Loss:   0.3023 Training Acc:    86.44    86.44    86.44%\n",
      "Epoch   2703 Step:      0 Loss:   0.3028 Training Acc:    86.39    86.39    86.39%\n",
      "Epoch   2704 Step:      0 Loss:   0.3035 Training Acc:    86.35    86.35    86.35%\n",
      "Epoch   2705 Step:      0 Loss:   0.3041 Training Acc:    86.32    86.32    86.32%\n",
      "Epoch   2706 Step:      0 Loss:   0.3048 Training Acc:    86.25    86.25    86.25%\n",
      "Epoch   2707 Step:      0 Loss:   0.3050 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2708 Step:      0 Loss:   0.3047 Training Acc:    86.26    86.26    86.26%\n",
      "Epoch   2709 Step:      0 Loss:   0.3035 Training Acc:    86.36    86.36    86.36%\n",
      "Epoch   2710 Step:      0 Loss:   0.3020 Training Acc:    86.45    86.45    86.45%\n",
      "Epoch   2711 Step:      0 Loss:   0.3006 Training Acc:    86.56    86.56    86.56%\n",
      "Epoch   2712 Step:      0 Loss:   0.2999 Training Acc:    86.61    86.61    86.61%\n",
      "Epoch   2713 Step:      0 Loss:   0.3002 Training Acc:    86.58    86.58    86.58%\n",
      "Epoch   2714 Step:      0 Loss:   0.3008 Training Acc:    86.52    86.52    86.52%\n",
      "Epoch   2715 Step:      0 Loss:   0.3015 Training Acc:    86.50    86.50    86.50%\n",
      "Epoch   2716 Step:      0 Loss:   0.3016 Training Acc:    86.46    86.46    86.46%\n",
      "Epoch   2717 Step:      0 Loss:   0.3013 Training Acc:    86.50    86.50    86.50%\n",
      "Epoch   2718 Step:      0 Loss:   0.3005 Training Acc:    86.55    86.55    86.55%\n",
      "Epoch   2719 Step:      0 Loss:   0.2998 Training Acc:    86.60    86.60    86.60%\n",
      "Epoch   2720 Step:      0 Loss:   0.2994 Training Acc:    86.63    86.63    86.63%\n",
      "Epoch   2721 Step:      0 Loss:   0.2995 Training Acc:    86.63    86.63    86.63%\n",
      "Epoch   2722 Step:      0 Loss:   0.2998 Training Acc:    86.60    86.60    86.60%\n",
      "Epoch   2723 Step:      0 Loss:   0.3001 Training Acc:    86.58    86.58    86.58%\n",
      "Epoch   2724 Step:      0 Loss:   0.3001 Training Acc:    86.57    86.57    86.57%\n",
      "Epoch   2725 Step:      0 Loss:   0.2999 Training Acc:    86.59    86.59    86.59%\n",
      "Epoch   2726 Step:      0 Loss:   0.2995 Training Acc:    86.61    86.61    86.61%\n",
      "Epoch   2727 Step:      0 Loss:   0.2991 Training Acc:    86.65    86.65    86.65%\n",
      "Epoch   2728 Step:      0 Loss:   0.2989 Training Acc:    86.66    86.66    86.66%\n",
      "Epoch   2729 Step:      0 Loss:   0.2988 Training Acc:    86.65    86.65    86.65%\n",
      "Epoch   2730 Step:      0 Loss:   0.2989 Training Acc:    86.67    86.67    86.67%\n",
      "Epoch   2731 Step:      0 Loss:   0.2990 Training Acc:    86.65    86.65    86.65%\n",
      "Epoch   2732 Step:      0 Loss:   0.2991 Training Acc:    86.65    86.65    86.65%\n",
      "Epoch   2733 Step:      0 Loss:   0.2990 Training Acc:    86.64    86.64    86.64%\n",
      "Epoch   2734 Step:      0 Loss:   0.2988 Training Acc:    86.67    86.67    86.67%\n",
      "Epoch   2735 Step:      0 Loss:   0.2986 Training Acc:    86.67    86.67    86.67%\n",
      "Epoch   2736 Step:      0 Loss:   0.2984 Training Acc:    86.70    86.70    86.70%\n",
      "Epoch   2737 Step:      0 Loss:   0.2983 Training Acc:    86.70    86.70    86.70%\n",
      "Epoch   2738 Step:      0 Loss:   0.2983 Training Acc:    86.69    86.69    86.69%\n",
      "Epoch   2739 Step:      0 Loss:   0.2982 Training Acc:    86.70    86.70    86.70%\n",
      "Epoch   2740 Step:      0 Loss:   0.2982 Training Acc:    86.69    86.69    86.69%\n",
      "Epoch   2741 Step:      0 Loss:   0.2982 Training Acc:    86.71    86.71    86.71%\n",
      "Epoch   2742 Step:      0 Loss:   0.2982 Training Acc:    86.69    86.69    86.69%\n",
      "Epoch   2743 Step:      0 Loss:   0.2981 Training Acc:    86.72    86.72    86.72%\n",
      "Epoch   2744 Step:      0 Loss:   0.2980 Training Acc:    86.71    86.71    86.71%\n",
      "Epoch   2745 Step:      0 Loss:   0.2978 Training Acc:    86.73    86.73    86.73%\n",
      "Epoch   2746 Step:      0 Loss:   0.2977 Training Acc:    86.73    86.73    86.73%\n",
      "Epoch   2747 Step:      0 Loss:   0.2977 Training Acc:    86.73    86.73    86.73%\n",
      "Epoch   2748 Step:      0 Loss:   0.2976 Training Acc:    86.74    86.74    86.74%\n",
      "Epoch   2749 Step:      0 Loss:   0.2976 Training Acc:    86.73    86.73    86.73%\n",
      "Epoch   2750 Step:      0 Loss:   0.2976 Training Acc:    86.75    86.75    86.75%\n",
      "Epoch   2751 Step:      0 Loss:   0.2975 Training Acc:    86.74    86.74    86.74%\n",
      "Epoch   2752 Step:      0 Loss:   0.2975 Training Acc:    86.74    86.74    86.74%\n",
      "Epoch   2753 Step:      0 Loss:   0.2975 Training Acc:    86.74    86.74    86.74%\n",
      "Epoch   2754 Step:      0 Loss:   0.2975 Training Acc:    86.74    86.74    86.74%\n",
      "Epoch   2755 Step:      0 Loss:   0.2976 Training Acc:    86.74    86.74    86.74%\n",
      "Epoch   2756 Step:      0 Loss:   0.2978 Training Acc:    86.71    86.71    86.71%\n",
      "Epoch   2757 Step:      0 Loss:   0.2980 Training Acc:    86.72    86.72    86.72%\n",
      "Epoch   2758 Step:      0 Loss:   0.2985 Training Acc:    86.62    86.62    86.62%\n",
      "Epoch   2759 Step:      0 Loss:   0.2991 Training Acc:    86.63    86.63    86.63%\n",
      "Epoch   2760 Step:      0 Loss:   0.3001 Training Acc:    86.47    86.47    86.47%\n",
      "Epoch   2761 Step:      0 Loss:   0.3012 Training Acc:    86.50    86.50    86.50%\n",
      "Epoch   2762 Step:      0 Loss:   0.3023 Training Acc:    86.29    86.29    86.29%\n",
      "Epoch   2763 Step:      0 Loss:   0.3030 Training Acc:    86.36    86.36    86.36%\n",
      "Epoch   2764 Step:      0 Loss:   0.3028 Training Acc:    86.27    86.27    86.27%\n",
      "Epoch   2765 Step:      0 Loss:   0.3013 Training Acc:    86.47    86.47    86.47%\n",
      "Epoch   2766 Step:      0 Loss:   0.2994 Training Acc:    86.52    86.52    86.52%\n",
      "Epoch   2767 Step:      0 Loss:   0.2978 Training Acc:    86.67    86.67    86.67%\n",
      "Epoch   2768 Step:      0 Loss:   0.2972 Training Acc:    86.74    86.74    86.74%\n",
      "Epoch   2769 Step:      0 Loss:   0.2976 Training Acc:    86.71    86.71    86.71%\n",
      "Epoch   2770 Step:      0 Loss:   0.2982 Training Acc:    86.71    86.71    86.71%\n",
      "Epoch   2771 Step:      0 Loss:   0.2986 Training Acc:    86.60    86.60    86.60%\n",
      "Epoch   2772 Step:      0 Loss:   0.2982 Training Acc:    86.69    86.69    86.69%\n",
      "Epoch   2773 Step:      0 Loss:   0.2974 Training Acc:    86.66    86.66    86.66%\n",
      "Epoch   2774 Step:      0 Loss:   0.2966 Training Acc:    86.77    86.77    86.77%\n",
      "Epoch   2775 Step:      0 Loss:   0.2964 Training Acc:    86.78    86.78    86.78%\n",
      "Epoch   2776 Step:      0 Loss:   0.2966 Training Acc:    86.76    86.76    86.76%\n",
      "Epoch   2777 Step:      0 Loss:   0.2970 Training Acc:    86.77    86.77    86.77%\n",
      "Epoch   2778 Step:      0 Loss:   0.2972 Training Acc:    86.72    86.72    86.72%\n",
      "Epoch   2779 Step:      0 Loss:   0.2969 Training Acc:    86.79    86.79    86.79%\n",
      "Epoch   2780 Step:      0 Loss:   0.2964 Training Acc:    86.78    86.78    86.78%\n",
      "Epoch   2781 Step:      0 Loss:   0.2958 Training Acc:    86.85    86.85    86.85%\n",
      "Epoch   2782 Step:      0 Loss:   0.2955 Training Acc:    86.85    86.85    86.85%\n",
      "Epoch   2783 Step:      0 Loss:   0.2955 Training Acc:    86.85    86.85    86.85%\n",
      "Epoch   2784 Step:      0 Loss:   0.2958 Training Acc:    86.84    86.84    86.84%\n",
      "Epoch   2785 Step:      0 Loss:   0.2960 Training Acc:    86.80    86.80    86.80%\n",
      "Epoch   2786 Step:      0 Loss:   0.2960 Training Acc:    86.83    86.83    86.83%\n",
      "Epoch   2787 Step:      0 Loss:   0.2958 Training Acc:    86.83    86.83    86.83%\n",
      "Epoch   2788 Step:      0 Loss:   0.2954 Training Acc:    86.85    86.85    86.85%\n",
      "Epoch   2789 Step:      0 Loss:   0.2951 Training Acc:    86.86    86.86    86.86%\n",
      "Epoch   2790 Step:      0 Loss:   0.2950 Training Acc:    86.88    86.88    86.88%\n",
      "Epoch   2791 Step:      0 Loss:   0.2950 Training Acc:    86.88    86.88    86.88%\n",
      "Epoch   2792 Step:      0 Loss:   0.2950 Training Acc:    86.88    86.88    86.88%\n",
      "Epoch   2793 Step:      0 Loss:   0.2951 Training Acc:    86.89    86.89    86.89%\n",
      "Epoch   2794 Step:      0 Loss:   0.2950 Training Acc:    86.88    86.88    86.88%\n",
      "Epoch   2795 Step:      0 Loss:   0.2949 Training Acc:    86.90    86.90    86.90%\n",
      "Epoch   2796 Step:      0 Loss:   0.2947 Training Acc:    86.90    86.90    86.90%\n",
      "Epoch   2797 Step:      0 Loss:   0.2945 Training Acc:    86.91    86.91    86.91%\n",
      "Epoch   2798 Step:      0 Loss:   0.2944 Training Acc:    86.89    86.89    86.89%\n",
      "Epoch   2799 Step:      0 Loss:   0.2944 Training Acc:    86.91    86.91    86.91%\n",
      "Epoch   2800 Step:      0 Loss:   0.2944 Training Acc:    86.91    86.91    86.91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2801 Step:      0 Loss:   0.2944 Training Acc:    86.92    86.92    86.92%\n",
      "Epoch   2802 Step:      0 Loss:   0.2944 Training Acc:    86.91    86.91    86.91%\n",
      "Epoch   2803 Step:      0 Loss:   0.2943 Training Acc:    86.93    86.93    86.93%\n",
      "Epoch   2804 Step:      0 Loss:   0.2942 Training Acc:    86.92    86.92    86.92%\n",
      "Epoch   2805 Step:      0 Loss:   0.2941 Training Acc:    86.94    86.94    86.94%\n",
      "Epoch   2806 Step:      0 Loss:   0.2940 Training Acc:    86.94    86.94    86.94%\n",
      "Epoch   2807 Step:      0 Loss:   0.2939 Training Acc:    86.94    86.94    86.94%\n",
      "Epoch   2808 Step:      0 Loss:   0.2938 Training Acc:    86.94    86.94    86.94%\n",
      "Epoch   2809 Step:      0 Loss:   0.2938 Training Acc:    86.95    86.95    86.95%\n",
      "Epoch   2810 Step:      0 Loss:   0.2937 Training Acc:    86.95    86.95    86.95%\n",
      "Epoch   2811 Step:      0 Loss:   0.2937 Training Acc:    86.96    86.96    86.96%\n",
      "Epoch   2812 Step:      0 Loss:   0.2936 Training Acc:    86.95    86.95    86.95%\n",
      "Epoch   2813 Step:      0 Loss:   0.2936 Training Acc:    86.96    86.96    86.96%\n",
      "Epoch   2814 Step:      0 Loss:   0.2935 Training Acc:    86.96    86.96    86.96%\n",
      "Epoch   2815 Step:      0 Loss:   0.2934 Training Acc:    86.97    86.97    86.97%\n",
      "Epoch   2816 Step:      0 Loss:   0.2933 Training Acc:    86.97    86.97    86.97%\n",
      "Epoch   2817 Step:      0 Loss:   0.2933 Training Acc:    86.97    86.97    86.97%\n",
      "Epoch   2818 Step:      0 Loss:   0.2932 Training Acc:    86.97    86.97    86.97%\n",
      "Epoch   2819 Step:      0 Loss:   0.2932 Training Acc:    86.97    86.97    86.97%\n",
      "Epoch   2820 Step:      0 Loss:   0.2932 Training Acc:    87.00    87.00    87.00%\n",
      "Epoch   2821 Step:      0 Loss:   0.2931 Training Acc:    86.98    86.98    86.98%\n",
      "Epoch   2822 Step:      0 Loss:   0.2931 Training Acc:    86.98    86.98    86.98%\n",
      "Epoch   2823 Step:      0 Loss:   0.2931 Training Acc:    86.97    86.97    86.97%\n",
      "Epoch   2824 Step:      0 Loss:   0.2932 Training Acc:    86.96    86.96    86.96%\n",
      "Epoch   2825 Step:      0 Loss:   0.2933 Training Acc:    86.93    86.93    86.93%\n",
      "Epoch   2826 Step:      0 Loss:   0.2934 Training Acc:    86.93    86.93    86.93%\n",
      "Epoch   2827 Step:      0 Loss:   0.2937 Training Acc:    86.88    86.88    86.88%\n",
      "Epoch   2828 Step:      0 Loss:   0.2941 Training Acc:    86.87    86.87    86.87%\n",
      "Epoch   2829 Step:      0 Loss:   0.2947 Training Acc:    86.80    86.80    86.80%\n",
      "Epoch   2830 Step:      0 Loss:   0.2956 Training Acc:    86.76    86.76    86.76%\n",
      "Epoch   2831 Step:      0 Loss:   0.2968 Training Acc:    86.63    86.63    86.63%\n",
      "Epoch   2832 Step:      0 Loss:   0.2980 Training Acc:    86.58    86.58    86.58%\n",
      "Epoch   2833 Step:      0 Loss:   0.2996 Training Acc:    86.44    86.44    86.44%\n",
      "Epoch   2834 Step:      0 Loss:   0.3001 Training Acc:    86.43    86.43    86.43%\n",
      "Epoch   2835 Step:      0 Loss:   0.3001 Training Acc:    86.39    86.39    86.39%\n",
      "Epoch   2836 Step:      0 Loss:   0.2978 Training Acc:    86.61    86.61    86.61%\n",
      "Epoch   2837 Step:      0 Loss:   0.2951 Training Acc:    86.77    86.77    86.77%\n",
      "Epoch   2838 Step:      0 Loss:   0.2927 Training Acc:    86.98    86.98    86.98%\n",
      "Epoch   2839 Step:      0 Loss:   0.2923 Training Acc:    87.02    87.02    87.02%\n",
      "Epoch   2840 Step:      0 Loss:   0.2934 Training Acc:    86.94    86.94    86.94%\n",
      "Epoch   2841 Step:      0 Loss:   0.2946 Training Acc:    86.81    86.81    86.81%\n",
      "Epoch   2842 Step:      0 Loss:   0.2949 Training Acc:    86.78    86.78    86.78%\n",
      "Epoch   2843 Step:      0 Loss:   0.2940 Training Acc:    86.86    86.86    86.86%\n",
      "Epoch   2844 Step:      0 Loss:   0.2926 Training Acc:    86.94    86.94    86.94%\n",
      "Epoch   2845 Step:      0 Loss:   0.2918 Training Acc:    87.04    87.04    87.04%\n",
      "Epoch   2846 Step:      0 Loss:   0.2921 Training Acc:    87.02    87.02    87.02%\n",
      "Epoch   2847 Step:      0 Loss:   0.2928 Training Acc:    86.98    86.98    86.98%\n",
      "Epoch   2848 Step:      0 Loss:   0.2932 Training Acc:    86.90    86.90    86.90%\n",
      "Epoch   2849 Step:      0 Loss:   0.2928 Training Acc:    86.95    86.95    86.95%\n",
      "Epoch   2850 Step:      0 Loss:   0.2919 Training Acc:    87.03    87.03    87.03%\n",
      "Epoch   2851 Step:      0 Loss:   0.2913 Training Acc:    87.09    87.09    87.09%\n",
      "Epoch   2852 Step:      0 Loss:   0.2912 Training Acc:    87.09    87.09    87.09%\n",
      "Epoch   2853 Step:      0 Loss:   0.2916 Training Acc:    87.03    87.03    87.03%\n",
      "Epoch   2854 Step:      0 Loss:   0.2919 Training Acc:    87.02    87.02    87.02%\n",
      "Epoch   2855 Step:      0 Loss:   0.2919 Training Acc:    87.01    87.01    87.01%\n",
      "Epoch   2856 Step:      0 Loss:   0.2915 Training Acc:    87.07    87.07    87.07%\n",
      "Epoch   2857 Step:      0 Loss:   0.2910 Training Acc:    87.08    87.08    87.08%\n",
      "Epoch   2858 Step:      0 Loss:   0.2908 Training Acc:    87.13    87.13    87.13%\n",
      "Epoch   2859 Step:      0 Loss:   0.2908 Training Acc:    87.12    87.12    87.12%\n",
      "Epoch   2860 Step:      0 Loss:   0.2910 Training Acc:    87.08    87.08    87.08%\n",
      "Epoch   2861 Step:      0 Loss:   0.2911 Training Acc:    87.07    87.07    87.07%\n",
      "Epoch   2862 Step:      0 Loss:   0.2909 Training Acc:    87.08    87.08    87.08%\n",
      "Epoch   2863 Step:      0 Loss:   0.2907 Training Acc:    87.11    87.11    87.11%\n",
      "Epoch   2864 Step:      0 Loss:   0.2904 Training Acc:    87.13    87.13    87.13%\n",
      "Epoch   2865 Step:      0 Loss:   0.2903 Training Acc:    87.15    87.15    87.15%\n",
      "Epoch   2866 Step:      0 Loss:   0.2903 Training Acc:    87.15    87.15    87.15%\n",
      "Epoch   2867 Step:      0 Loss:   0.2904 Training Acc:    87.12    87.12    87.12%\n",
      "Epoch   2868 Step:      0 Loss:   0.2904 Training Acc:    87.11    87.11    87.11%\n",
      "Epoch   2869 Step:      0 Loss:   0.2903 Training Acc:    87.14    87.14    87.14%\n",
      "Epoch   2870 Step:      0 Loss:   0.2901 Training Acc:    87.14    87.14    87.14%\n",
      "Epoch   2871 Step:      0 Loss:   0.2900 Training Acc:    87.16    87.16    87.16%\n",
      "Epoch   2872 Step:      0 Loss:   0.2899 Training Acc:    87.16    87.16    87.16%\n",
      "Epoch   2873 Step:      0 Loss:   0.2899 Training Acc:    87.16    87.16    87.16%\n",
      "Epoch   2874 Step:      0 Loss:   0.2899 Training Acc:    87.16    87.16    87.16%\n",
      "Epoch   2875 Step:      0 Loss:   0.2898 Training Acc:    87.18    87.18    87.18%\n",
      "Epoch   2876 Step:      0 Loss:   0.2898 Training Acc:    87.16    87.16    87.16%\n",
      "Epoch   2877 Step:      0 Loss:   0.2897 Training Acc:    87.19    87.19    87.19%\n",
      "Epoch   2878 Step:      0 Loss:   0.2895 Training Acc:    87.18    87.18    87.18%\n",
      "Epoch   2879 Step:      0 Loss:   0.2895 Training Acc:    87.19    87.19    87.19%\n",
      "Epoch   2880 Step:      0 Loss:   0.2894 Training Acc:    87.19    87.19    87.19%\n",
      "Epoch   2881 Step:      0 Loss:   0.2894 Training Acc:    87.20    87.20    87.20%\n",
      "Epoch   2882 Step:      0 Loss:   0.2893 Training Acc:    87.19    87.19    87.19%\n",
      "Epoch   2883 Step:      0 Loss:   0.2893 Training Acc:    87.20    87.20    87.20%\n",
      "Epoch   2884 Step:      0 Loss:   0.2892 Training Acc:    87.19    87.19    87.19%\n",
      "Epoch   2885 Step:      0 Loss:   0.2891 Training Acc:    87.22    87.22    87.22%\n",
      "Epoch   2886 Step:      0 Loss:   0.2891 Training Acc:    87.19    87.19    87.19%\n",
      "Epoch   2887 Step:      0 Loss:   0.2890 Training Acc:    87.22    87.22    87.22%\n",
      "Epoch   2888 Step:      0 Loss:   0.2889 Training Acc:    87.20    87.20    87.20%\n",
      "Epoch   2889 Step:      0 Loss:   0.2889 Training Acc:    87.22    87.22    87.22%\n",
      "Epoch   2890 Step:      0 Loss:   0.2889 Training Acc:    87.20    87.20    87.20%\n",
      "Epoch   2891 Step:      0 Loss:   0.2888 Training Acc:    87.23    87.23    87.23%\n",
      "Epoch   2892 Step:      0 Loss:   0.2888 Training Acc:    87.21    87.21    87.21%\n",
      "Epoch   2893 Step:      0 Loss:   0.2888 Training Acc:    87.24    87.24    87.24%\n",
      "Epoch   2894 Step:      0 Loss:   0.2888 Training Acc:    87.19    87.19    87.19%\n",
      "Epoch   2895 Step:      0 Loss:   0.2888 Training Acc:    87.24    87.24    87.24%\n",
      "Epoch   2896 Step:      0 Loss:   0.2889 Training Acc:    87.19    87.19    87.19%\n",
      "Epoch   2897 Step:      0 Loss:   0.2890 Training Acc:    87.22    87.22    87.22%\n",
      "Epoch   2898 Step:      0 Loss:   0.2893 Training Acc:    87.14    87.14    87.14%\n",
      "Epoch   2899 Step:      0 Loss:   0.2896 Training Acc:    87.19    87.19    87.19%\n",
      "Epoch   2900 Step:      0 Loss:   0.2901 Training Acc:    87.06    87.06    87.06%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2901 Step:      0 Loss:   0.2908 Training Acc:    87.09    87.09    87.09%\n",
      "Epoch   2902 Step:      0 Loss:   0.2915 Training Acc:    86.94    86.94    86.94%\n",
      "Epoch   2903 Step:      0 Loss:   0.2921 Training Acc:    86.99    86.99    86.99%\n",
      "Epoch   2904 Step:      0 Loss:   0.2925 Training Acc:    86.85    86.85    86.85%\n",
      "Epoch   2905 Step:      0 Loss:   0.2922 Training Acc:    86.97    86.97    86.97%\n",
      "Epoch   2906 Step:      0 Loss:   0.2913 Training Acc:    86.93    86.93    86.93%\n",
      "Epoch   2907 Step:      0 Loss:   0.2899 Training Acc:    87.13    87.13    87.13%\n",
      "Epoch   2908 Step:      0 Loss:   0.2886 Training Acc:    87.16    87.16    87.16%\n",
      "Epoch   2909 Step:      0 Loss:   0.2878 Training Acc:    87.27    87.27    87.27%\n",
      "Epoch   2910 Step:      0 Loss:   0.2878 Training Acc:    87.29    87.29    87.29%\n",
      "Epoch   2911 Step:      0 Loss:   0.2882 Training Acc:    87.22    87.22    87.22%\n",
      "Epoch   2912 Step:      0 Loss:   0.2887 Training Acc:    87.23    87.23    87.23%\n",
      "Epoch   2913 Step:      0 Loss:   0.2891 Training Acc:    87.14    87.14    87.14%\n",
      "Epoch   2914 Step:      0 Loss:   0.2890 Training Acc:    87.20    87.20    87.20%\n",
      "Epoch   2915 Step:      0 Loss:   0.2886 Training Acc:    87.15    87.15    87.15%\n",
      "Epoch   2916 Step:      0 Loss:   0.2881 Training Acc:    87.26    87.26    87.26%\n",
      "Epoch   2917 Step:      0 Loss:   0.2876 Training Acc:    87.24    87.24    87.24%\n",
      "Epoch   2918 Step:      0 Loss:   0.2873 Training Acc:    87.28    87.28    87.28%\n",
      "Epoch   2919 Step:      0 Loss:   0.2873 Training Acc:    87.31    87.31    87.31%\n",
      "Epoch   2920 Step:      0 Loss:   0.2874 Training Acc:    87.26    87.26    87.26%\n",
      "Epoch   2921 Step:      0 Loss:   0.2875 Training Acc:    87.30    87.30    87.30%\n",
      "Epoch   2922 Step:      0 Loss:   0.2875 Training Acc:    87.26    87.26    87.26%\n",
      "Epoch   2923 Step:      0 Loss:   0.2874 Training Acc:    87.30    87.30    87.30%\n",
      "Epoch   2924 Step:      0 Loss:   0.2872 Training Acc:    87.26    87.26    87.26%\n",
      "Epoch   2925 Step:      0 Loss:   0.2870 Training Acc:    87.31    87.31    87.31%\n",
      "Epoch   2926 Step:      0 Loss:   0.2868 Training Acc:    87.31    87.31    87.31%\n",
      "Epoch   2927 Step:      0 Loss:   0.2867 Training Acc:    87.32    87.32    87.32%\n",
      "Epoch   2928 Step:      0 Loss:   0.2867 Training Acc:    87.34    87.34    87.34%\n",
      "Epoch   2929 Step:      0 Loss:   0.2867 Training Acc:    87.29    87.29    87.29%\n",
      "Epoch   2930 Step:      0 Loss:   0.2867 Training Acc:    87.34    87.34    87.34%\n",
      "Epoch   2931 Step:      0 Loss:   0.2867 Training Acc:    87.30    87.30    87.30%\n",
      "Epoch   2932 Step:      0 Loss:   0.2866 Training Acc:    87.35    87.35    87.35%\n",
      "Epoch   2933 Step:      0 Loss:   0.2864 Training Acc:    87.32    87.32    87.32%\n",
      "Epoch   2934 Step:      0 Loss:   0.2863 Training Acc:    87.37    87.37    87.37%\n",
      "Epoch   2935 Step:      0 Loss:   0.2861 Training Acc:    87.35    87.35    87.35%\n",
      "Epoch   2936 Step:      0 Loss:   0.2860 Training Acc:    87.37    87.37    87.37%\n",
      "Epoch   2937 Step:      0 Loss:   0.2860 Training Acc:    87.38    87.38    87.38%\n",
      "Epoch   2938 Step:      0 Loss:   0.2859 Training Acc:    87.36    87.36    87.36%\n",
      "Epoch   2939 Step:      0 Loss:   0.2859 Training Acc:    87.38    87.38    87.38%\n",
      "Epoch   2940 Step:      0 Loss:   0.2859 Training Acc:    87.34    87.34    87.34%\n",
      "Epoch   2941 Step:      0 Loss:   0.2859 Training Acc:    87.38    87.38    87.38%\n",
      "Epoch   2942 Step:      0 Loss:   0.2858 Training Acc:    87.34    87.34    87.34%\n",
      "Epoch   2943 Step:      0 Loss:   0.2858 Training Acc:    87.39    87.39    87.39%\n",
      "Epoch   2944 Step:      0 Loss:   0.2857 Training Acc:    87.35    87.35    87.35%\n",
      "Epoch   2945 Step:      0 Loss:   0.2856 Training Acc:    87.40    87.40    87.40%\n",
      "Epoch   2946 Step:      0 Loss:   0.2855 Training Acc:    87.37    87.37    87.37%\n",
      "Epoch   2947 Step:      0 Loss:   0.2854 Training Acc:    87.41    87.41    87.41%\n",
      "Epoch   2948 Step:      0 Loss:   0.2853 Training Acc:    87.39    87.39    87.39%\n",
      "Epoch   2949 Step:      0 Loss:   0.2853 Training Acc:    87.41    87.41    87.41%\n",
      "Epoch   2950 Step:      0 Loss:   0.2852 Training Acc:    87.41    87.41    87.41%\n",
      "Epoch   2951 Step:      0 Loss:   0.2852 Training Acc:    87.40    87.40    87.40%\n",
      "Epoch   2952 Step:      0 Loss:   0.2851 Training Acc:    87.41    87.41    87.41%\n",
      "Epoch   2953 Step:      0 Loss:   0.2851 Training Acc:    87.40    87.40    87.40%\n",
      "Epoch   2954 Step:      0 Loss:   0.2850 Training Acc:    87.41    87.41    87.41%\n",
      "Epoch   2955 Step:      0 Loss:   0.2850 Training Acc:    87.39    87.39    87.39%\n",
      "Epoch   2956 Step:      0 Loss:   0.2849 Training Acc:    87.42    87.42    87.42%\n",
      "Epoch   2957 Step:      0 Loss:   0.2849 Training Acc:    87.40    87.40    87.40%\n",
      "Epoch   2958 Step:      0 Loss:   0.2849 Training Acc:    87.42    87.42    87.42%\n",
      "Epoch   2959 Step:      0 Loss:   0.2848 Training Acc:    87.41    87.41    87.41%\n",
      "Epoch   2960 Step:      0 Loss:   0.2848 Training Acc:    87.43    87.43    87.43%\n",
      "Epoch   2961 Step:      0 Loss:   0.2848 Training Acc:    87.40    87.40    87.40%\n",
      "Epoch   2962 Step:      0 Loss:   0.2848 Training Acc:    87.42    87.42    87.42%\n",
      "Epoch   2963 Step:      0 Loss:   0.2849 Training Acc:    87.39    87.39    87.39%\n",
      "Epoch   2964 Step:      0 Loss:   0.2850 Training Acc:    87.41    87.41    87.41%\n",
      "Epoch   2965 Step:      0 Loss:   0.2852 Training Acc:    87.36    87.36    87.36%\n",
      "Epoch   2966 Step:      0 Loss:   0.2855 Training Acc:    87.36    87.36    87.36%\n",
      "Epoch   2967 Step:      0 Loss:   0.2860 Training Acc:    87.28    87.28    87.28%\n",
      "Epoch   2968 Step:      0 Loss:   0.2866 Training Acc:    87.27    87.27    87.27%\n",
      "Epoch   2969 Step:      0 Loss:   0.2875 Training Acc:    87.15    87.15    87.15%\n",
      "Epoch   2970 Step:      0 Loss:   0.2886 Training Acc:    87.11    87.11    87.11%\n",
      "Epoch   2971 Step:      0 Loss:   0.2899 Training Acc:    86.93    86.93    86.93%\n",
      "Epoch   2972 Step:      0 Loss:   0.2909 Training Acc:    86.93    86.93    86.93%\n",
      "Epoch   2973 Step:      0 Loss:   0.2917 Training Acc:    86.80    86.80    86.80%\n",
      "Epoch   2974 Step:      0 Loss:   0.2909 Training Acc:    86.93    86.93    86.93%\n",
      "Epoch   2975 Step:      0 Loss:   0.2893 Training Acc:    86.99    86.99    86.99%\n",
      "Epoch   2976 Step:      0 Loss:   0.2865 Training Acc:    87.26    87.26    87.26%\n",
      "Epoch   2977 Step:      0 Loss:   0.2843 Training Acc:    87.43    87.43    87.43%\n",
      "Epoch   2978 Step:      0 Loss:   0.2837 Training Acc:    87.49    87.49    87.49%\n",
      "Epoch   2979 Step:      0 Loss:   0.2845 Training Acc:    87.39    87.39    87.39%\n",
      "Epoch   2980 Step:      0 Loss:   0.2859 Training Acc:    87.30    87.30    87.30%\n",
      "Epoch   2981 Step:      0 Loss:   0.2864 Training Acc:    87.26    87.26    87.26%\n",
      "Epoch   2982 Step:      0 Loss:   0.2860 Training Acc:    87.24    87.24    87.24%\n",
      "Epoch   2983 Step:      0 Loss:   0.2848 Training Acc:    87.39    87.39    87.39%\n",
      "Epoch   2984 Step:      0 Loss:   0.2837 Training Acc:    87.44    87.44    87.44%\n",
      "Epoch   2985 Step:      0 Loss:   0.2833 Training Acc:    87.51    87.51    87.51%\n",
      "Epoch   2986 Step:      0 Loss:   0.2838 Training Acc:    87.44    87.44    87.44%\n",
      "Epoch   2987 Step:      0 Loss:   0.2844 Training Acc:    87.42    87.42    87.42%\n",
      "Epoch   2988 Step:      0 Loss:   0.2846 Training Acc:    87.36    87.36    87.36%\n",
      "Epoch   2989 Step:      0 Loss:   0.2842 Training Acc:    87.42    87.42    87.42%\n",
      "Epoch   2990 Step:      0 Loss:   0.2835 Training Acc:    87.46    87.46    87.46%\n",
      "Epoch   2991 Step:      0 Loss:   0.2829 Training Acc:    87.53    87.53    87.53%\n",
      "Epoch   2992 Step:      0 Loss:   0.2828 Training Acc:    87.52    87.52    87.52%\n",
      "Epoch   2993 Step:      0 Loss:   0.2830 Training Acc:    87.48    87.48    87.48%\n",
      "Epoch   2994 Step:      0 Loss:   0.2833 Training Acc:    87.51    87.51    87.51%\n",
      "Epoch   2995 Step:      0 Loss:   0.2834 Training Acc:    87.47    87.47    87.47%\n",
      "Epoch   2996 Step:      0 Loss:   0.2832 Training Acc:    87.52    87.52    87.52%\n",
      "Epoch   2997 Step:      0 Loss:   0.2828 Training Acc:    87.50    87.50    87.50%\n",
      "Epoch   2998 Step:      0 Loss:   0.2825 Training Acc:    87.55    87.55    87.55%\n",
      "Epoch   2999 Step:      0 Loss:   0.2824 Training Acc:    87.53    87.53    87.53%\n",
      "Epoch   3000 Step:      0 Loss:   0.2824 Training Acc:    87.53    87.53    87.53%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3001 Step:      0 Loss:   0.2826 Training Acc:    87.55    87.55    87.55%\n",
      "Epoch   3002 Step:      0 Loss:   0.2826 Training Acc:    87.51    87.51    87.51%\n",
      "Epoch   3003 Step:      0 Loss:   0.2825 Training Acc:    87.54    87.54    87.54%\n",
      "Epoch   3004 Step:      0 Loss:   0.2823 Training Acc:    87.53    87.53    87.53%\n",
      "Epoch   3005 Step:      0 Loss:   0.2821 Training Acc:    87.55    87.55    87.55%\n",
      "Epoch   3006 Step:      0 Loss:   0.2819 Training Acc:    87.55    87.55    87.55%\n",
      "Epoch   3007 Step:      0 Loss:   0.2819 Training Acc:    87.54    87.54    87.54%\n",
      "Epoch   3008 Step:      0 Loss:   0.2819 Training Acc:    87.58    87.58    87.58%\n",
      "Epoch   3009 Step:      0 Loss:   0.2819 Training Acc:    87.55    87.55    87.55%\n",
      "Epoch   3010 Step:      0 Loss:   0.2819 Training Acc:    87.58    87.58    87.58%\n",
      "Epoch   3011 Step:      0 Loss:   0.2818 Training Acc:    87.55    87.55    87.55%\n",
      "Epoch   3012 Step:      0 Loss:   0.2817 Training Acc:    87.59    87.59    87.59%\n",
      "Epoch   3013 Step:      0 Loss:   0.2816 Training Acc:    87.55    87.55    87.55%\n",
      "Epoch   3014 Step:      0 Loss:   0.2815 Training Acc:    87.58    87.58    87.58%\n",
      "Epoch   3015 Step:      0 Loss:   0.2814 Training Acc:    87.57    87.57    87.57%\n",
      "Epoch   3016 Step:      0 Loss:   0.2814 Training Acc:    87.58    87.58    87.58%\n",
      "Epoch   3017 Step:      0 Loss:   0.2814 Training Acc:    87.59    87.59    87.59%\n",
      "Epoch   3018 Step:      0 Loss:   0.2814 Training Acc:    87.59    87.59    87.59%\n",
      "Epoch   3019 Step:      0 Loss:   0.2813 Training Acc:    87.58    87.58    87.58%\n",
      "Epoch   3020 Step:      0 Loss:   0.2813 Training Acc:    87.61    87.61    87.61%\n",
      "Epoch   3021 Step:      0 Loss:   0.2812 Training Acc:    87.58    87.58    87.58%\n",
      "Epoch   3022 Step:      0 Loss:   0.2812 Training Acc:    87.63    87.63    87.63%\n",
      "Epoch   3023 Step:      0 Loss:   0.2812 Training Acc:    87.56    87.56    87.56%\n",
      "Epoch   3024 Step:      0 Loss:   0.2813 Training Acc:    87.63    87.63    87.63%\n",
      "Epoch   3025 Step:      0 Loss:   0.2814 Training Acc:    87.52    87.52    87.52%\n",
      "Epoch   3026 Step:      0 Loss:   0.2816 Training Acc:    87.61    87.61    87.61%\n",
      "Epoch   3027 Step:      0 Loss:   0.2819 Training Acc:    87.49    87.49    87.49%\n",
      "Epoch   3028 Step:      0 Loss:   0.2822 Training Acc:    87.56    87.56    87.56%\n",
      "Epoch   3029 Step:      0 Loss:   0.2827 Training Acc:    87.41    87.41    87.41%\n",
      "Epoch   3030 Step:      0 Loss:   0.2832 Training Acc:    87.47    87.47    87.47%\n",
      "Epoch   3031 Step:      0 Loss:   0.2838 Training Acc:    87.33    87.33    87.33%\n",
      "Epoch   3032 Step:      0 Loss:   0.2840 Training Acc:    87.43    87.43    87.43%\n",
      "Epoch   3033 Step:      0 Loss:   0.2841 Training Acc:    87.30    87.30    87.30%\n",
      "Epoch   3034 Step:      0 Loss:   0.2835 Training Acc:    87.47    87.47    87.47%\n",
      "Epoch   3035 Step:      0 Loss:   0.2826 Training Acc:    87.42    87.42    87.42%\n",
      "Epoch   3036 Step:      0 Loss:   0.2815 Training Acc:    87.59    87.59    87.59%\n",
      "Epoch   3037 Step:      0 Loss:   0.2806 Training Acc:    87.58    87.58    87.58%\n",
      "Epoch   3038 Step:      0 Loss:   0.2802 Training Acc:    87.62    87.62    87.62%\n",
      "Epoch   3039 Step:      0 Loss:   0.2802 Training Acc:    87.66    87.66    87.66%\n",
      "Epoch   3040 Step:      0 Loss:   0.2806 Training Acc:    87.56    87.56    87.56%\n",
      "Epoch   3041 Step:      0 Loss:   0.2810 Training Acc:    87.64    87.64    87.64%\n",
      "Epoch   3042 Step:      0 Loss:   0.2812 Training Acc:    87.51    87.51    87.51%\n",
      "Epoch   3043 Step:      0 Loss:   0.2812 Training Acc:    87.61    87.61    87.61%\n",
      "Epoch   3044 Step:      0 Loss:   0.2809 Training Acc:    87.53    87.53    87.53%\n",
      "Epoch   3045 Step:      0 Loss:   0.2805 Training Acc:    87.66    87.66    87.66%\n",
      "Epoch   3046 Step:      0 Loss:   0.2800 Training Acc:    87.62    87.62    87.62%\n",
      "Epoch   3047 Step:      0 Loss:   0.2798 Training Acc:    87.66    87.66    87.66%\n",
      "Epoch   3048 Step:      0 Loss:   0.2796 Training Acc:    87.69    87.69    87.69%\n",
      "Epoch   3049 Step:      0 Loss:   0.2796 Training Acc:    87.64    87.64    87.64%\n",
      "Epoch   3050 Step:      0 Loss:   0.2797 Training Acc:    87.68    87.68    87.68%\n",
      "Epoch   3051 Step:      0 Loss:   0.2798 Training Acc:    87.61    87.61    87.61%\n",
      "Epoch   3052 Step:      0 Loss:   0.2798 Training Acc:    87.70    87.70    87.70%\n",
      "Epoch   3053 Step:      0 Loss:   0.2797 Training Acc:    87.61    87.61    87.61%\n",
      "Epoch   3054 Step:      0 Loss:   0.2796 Training Acc:    87.70    87.70    87.70%\n",
      "Epoch   3055 Step:      0 Loss:   0.2794 Training Acc:    87.66    87.66    87.66%\n",
      "Epoch   3056 Step:      0 Loss:   0.2792 Training Acc:    87.71    87.71    87.71%\n",
      "Epoch   3057 Step:      0 Loss:   0.2791 Training Acc:    87.70    87.70    87.70%\n",
      "Epoch   3058 Step:      0 Loss:   0.2790 Training Acc:    87.69    87.69    87.69%\n",
      "Epoch   3059 Step:      0 Loss:   0.2790 Training Acc:    87.72    87.72    87.72%\n",
      "Epoch   3060 Step:      0 Loss:   0.2790 Training Acc:    87.68    87.68    87.68%\n",
      "Epoch   3061 Step:      0 Loss:   0.2790 Training Acc:    87.72    87.72    87.72%\n",
      "Epoch   3062 Step:      0 Loss:   0.2789 Training Acc:    87.68    87.68    87.68%\n",
      "Epoch   3063 Step:      0 Loss:   0.2789 Training Acc:    87.73    87.73    87.73%\n",
      "Epoch   3064 Step:      0 Loss:   0.2788 Training Acc:    87.69    87.69    87.69%\n",
      "Epoch   3065 Step:      0 Loss:   0.2787 Training Acc:    87.74    87.74    87.74%\n",
      "Epoch   3066 Step:      0 Loss:   0.2786 Training Acc:    87.71    87.71    87.71%\n",
      "Epoch   3067 Step:      0 Loss:   0.2785 Training Acc:    87.74    87.74    87.74%\n",
      "Epoch   3068 Step:      0 Loss:   0.2784 Training Acc:    87.73    87.73    87.73%\n",
      "Epoch   3069 Step:      0 Loss:   0.2783 Training Acc:    87.75    87.75    87.75%\n",
      "Epoch   3070 Step:      0 Loss:   0.2783 Training Acc:    87.75    87.75    87.75%\n",
      "Epoch   3071 Step:      0 Loss:   0.2782 Training Acc:    87.74    87.74    87.74%\n",
      "Epoch   3072 Step:      0 Loss:   0.2782 Training Acc:    87.76    87.76    87.76%\n",
      "Epoch   3073 Step:      0 Loss:   0.2781 Training Acc:    87.74    87.74    87.74%\n",
      "Epoch   3074 Step:      0 Loss:   0.2781 Training Acc:    87.77    87.77    87.77%\n",
      "Epoch   3075 Step:      0 Loss:   0.2781 Training Acc:    87.74    87.74    87.74%\n",
      "Epoch   3076 Step:      0 Loss:   0.2780 Training Acc:    87.77    87.77    87.77%\n",
      "Epoch   3077 Step:      0 Loss:   0.2780 Training Acc:    87.75    87.75    87.75%\n",
      "Epoch   3078 Step:      0 Loss:   0.2779 Training Acc:    87.78    87.78    87.78%\n",
      "Epoch   3079 Step:      0 Loss:   0.2778 Training Acc:    87.75    87.75    87.75%\n",
      "Epoch   3080 Step:      0 Loss:   0.2778 Training Acc:    87.78    87.78    87.78%\n",
      "Epoch   3081 Step:      0 Loss:   0.2777 Training Acc:    87.77    87.77    87.77%\n",
      "Epoch   3082 Step:      0 Loss:   0.2777 Training Acc:    87.78    87.78    87.78%\n",
      "Epoch   3083 Step:      0 Loss:   0.2776 Training Acc:    87.78    87.78    87.78%\n",
      "Epoch   3084 Step:      0 Loss:   0.2776 Training Acc:    87.80    87.80    87.80%\n",
      "Epoch   3085 Step:      0 Loss:   0.2776 Training Acc:    87.78    87.78    87.78%\n",
      "Epoch   3086 Step:      0 Loss:   0.2775 Training Acc:    87.79    87.79    87.79%\n",
      "Epoch   3087 Step:      0 Loss:   0.2775 Training Acc:    87.79    87.79    87.79%\n",
      "Epoch   3088 Step:      0 Loss:   0.2776 Training Acc:    87.77    87.77    87.77%\n",
      "Epoch   3089 Step:      0 Loss:   0.2776 Training Acc:    87.79    87.79    87.79%\n",
      "Epoch   3090 Step:      0 Loss:   0.2778 Training Acc:    87.76    87.76    87.76%\n",
      "Epoch   3091 Step:      0 Loss:   0.2779 Training Acc:    87.76    87.76    87.76%\n",
      "Epoch   3092 Step:      0 Loss:   0.2782 Training Acc:    87.70    87.70    87.70%\n",
      "Epoch   3093 Step:      0 Loss:   0.2786 Training Acc:    87.70    87.70    87.70%\n",
      "Epoch   3094 Step:      0 Loss:   0.2793 Training Acc:    87.60    87.60    87.60%\n",
      "Epoch   3095 Step:      0 Loss:   0.2800 Training Acc:    87.59    87.59    87.59%\n",
      "Epoch   3096 Step:      0 Loss:   0.2810 Training Acc:    87.46    87.46    87.46%\n",
      "Epoch   3097 Step:      0 Loss:   0.2819 Training Acc:    87.48    87.48    87.48%\n",
      "Epoch   3098 Step:      0 Loss:   0.2830 Training Acc:    87.29    87.29    87.29%\n",
      "Epoch   3099 Step:      0 Loss:   0.2833 Training Acc:    87.39    87.39    87.39%\n",
      "Epoch   3100 Step:      0 Loss:   0.2832 Training Acc:    87.28    87.28    87.28%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3101 Step:      0 Loss:   0.2817 Training Acc:    87.48    87.48    87.48%\n",
      "Epoch   3102 Step:      0 Loss:   0.2797 Training Acc:    87.56    87.56    87.56%\n",
      "Epoch   3103 Step:      0 Loss:   0.2776 Training Acc:    87.77    87.77    87.77%\n",
      "Epoch   3104 Step:      0 Loss:   0.2765 Training Acc:    87.84    87.84    87.84%\n",
      "Epoch   3105 Step:      0 Loss:   0.2766 Training Acc:    87.82    87.82    87.82%\n",
      "Epoch   3106 Step:      0 Loss:   0.2775 Training Acc:    87.77    87.77    87.77%\n",
      "Epoch   3107 Step:      0 Loss:   0.2785 Training Acc:    87.64    87.64    87.64%\n",
      "Epoch   3108 Step:      0 Loss:   0.2788 Training Acc:    87.68    87.68    87.68%\n",
      "Epoch   3109 Step:      0 Loss:   0.2784 Training Acc:    87.64    87.64    87.64%\n",
      "Epoch   3110 Step:      0 Loss:   0.2774 Training Acc:    87.77    87.77    87.77%\n",
      "Epoch   3111 Step:      0 Loss:   0.2765 Training Acc:    87.80    87.80    87.80%\n",
      "Epoch   3112 Step:      0 Loss:   0.2760 Training Acc:    87.88    87.88    87.88%\n",
      "Epoch   3113 Step:      0 Loss:   0.2761 Training Acc:    87.87    87.87    87.87%\n",
      "Epoch   3114 Step:      0 Loss:   0.2764 Training Acc:    87.82    87.82    87.82%\n",
      "Epoch   3115 Step:      0 Loss:   0.2768 Training Acc:    87.80    87.80    87.80%\n",
      "Epoch   3116 Step:      0 Loss:   0.2769 Training Acc:    87.76    87.76    87.76%\n",
      "Epoch   3117 Step:      0 Loss:   0.2766 Training Acc:    87.83    87.83    87.83%\n",
      "Epoch   3118 Step:      0 Loss:   0.2762 Training Acc:    87.81    87.81    87.81%\n",
      "Epoch   3119 Step:      0 Loss:   0.2758 Training Acc:    87.89    87.89    87.89%\n",
      "Epoch   3120 Step:      0 Loss:   0.2755 Training Acc:    87.87    87.87    87.87%\n",
      "Epoch   3121 Step:      0 Loss:   0.2755 Training Acc:    87.90    87.90    87.90%\n",
      "Epoch   3122 Step:      0 Loss:   0.2756 Training Acc:    87.89    87.89    87.89%\n",
      "Epoch   3123 Step:      0 Loss:   0.2757 Training Acc:    87.87    87.87    87.87%\n",
      "Epoch   3124 Step:      0 Loss:   0.2758 Training Acc:    87.87    87.87    87.87%\n",
      "Epoch   3125 Step:      0 Loss:   0.2757 Training Acc:    87.85    87.85    87.85%\n",
      "Epoch   3126 Step:      0 Loss:   0.2755 Training Acc:    87.89    87.89    87.89%\n",
      "Epoch   3127 Step:      0 Loss:   0.2753 Training Acc:    87.90    87.90    87.90%\n",
      "Epoch   3128 Step:      0 Loss:   0.2751 Training Acc:    87.93    87.93    87.93%\n",
      "Epoch   3129 Step:      0 Loss:   0.2750 Training Acc:    87.91    87.91    87.91%\n",
      "Epoch   3130 Step:      0 Loss:   0.2749 Training Acc:    87.92    87.92    87.92%\n",
      "Epoch   3131 Step:      0 Loss:   0.2749 Training Acc:    87.93    87.93    87.93%\n",
      "Epoch   3132 Step:      0 Loss:   0.2750 Training Acc:    87.91    87.91    87.91%\n",
      "Epoch   3133 Step:      0 Loss:   0.2750 Training Acc:    87.93    87.93    87.93%\n",
      "Epoch   3134 Step:      0 Loss:   0.2749 Training Acc:    87.91    87.91    87.91%\n",
      "Epoch   3135 Step:      0 Loss:   0.2748 Training Acc:    87.93    87.93    87.93%\n",
      "Epoch   3136 Step:      0 Loss:   0.2747 Training Acc:    87.95    87.95    87.95%\n",
      "Epoch   3137 Step:      0 Loss:   0.2746 Training Acc:    87.95    87.95    87.95%\n",
      "Epoch   3138 Step:      0 Loss:   0.2745 Training Acc:    87.94    87.94    87.94%\n",
      "Epoch   3139 Step:      0 Loss:   0.2744 Training Acc:    87.96    87.96    87.96%\n",
      "Epoch   3140 Step:      0 Loss:   0.2743 Training Acc:    87.96    87.96    87.96%\n",
      "Epoch   3141 Step:      0 Loss:   0.2743 Training Acc:    87.94    87.94    87.94%\n",
      "Epoch   3142 Step:      0 Loss:   0.2743 Training Acc:    87.97    87.97    87.97%\n",
      "Epoch   3143 Step:      0 Loss:   0.2743 Training Acc:    87.95    87.95    87.95%\n",
      "Epoch   3144 Step:      0 Loss:   0.2742 Training Acc:    87.97    87.97    87.97%\n",
      "Epoch   3145 Step:      0 Loss:   0.2742 Training Acc:    87.95    87.95    87.95%\n",
      "Epoch   3146 Step:      0 Loss:   0.2741 Training Acc:    87.98    87.98    87.98%\n",
      "Epoch   3147 Step:      0 Loss:   0.2740 Training Acc:    87.96    87.96    87.96%\n",
      "Epoch   3148 Step:      0 Loss:   0.2739 Training Acc:    87.98    87.98    87.98%\n",
      "Epoch   3149 Step:      0 Loss:   0.2739 Training Acc:    87.96    87.96    87.96%\n",
      "Epoch   3150 Step:      0 Loss:   0.2738 Training Acc:    87.98    87.98    87.98%\n",
      "Epoch   3151 Step:      0 Loss:   0.2738 Training Acc:    87.97    87.97    87.97%\n",
      "Epoch   3152 Step:      0 Loss:   0.2737 Training Acc:    87.98    87.98    87.98%\n",
      "Epoch   3153 Step:      0 Loss:   0.2737 Training Acc:    87.98    87.98    87.98%\n",
      "Epoch   3154 Step:      0 Loss:   0.2737 Training Acc:    87.99    87.99    87.99%\n",
      "Epoch   3155 Step:      0 Loss:   0.2736 Training Acc:    87.96    87.96    87.96%\n",
      "Epoch   3156 Step:      0 Loss:   0.2737 Training Acc:    88.00    88.00    88.00%\n",
      "Epoch   3157 Step:      0 Loss:   0.2737 Training Acc:    87.95    87.95    87.95%\n",
      "Epoch   3158 Step:      0 Loss:   0.2738 Training Acc:    88.01    88.01    88.01%\n",
      "Epoch   3159 Step:      0 Loss:   0.2739 Training Acc:    87.92    87.92    87.92%\n",
      "Epoch   3160 Step:      0 Loss:   0.2742 Training Acc:    87.98    87.98    87.98%\n",
      "Epoch   3161 Step:      0 Loss:   0.2746 Training Acc:    87.85    87.85    87.85%\n",
      "Epoch   3162 Step:      0 Loss:   0.2751 Training Acc:    87.91    87.91    87.91%\n",
      "Epoch   3163 Step:      0 Loss:   0.2759 Training Acc:    87.75    87.75    87.75%\n",
      "Epoch   3164 Step:      0 Loss:   0.2768 Training Acc:    87.82    87.82    87.82%\n",
      "Epoch   3165 Step:      0 Loss:   0.2780 Training Acc:    87.61    87.61    87.61%\n",
      "Epoch   3166 Step:      0 Loss:   0.2784 Training Acc:    87.71    87.71    87.71%\n",
      "Epoch   3167 Step:      0 Loss:   0.2787 Training Acc:    87.55    87.55    87.55%\n",
      "Epoch   3168 Step:      0 Loss:   0.2773 Training Acc:    87.80    87.80    87.80%\n",
      "Epoch   3169 Step:      0 Loss:   0.2756 Training Acc:    87.77    87.77    87.77%\n",
      "Epoch   3170 Step:      0 Loss:   0.2737 Training Acc:    88.00    88.00    88.00%\n",
      "Epoch   3171 Step:      0 Loss:   0.2727 Training Acc:    88.03    88.03    88.03%\n",
      "Epoch   3172 Step:      0 Loss:   0.2729 Training Acc:    87.99    87.99    87.99%\n",
      "Epoch   3173 Step:      0 Loss:   0.2737 Training Acc:    88.00    88.00    88.00%\n",
      "Epoch   3174 Step:      0 Loss:   0.2746 Training Acc:    87.83    87.83    87.83%\n",
      "Epoch   3175 Step:      0 Loss:   0.2749 Training Acc:    87.90    87.90    87.90%\n",
      "Epoch   3176 Step:      0 Loss:   0.2745 Training Acc:    87.84    87.84    87.84%\n",
      "Epoch   3177 Step:      0 Loss:   0.2735 Training Acc:    88.00    88.00    88.00%\n",
      "Epoch   3178 Step:      0 Loss:   0.2726 Training Acc:    88.00    88.00    88.00%\n",
      "Epoch   3179 Step:      0 Loss:   0.2722 Training Acc:    88.05    88.05    88.05%\n",
      "Epoch   3180 Step:      0 Loss:   0.2724 Training Acc:    88.06    88.06    88.06%\n",
      "Epoch   3181 Step:      0 Loss:   0.2728 Training Acc:    87.98    87.98    87.98%\n",
      "Epoch   3182 Step:      0 Loss:   0.2732 Training Acc:    88.02    88.02    88.02%\n",
      "Epoch   3183 Step:      0 Loss:   0.2732 Training Acc:    87.93    87.93    87.93%\n",
      "Epoch   3184 Step:      0 Loss:   0.2728 Training Acc:    88.05    88.05    88.05%\n",
      "Epoch   3185 Step:      0 Loss:   0.2723 Training Acc:    88.02    88.02    88.02%\n",
      "Epoch   3186 Step:      0 Loss:   0.2719 Training Acc:    88.07    88.07    88.07%\n",
      "Epoch   3187 Step:      0 Loss:   0.2718 Training Acc:    88.08    88.08    88.08%\n",
      "Epoch   3188 Step:      0 Loss:   0.2719 Training Acc:    88.05    88.05    88.05%\n",
      "Epoch   3189 Step:      0 Loss:   0.2721 Training Acc:    88.09    88.09    88.09%\n",
      "Epoch   3190 Step:      0 Loss:   0.2722 Training Acc:    88.01    88.01    88.01%\n",
      "Epoch   3191 Step:      0 Loss:   0.2721 Training Acc:    88.08    88.08    88.08%\n",
      "Epoch   3192 Step:      0 Loss:   0.2719 Training Acc:    88.04    88.04    88.04%\n",
      "Epoch   3193 Step:      0 Loss:   0.2716 Training Acc:    88.10    88.10    88.10%\n",
      "Epoch   3194 Step:      0 Loss:   0.2714 Training Acc:    88.08    88.08    88.08%\n",
      "Epoch   3195 Step:      0 Loss:   0.2713 Training Acc:    88.10    88.10    88.10%\n",
      "Epoch   3196 Step:      0 Loss:   0.2713 Training Acc:    88.10    88.10    88.10%\n",
      "Epoch   3197 Step:      0 Loss:   0.2714 Training Acc:    88.07    88.07    88.07%\n",
      "Epoch   3198 Step:      0 Loss:   0.2714 Training Acc:    88.12    88.12    88.12%\n",
      "Epoch   3199 Step:      0 Loss:   0.2714 Training Acc:    88.07    88.07    88.07%\n",
      "Epoch   3200 Step:      0 Loss:   0.2713 Training Acc:    88.13    88.13    88.13%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3201 Step:      0 Loss:   0.2711 Training Acc:    88.09    88.09    88.09%\n",
      "Epoch   3202 Step:      0 Loss:   0.2710 Training Acc:    88.12    88.12    88.12%\n",
      "Epoch   3203 Step:      0 Loss:   0.2709 Training Acc:    88.11    88.11    88.11%\n",
      "Epoch   3204 Step:      0 Loss:   0.2708 Training Acc:    88.11    88.11    88.11%\n",
      "Epoch   3205 Step:      0 Loss:   0.2708 Training Acc:    88.12    88.12    88.12%\n",
      "Epoch   3206 Step:      0 Loss:   0.2708 Training Acc:    88.11    88.11    88.11%\n",
      "Epoch   3207 Step:      0 Loss:   0.2708 Training Acc:    88.13    88.13    88.13%\n",
      "Epoch   3208 Step:      0 Loss:   0.2707 Training Acc:    88.11    88.11    88.11%\n",
      "Epoch   3209 Step:      0 Loss:   0.2707 Training Acc:    88.14    88.14    88.14%\n",
      "Epoch   3210 Step:      0 Loss:   0.2706 Training Acc:    88.12    88.12    88.12%\n",
      "Epoch   3211 Step:      0 Loss:   0.2705 Training Acc:    88.15    88.15    88.15%\n",
      "Epoch   3212 Step:      0 Loss:   0.2704 Training Acc:    88.13    88.13    88.13%\n",
      "Epoch   3213 Step:      0 Loss:   0.2704 Training Acc:    88.15    88.15    88.15%\n",
      "Epoch   3214 Step:      0 Loss:   0.2703 Training Acc:    88.12    88.12    88.12%\n",
      "Epoch   3215 Step:      0 Loss:   0.2703 Training Acc:    88.15    88.15    88.15%\n",
      "Epoch   3216 Step:      0 Loss:   0.2703 Training Acc:    88.13    88.13    88.13%\n",
      "Epoch   3217 Step:      0 Loss:   0.2702 Training Acc:    88.13    88.13    88.13%\n",
      "Epoch   3218 Step:      0 Loss:   0.2702 Training Acc:    88.14    88.14    88.14%\n",
      "Epoch   3219 Step:      0 Loss:   0.2702 Training Acc:    88.12    88.12    88.12%\n",
      "Epoch   3220 Step:      0 Loss:   0.2703 Training Acc:    88.14    88.14    88.14%\n",
      "Epoch   3221 Step:      0 Loss:   0.2704 Training Acc:    88.12    88.12    88.12%\n",
      "Epoch   3222 Step:      0 Loss:   0.2706 Training Acc:    88.12    88.12    88.12%\n",
      "Epoch   3223 Step:      0 Loss:   0.2710 Training Acc:    88.09    88.09    88.09%\n",
      "Epoch   3224 Step:      0 Loss:   0.2716 Training Acc:    88.05    88.05    88.05%\n",
      "Epoch   3225 Step:      0 Loss:   0.2726 Training Acc:    87.98    87.98    87.98%\n",
      "Epoch   3226 Step:      0 Loss:   0.2743 Training Acc:    87.81    87.81    87.81%\n",
      "Epoch   3227 Step:      0 Loss:   0.2765 Training Acc:    87.67    87.67    87.67%\n",
      "Epoch   3228 Step:      0 Loss:   0.2802 Training Acc:    87.36    87.36    87.36%\n",
      "Epoch   3229 Step:      0 Loss:   0.2838 Training Acc:    87.17    87.17    87.17%\n",
      "Epoch   3230 Step:      0 Loss:   0.2898 Training Acc:    86.72    86.72    86.72%\n",
      "Epoch   3231 Step:      0 Loss:   0.2924 Training Acc:    86.67    86.67    86.67%\n",
      "Epoch   3232 Step:      0 Loss:   0.2977 Training Acc:    86.41    86.41    86.41%\n",
      "Epoch   3233 Step:      0 Loss:   0.2828 Training Acc:    87.28    87.28    87.28%\n",
      "Epoch   3234 Step:      0 Loss:   0.2720 Training Acc:    88.04    88.04    88.04%\n",
      "Epoch   3235 Step:      0 Loss:   0.2742 Training Acc:    87.83    87.83    87.83%\n",
      "Epoch   3236 Step:      0 Loss:   0.2796 Training Acc:    87.50    87.50    87.50%\n",
      "Epoch   3237 Step:      0 Loss:   0.2796 Training Acc:    87.42    87.42    87.42%\n",
      "Epoch   3238 Step:      0 Loss:   0.2742 Training Acc:    87.85    87.85    87.85%\n",
      "Epoch   3239 Step:      0 Loss:   0.2727 Training Acc:    87.91    87.91    87.91%\n",
      "Epoch   3240 Step:      0 Loss:   0.2735 Training Acc:    87.87    87.87    87.87%\n",
      "Epoch   3241 Step:      0 Loss:   0.2761 Training Acc:    87.73    87.73    87.73%\n",
      "Epoch   3242 Step:      0 Loss:   0.2750 Training Acc:    87.70    87.70    87.70%\n",
      "Epoch   3243 Step:      0 Loss:   0.2709 Training Acc:    88.13    88.13    88.13%\n",
      "Epoch   3244 Step:      0 Loss:   0.2731 Training Acc:    87.91    87.91    87.91%\n",
      "Epoch   3245 Step:      0 Loss:   0.2734 Training Acc:    87.87    87.87    87.87%\n",
      "Epoch   3246 Step:      0 Loss:   0.2721 Training Acc:    88.00    88.00    88.00%\n",
      "Epoch   3247 Step:      0 Loss:   0.2728 Training Acc:    87.89    87.89    87.89%\n",
      "Epoch   3248 Step:      0 Loss:   0.2703 Training Acc:    88.12    88.12    88.12%\n",
      "Epoch   3249 Step:      0 Loss:   0.2716 Training Acc:    87.99    87.99    87.99%\n",
      "Epoch   3250 Step:      0 Loss:   0.2721 Training Acc:    87.99    87.99    87.99%\n",
      "Epoch   3251 Step:      0 Loss:   0.2695 Training Acc:    88.19    88.19    88.19%\n",
      "Epoch   3252 Step:      0 Loss:   0.2705 Training Acc:    88.07    88.07    88.07%\n",
      "Epoch   3253 Step:      0 Loss:   0.2700 Training Acc:    88.17    88.17    88.17%\n",
      "Epoch   3254 Step:      0 Loss:   0.2705 Training Acc:    88.10    88.10    88.10%\n",
      "Epoch   3255 Step:      0 Loss:   0.2694 Training Acc:    88.17    88.17    88.17%\n",
      "Epoch   3256 Step:      0 Loss:   0.2688 Training Acc:    88.20    88.20    88.20%\n",
      "Epoch   3257 Step:      0 Loss:   0.2695 Training Acc:    88.14    88.14    88.14%\n",
      "Epoch   3258 Step:      0 Loss:   0.2694 Training Acc:    88.19    88.19    88.19%\n",
      "Epoch   3259 Step:      0 Loss:   0.2688 Training Acc:    88.18    88.18    88.18%\n",
      "Epoch   3260 Step:      0 Loss:   0.2687 Training Acc:    88.21    88.21    88.21%\n",
      "Epoch   3261 Step:      0 Loss:   0.2684 Training Acc:    88.22    88.22    88.22%\n",
      "Epoch   3262 Step:      0 Loss:   0.2687 Training Acc:    88.19    88.19    88.19%\n",
      "Epoch   3263 Step:      0 Loss:   0.2685 Training Acc:    88.24    88.24    88.24%\n",
      "Epoch   3264 Step:      0 Loss:   0.2681 Training Acc:    88.22    88.22    88.22%\n",
      "Epoch   3265 Step:      0 Loss:   0.2682 Training Acc:    88.24    88.24    88.24%\n",
      "Epoch   3266 Step:      0 Loss:   0.2682 Training Acc:    88.25    88.25    88.25%\n",
      "Epoch   3267 Step:      0 Loss:   0.2681 Training Acc:    88.25    88.25    88.25%\n",
      "Epoch   3268 Step:      0 Loss:   0.2679 Training Acc:    88.26    88.26    88.26%\n",
      "Epoch   3269 Step:      0 Loss:   0.2677 Training Acc:    88.27    88.27    88.27%\n",
      "Epoch   3270 Step:      0 Loss:   0.2679 Training Acc:    88.24    88.24    88.24%\n",
      "Epoch   3271 Step:      0 Loss:   0.2677 Training Acc:    88.26    88.26    88.26%\n",
      "Epoch   3272 Step:      0 Loss:   0.2676 Training Acc:    88.28    88.28    88.28%\n",
      "Epoch   3273 Step:      0 Loss:   0.2675 Training Acc:    88.26    88.26    88.26%\n",
      "Epoch   3274 Step:      0 Loss:   0.2674 Training Acc:    88.28    88.28    88.28%\n",
      "Epoch   3275 Step:      0 Loss:   0.2675 Training Acc:    88.29    88.29    88.29%\n",
      "Epoch   3276 Step:      0 Loss:   0.2673 Training Acc:    88.26    88.26    88.26%\n",
      "Epoch   3277 Step:      0 Loss:   0.2673 Training Acc:    88.29    88.29    88.29%\n",
      "Epoch   3278 Step:      0 Loss:   0.2672 Training Acc:    88.29    88.29    88.29%\n",
      "Epoch   3279 Step:      0 Loss:   0.2672 Training Acc:    88.29    88.29    88.29%\n",
      "Epoch   3280 Step:      0 Loss:   0.2671 Training Acc:    88.30    88.30    88.30%\n",
      "Epoch   3281 Step:      0 Loss:   0.2670 Training Acc:    88.30    88.30    88.30%\n",
      "Epoch   3282 Step:      0 Loss:   0.2669 Training Acc:    88.31    88.31    88.31%\n",
      "Epoch   3283 Step:      0 Loss:   0.2669 Training Acc:    88.29    88.29    88.29%\n",
      "Epoch   3284 Step:      0 Loss:   0.2669 Training Acc:    88.31    88.31    88.31%\n",
      "Epoch   3285 Step:      0 Loss:   0.2668 Training Acc:    88.31    88.31    88.31%\n",
      "Epoch   3286 Step:      0 Loss:   0.2667 Training Acc:    88.31    88.31    88.31%\n",
      "Epoch   3287 Step:      0 Loss:   0.2667 Training Acc:    88.31    88.31    88.31%\n",
      "Epoch   3288 Step:      0 Loss:   0.2667 Training Acc:    88.31    88.31    88.31%\n",
      "Epoch   3289 Step:      0 Loss:   0.2666 Training Acc:    88.32    88.32    88.32%\n",
      "Epoch   3290 Step:      0 Loss:   0.2665 Training Acc:    88.33    88.33    88.33%\n",
      "Epoch   3291 Step:      0 Loss:   0.2665 Training Acc:    88.33    88.33    88.33%\n",
      "Epoch   3292 Step:      0 Loss:   0.2664 Training Acc:    88.33    88.33    88.33%\n",
      "Epoch   3293 Step:      0 Loss:   0.2664 Training Acc:    88.33    88.33    88.33%\n",
      "Epoch   3294 Step:      0 Loss:   0.2663 Training Acc:    88.32    88.32    88.32%\n",
      "Epoch   3295 Step:      0 Loss:   0.2663 Training Acc:    88.34    88.34    88.34%\n",
      "Epoch   3296 Step:      0 Loss:   0.2662 Training Acc:    88.34    88.34    88.34%\n",
      "Epoch   3297 Step:      0 Loss:   0.2662 Training Acc:    88.34    88.34    88.34%\n",
      "Epoch   3298 Step:      0 Loss:   0.2661 Training Acc:    88.34    88.34    88.34%\n",
      "Epoch   3299 Step:      0 Loss:   0.2661 Training Acc:    88.35    88.35    88.35%\n",
      "Epoch   3300 Step:      0 Loss:   0.2660 Training Acc:    88.36    88.36    88.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3301 Step:      0 Loss:   0.2660 Training Acc:    88.35    88.35    88.35%\n",
      "Epoch   3302 Step:      0 Loss:   0.2659 Training Acc:    88.36    88.36    88.36%\n",
      "Epoch   3303 Step:      0 Loss:   0.2659 Training Acc:    88.36    88.36    88.36%\n",
      "Epoch   3304 Step:      0 Loss:   0.2658 Training Acc:    88.36    88.36    88.36%\n",
      "Epoch   3305 Step:      0 Loss:   0.2658 Training Acc:    88.36    88.36    88.36%\n",
      "Epoch   3306 Step:      0 Loss:   0.2657 Training Acc:    88.37    88.37    88.37%\n",
      "Epoch   3307 Step:      0 Loss:   0.2657 Training Acc:    88.36    88.36    88.36%\n",
      "Epoch   3308 Step:      0 Loss:   0.2656 Training Acc:    88.36    88.36    88.36%\n",
      "Epoch   3309 Step:      0 Loss:   0.2656 Training Acc:    88.38    88.38    88.38%\n",
      "Epoch   3310 Step:      0 Loss:   0.2655 Training Acc:    88.37    88.37    88.37%\n",
      "Epoch   3311 Step:      0 Loss:   0.2655 Training Acc:    88.38    88.38    88.38%\n",
      "Epoch   3312 Step:      0 Loss:   0.2654 Training Acc:    88.38    88.38    88.38%\n",
      "Epoch   3313 Step:      0 Loss:   0.2654 Training Acc:    88.38    88.38    88.38%\n",
      "Epoch   3314 Step:      0 Loss:   0.2653 Training Acc:    88.38    88.38    88.38%\n",
      "Epoch   3315 Step:      0 Loss:   0.2653 Training Acc:    88.39    88.39    88.39%\n",
      "Epoch   3316 Step:      0 Loss:   0.2652 Training Acc:    88.39    88.39    88.39%\n",
      "Epoch   3317 Step:      0 Loss:   0.2652 Training Acc:    88.39    88.39    88.39%\n",
      "Epoch   3318 Step:      0 Loss:   0.2651 Training Acc:    88.40    88.40    88.40%\n",
      "Epoch   3319 Step:      0 Loss:   0.2651 Training Acc:    88.40    88.40    88.40%\n",
      "Epoch   3320 Step:      0 Loss:   0.2650 Training Acc:    88.40    88.40    88.40%\n",
      "Epoch   3321 Step:      0 Loss:   0.2650 Training Acc:    88.41    88.41    88.41%\n",
      "Epoch   3322 Step:      0 Loss:   0.2649 Training Acc:    88.41    88.41    88.41%\n",
      "Epoch   3323 Step:      0 Loss:   0.2649 Training Acc:    88.41    88.41    88.41%\n",
      "Epoch   3324 Step:      0 Loss:   0.2648 Training Acc:    88.41    88.41    88.41%\n",
      "Epoch   3325 Step:      0 Loss:   0.2647 Training Acc:    88.42    88.42    88.42%\n",
      "Epoch   3326 Step:      0 Loss:   0.2647 Training Acc:    88.42    88.42    88.42%\n",
      "Epoch   3327 Step:      0 Loss:   0.2646 Training Acc:    88.42    88.42    88.42%\n",
      "Epoch   3328 Step:      0 Loss:   0.2646 Training Acc:    88.44    88.44    88.44%\n",
      "Epoch   3329 Step:      0 Loss:   0.2645 Training Acc:    88.43    88.43    88.43%\n",
      "Epoch   3330 Step:      0 Loss:   0.2645 Training Acc:    88.43    88.43    88.43%\n",
      "Epoch   3331 Step:      0 Loss:   0.2644 Training Acc:    88.44    88.44    88.44%\n",
      "Epoch   3332 Step:      0 Loss:   0.2644 Training Acc:    88.44    88.44    88.44%\n",
      "Epoch   3333 Step:      0 Loss:   0.2643 Training Acc:    88.44    88.44    88.44%\n",
      "Epoch   3334 Step:      0 Loss:   0.2643 Training Acc:    88.45    88.45    88.45%\n",
      "Epoch   3335 Step:      0 Loss:   0.2642 Training Acc:    88.45    88.45    88.45%\n",
      "Epoch   3336 Step:      0 Loss:   0.2642 Training Acc:    88.45    88.45    88.45%\n",
      "Epoch   3337 Step:      0 Loss:   0.2641 Training Acc:    88.46    88.46    88.46%\n",
      "Epoch   3338 Step:      0 Loss:   0.2641 Training Acc:    88.46    88.46    88.46%\n",
      "Epoch   3339 Step:      0 Loss:   0.2640 Training Acc:    88.46    88.46    88.46%\n",
      "Epoch   3340 Step:      0 Loss:   0.2640 Training Acc:    88.46    88.46    88.46%\n",
      "Epoch   3341 Step:      0 Loss:   0.2639 Training Acc:    88.46    88.46    88.46%\n",
      "Epoch   3342 Step:      0 Loss:   0.2639 Training Acc:    88.46    88.46    88.46%\n",
      "Epoch   3343 Step:      0 Loss:   0.2638 Training Acc:    88.47    88.47    88.47%\n",
      "Epoch   3344 Step:      0 Loss:   0.2638 Training Acc:    88.47    88.47    88.47%\n",
      "Epoch   3345 Step:      0 Loss:   0.2637 Training Acc:    88.47    88.47    88.47%\n",
      "Epoch   3346 Step:      0 Loss:   0.2637 Training Acc:    88.48    88.48    88.48%\n",
      "Epoch   3347 Step:      0 Loss:   0.2636 Training Acc:    88.48    88.48    88.48%\n",
      "Epoch   3348 Step:      0 Loss:   0.2636 Training Acc:    88.48    88.48    88.48%\n",
      "Epoch   3349 Step:      0 Loss:   0.2635 Training Acc:    88.49    88.49    88.49%\n",
      "Epoch   3350 Step:      0 Loss:   0.2635 Training Acc:    88.49    88.49    88.49%\n",
      "Epoch   3351 Step:      0 Loss:   0.2634 Training Acc:    88.49    88.49    88.49%\n",
      "Epoch   3352 Step:      0 Loss:   0.2634 Training Acc:    88.50    88.50    88.50%\n",
      "Epoch   3353 Step:      0 Loss:   0.2633 Training Acc:    88.50    88.50    88.50%\n",
      "Epoch   3354 Step:      0 Loss:   0.2633 Training Acc:    88.50    88.50    88.50%\n",
      "Epoch   3355 Step:      0 Loss:   0.2632 Training Acc:    88.50    88.50    88.50%\n",
      "Epoch   3356 Step:      0 Loss:   0.2632 Training Acc:    88.51    88.51    88.51%\n",
      "Epoch   3357 Step:      0 Loss:   0.2631 Training Acc:    88.51    88.51    88.51%\n",
      "Epoch   3358 Step:      0 Loss:   0.2631 Training Acc:    88.51    88.51    88.51%\n",
      "Epoch   3359 Step:      0 Loss:   0.2630 Training Acc:    88.51    88.51    88.51%\n",
      "Epoch   3360 Step:      0 Loss:   0.2630 Training Acc:    88.52    88.52    88.52%\n",
      "Epoch   3361 Step:      0 Loss:   0.2629 Training Acc:    88.52    88.52    88.52%\n",
      "Epoch   3362 Step:      0 Loss:   0.2629 Training Acc:    88.52    88.52    88.52%\n",
      "Epoch   3363 Step:      0 Loss:   0.2628 Training Acc:    88.52    88.52    88.52%\n",
      "Epoch   3364 Step:      0 Loss:   0.2628 Training Acc:    88.53    88.53    88.53%\n",
      "Epoch   3365 Step:      0 Loss:   0.2627 Training Acc:    88.53    88.53    88.53%\n",
      "Epoch   3366 Step:      0 Loss:   0.2627 Training Acc:    88.53    88.53    88.53%\n",
      "Epoch   3367 Step:      0 Loss:   0.2626 Training Acc:    88.53    88.53    88.53%\n",
      "Epoch   3368 Step:      0 Loss:   0.2626 Training Acc:    88.54    88.54    88.54%\n",
      "Epoch   3369 Step:      0 Loss:   0.2625 Training Acc:    88.54    88.54    88.54%\n",
      "Epoch   3370 Step:      0 Loss:   0.2625 Training Acc:    88.54    88.54    88.54%\n",
      "Epoch   3371 Step:      0 Loss:   0.2624 Training Acc:    88.55    88.55    88.55%\n",
      "Epoch   3372 Step:      0 Loss:   0.2624 Training Acc:    88.55    88.55    88.55%\n",
      "Epoch   3373 Step:      0 Loss:   0.2623 Training Acc:    88.55    88.55    88.55%\n",
      "Epoch   3374 Step:      0 Loss:   0.2623 Training Acc:    88.55    88.55    88.55%\n",
      "Epoch   3375 Step:      0 Loss:   0.2622 Training Acc:    88.56    88.56    88.56%\n",
      "Epoch   3376 Step:      0 Loss:   0.2622 Training Acc:    88.56    88.56    88.56%\n",
      "Epoch   3377 Step:      0 Loss:   0.2621 Training Acc:    88.56    88.56    88.56%\n",
      "Epoch   3378 Step:      0 Loss:   0.2621 Training Acc:    88.56    88.56    88.56%\n",
      "Epoch   3379 Step:      0 Loss:   0.2620 Training Acc:    88.57    88.57    88.57%\n",
      "Epoch   3380 Step:      0 Loss:   0.2620 Training Acc:    88.56    88.56    88.56%\n",
      "Epoch   3381 Step:      0 Loss:   0.2619 Training Acc:    88.58    88.58    88.58%\n",
      "Epoch   3382 Step:      0 Loss:   0.2619 Training Acc:    88.56    88.56    88.56%\n",
      "Epoch   3383 Step:      0 Loss:   0.2618 Training Acc:    88.59    88.59    88.59%\n",
      "Epoch   3384 Step:      0 Loss:   0.2618 Training Acc:    88.56    88.56    88.56%\n",
      "Epoch   3385 Step:      0 Loss:   0.2618 Training Acc:    88.59    88.59    88.59%\n",
      "Epoch   3386 Step:      0 Loss:   0.2618 Training Acc:    88.56    88.56    88.56%\n",
      "Epoch   3387 Step:      0 Loss:   0.2618 Training Acc:    88.59    88.59    88.59%\n",
      "Epoch   3388 Step:      0 Loss:   0.2620 Training Acc:    88.52    88.52    88.52%\n",
      "Epoch   3389 Step:      0 Loss:   0.2622 Training Acc:    88.58    88.58    88.58%\n",
      "Epoch   3390 Step:      0 Loss:   0.2627 Training Acc:    88.45    88.45    88.45%\n",
      "Epoch   3391 Step:      0 Loss:   0.2634 Training Acc:    88.51    88.51    88.51%\n",
      "Epoch   3392 Step:      0 Loss:   0.2645 Training Acc:    88.27    88.27    88.27%\n",
      "Epoch   3393 Step:      0 Loss:   0.2660 Training Acc:    88.34    88.34    88.34%\n",
      "Epoch   3394 Step:      0 Loss:   0.2682 Training Acc:    88.01    88.01    88.01%\n",
      "Epoch   3395 Step:      0 Loss:   0.2696 Training Acc:    88.11    88.11    88.11%\n",
      "Epoch   3396 Step:      0 Loss:   0.2702 Training Acc:    87.88    87.88    87.88%\n",
      "Epoch   3397 Step:      0 Loss:   0.2680 Training Acc:    88.21    88.21    88.21%\n",
      "Epoch   3398 Step:      0 Loss:   0.2646 Training Acc:    88.27    88.27    88.27%\n",
      "Epoch   3399 Step:      0 Loss:   0.2618 Training Acc:    88.58    88.58    88.58%\n",
      "Epoch   3400 Step:      0 Loss:   0.2613 Training Acc:    88.61    88.61    88.61%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3401 Step:      0 Loss:   0.2628 Training Acc:    88.41    88.41    88.41%\n",
      "Epoch   3402 Step:      0 Loss:   0.2644 Training Acc:    88.45    88.45    88.45%\n",
      "Epoch   3403 Step:      0 Loss:   0.2648 Training Acc:    88.24    88.24    88.24%\n",
      "Epoch   3404 Step:      0 Loss:   0.2634 Training Acc:    88.50    88.50    88.50%\n",
      "Epoch   3405 Step:      0 Loss:   0.2617 Training Acc:    88.53    88.53    88.53%\n",
      "Epoch   3406 Step:      0 Loss:   0.2609 Training Acc:    88.59    88.59    88.59%\n",
      "Epoch   3407 Step:      0 Loss:   0.2614 Training Acc:    88.61    88.61    88.61%\n",
      "Epoch   3408 Step:      0 Loss:   0.2624 Training Acc:    88.44    88.44    88.44%\n",
      "Epoch   3409 Step:      0 Loss:   0.2626 Training Acc:    88.55    88.55    88.55%\n",
      "Epoch   3410 Step:      0 Loss:   0.2620 Training Acc:    88.46    88.46    88.46%\n",
      "Epoch   3411 Step:      0 Loss:   0.2610 Training Acc:    88.64    88.64    88.64%\n",
      "Epoch   3412 Step:      0 Loss:   0.2605 Training Acc:    88.63    88.63    88.63%\n",
      "Epoch   3413 Step:      0 Loss:   0.2607 Training Acc:    88.58    88.58    88.58%\n",
      "Epoch   3414 Step:      0 Loss:   0.2612 Training Acc:    88.63    88.63    88.63%\n",
      "Epoch   3415 Step:      0 Loss:   0.2614 Training Acc:    88.51    88.51    88.51%\n",
      "Epoch   3416 Step:      0 Loss:   0.2611 Training Acc:    88.64    88.64    88.64%\n",
      "Epoch   3417 Step:      0 Loss:   0.2605 Training Acc:    88.60    88.60    88.60%\n",
      "Epoch   3418 Step:      0 Loss:   0.2601 Training Acc:    88.66    88.66    88.66%\n",
      "Epoch   3419 Step:      0 Loss:   0.2601 Training Acc:    88.68    88.68    88.68%\n",
      "Epoch   3420 Step:      0 Loss:   0.2604 Training Acc:    88.60    88.60    88.60%\n",
      "Epoch   3421 Step:      0 Loss:   0.2606 Training Acc:    88.67    88.67    88.67%\n",
      "Epoch   3422 Step:      0 Loss:   0.2605 Training Acc:    88.58    88.58    88.58%\n",
      "Epoch   3423 Step:      0 Loss:   0.2601 Training Acc:    88.68    88.68    88.68%\n",
      "Epoch   3424 Step:      0 Loss:   0.2598 Training Acc:    88.66    88.66    88.66%\n",
      "Epoch   3425 Step:      0 Loss:   0.2597 Training Acc:    88.67    88.67    88.67%\n",
      "Epoch   3426 Step:      0 Loss:   0.2598 Training Acc:    88.69    88.69    88.69%\n",
      "Epoch   3427 Step:      0 Loss:   0.2599 Training Acc:    88.64    88.64    88.64%\n",
      "Epoch   3428 Step:      0 Loss:   0.2599 Training Acc:    88.70    88.70    88.70%\n",
      "Epoch   3429 Step:      0 Loss:   0.2598 Training Acc:    88.65    88.65    88.65%\n",
      "Epoch   3430 Step:      0 Loss:   0.2596 Training Acc:    88.70    88.70    88.70%\n",
      "Epoch   3431 Step:      0 Loss:   0.2594 Training Acc:    88.69    88.69    88.69%\n",
      "Epoch   3432 Step:      0 Loss:   0.2594 Training Acc:    88.69    88.69    88.69%\n",
      "Epoch   3433 Step:      0 Loss:   0.2594 Training Acc:    88.72    88.72    88.72%\n",
      "Epoch   3434 Step:      0 Loss:   0.2594 Training Acc:    88.67    88.67    88.67%\n",
      "Epoch   3435 Step:      0 Loss:   0.2594 Training Acc:    88.71    88.71    88.71%\n",
      "Epoch   3436 Step:      0 Loss:   0.2593 Training Acc:    88.68    88.68    88.68%\n",
      "Epoch   3437 Step:      0 Loss:   0.2591 Training Acc:    88.71    88.71    88.71%\n",
      "Epoch   3438 Step:      0 Loss:   0.2591 Training Acc:    88.72    88.72    88.72%\n",
      "Epoch   3439 Step:      0 Loss:   0.2590 Training Acc:    88.71    88.71    88.71%\n",
      "Epoch   3440 Step:      0 Loss:   0.2590 Training Acc:    88.74    88.74    88.74%\n",
      "Epoch   3441 Step:      0 Loss:   0.2590 Training Acc:    88.70    88.70    88.70%\n",
      "Epoch   3442 Step:      0 Loss:   0.2589 Training Acc:    88.74    88.74    88.74%\n",
      "Epoch   3443 Step:      0 Loss:   0.2589 Training Acc:    88.71    88.71    88.71%\n",
      "Epoch   3444 Step:      0 Loss:   0.2588 Training Acc:    88.73    88.73    88.73%\n",
      "Epoch   3445 Step:      0 Loss:   0.2587 Training Acc:    88.73    88.73    88.73%\n",
      "Epoch   3446 Step:      0 Loss:   0.2587 Training Acc:    88.73    88.73    88.73%\n",
      "Epoch   3447 Step:      0 Loss:   0.2586 Training Acc:    88.73    88.73    88.73%\n",
      "Epoch   3448 Step:      0 Loss:   0.2586 Training Acc:    88.73    88.73    88.73%\n",
      "Epoch   3449 Step:      0 Loss:   0.2585 Training Acc:    88.74    88.74    88.74%\n",
      "Epoch   3450 Step:      0 Loss:   0.2585 Training Acc:    88.73    88.73    88.73%\n",
      "Epoch   3451 Step:      0 Loss:   0.2584 Training Acc:    88.75    88.75    88.75%\n",
      "Epoch   3452 Step:      0 Loss:   0.2584 Training Acc:    88.74    88.74    88.74%\n",
      "Epoch   3453 Step:      0 Loss:   0.2583 Training Acc:    88.75    88.75    88.75%\n",
      "Epoch   3454 Step:      0 Loss:   0.2582 Training Acc:    88.75    88.75    88.75%\n",
      "Epoch   3455 Step:      0 Loss:   0.2582 Training Acc:    88.75    88.75    88.75%\n",
      "Epoch   3456 Step:      0 Loss:   0.2582 Training Acc:    88.75    88.75    88.75%\n",
      "Epoch   3457 Step:      0 Loss:   0.2581 Training Acc:    88.76    88.76    88.76%\n",
      "Epoch   3458 Step:      0 Loss:   0.2581 Training Acc:    88.76    88.76    88.76%\n",
      "Epoch   3459 Step:      0 Loss:   0.2580 Training Acc:    88.76    88.76    88.76%\n",
      "Epoch   3460 Step:      0 Loss:   0.2580 Training Acc:    88.77    88.77    88.77%\n",
      "Epoch   3461 Step:      0 Loss:   0.2579 Training Acc:    88.77    88.77    88.77%\n",
      "Epoch   3462 Step:      0 Loss:   0.2578 Training Acc:    88.78    88.78    88.78%\n",
      "Epoch   3463 Step:      0 Loss:   0.2578 Training Acc:    88.79    88.79    88.79%\n",
      "Epoch   3464 Step:      0 Loss:   0.2577 Training Acc:    88.78    88.78    88.78%\n",
      "Epoch   3465 Step:      0 Loss:   0.2577 Training Acc:    88.79    88.79    88.79%\n",
      "Epoch   3466 Step:      0 Loss:   0.2576 Training Acc:    88.78    88.78    88.78%\n",
      "Epoch   3467 Step:      0 Loss:   0.2576 Training Acc:    88.79    88.79    88.79%\n",
      "Epoch   3468 Step:      0 Loss:   0.2575 Training Acc:    88.79    88.79    88.79%\n",
      "Epoch   3469 Step:      0 Loss:   0.2575 Training Acc:    88.80    88.80    88.80%\n",
      "Epoch   3470 Step:      0 Loss:   0.2574 Training Acc:    88.80    88.80    88.80%\n",
      "Epoch   3471 Step:      0 Loss:   0.2574 Training Acc:    88.80    88.80    88.80%\n",
      "Epoch   3472 Step:      0 Loss:   0.2573 Training Acc:    88.81    88.81    88.81%\n",
      "Epoch   3473 Step:      0 Loss:   0.2573 Training Acc:    88.81    88.81    88.81%\n",
      "Epoch   3474 Step:      0 Loss:   0.2572 Training Acc:    88.81    88.81    88.81%\n",
      "Epoch   3475 Step:      0 Loss:   0.2572 Training Acc:    88.81    88.81    88.81%\n",
      "Epoch   3476 Step:      0 Loss:   0.2571 Training Acc:    88.82    88.82    88.82%\n",
      "Epoch   3477 Step:      0 Loss:   0.2571 Training Acc:    88.82    88.82    88.82%\n",
      "Epoch   3478 Step:      0 Loss:   0.2570 Training Acc:    88.82    88.82    88.82%\n",
      "Epoch   3479 Step:      0 Loss:   0.2570 Training Acc:    88.82    88.82    88.82%\n",
      "Epoch   3480 Step:      0 Loss:   0.2569 Training Acc:    88.83    88.83    88.83%\n",
      "Epoch   3481 Step:      0 Loss:   0.2569 Training Acc:    88.83    88.83    88.83%\n",
      "Epoch   3482 Step:      0 Loss:   0.2568 Training Acc:    88.83    88.83    88.83%\n",
      "Epoch   3483 Step:      0 Loss:   0.2568 Training Acc:    88.84    88.84    88.84%\n",
      "Epoch   3484 Step:      0 Loss:   0.2567 Training Acc:    88.84    88.84    88.84%\n",
      "Epoch   3485 Step:      0 Loss:   0.2567 Training Acc:    88.84    88.84    88.84%\n",
      "Epoch   3486 Step:      0 Loss:   0.2566 Training Acc:    88.84    88.84    88.84%\n",
      "Epoch   3487 Step:      0 Loss:   0.2566 Training Acc:    88.85    88.85    88.85%\n",
      "Epoch   3488 Step:      0 Loss:   0.2565 Training Acc:    88.85    88.85    88.85%\n",
      "Epoch   3489 Step:      0 Loss:   0.2565 Training Acc:    88.86    88.86    88.86%\n",
      "Epoch   3490 Step:      0 Loss:   0.2564 Training Acc:    88.85    88.85    88.85%\n",
      "Epoch   3491 Step:      0 Loss:   0.2564 Training Acc:    88.86    88.86    88.86%\n",
      "Epoch   3492 Step:      0 Loss:   0.2563 Training Acc:    88.86    88.86    88.86%\n",
      "Epoch   3493 Step:      0 Loss:   0.2563 Training Acc:    88.86    88.86    88.86%\n",
      "Epoch   3494 Step:      0 Loss:   0.2562 Training Acc:    88.86    88.86    88.86%\n",
      "Epoch   3495 Step:      0 Loss:   0.2562 Training Acc:    88.86    88.86    88.86%\n",
      "Epoch   3496 Step:      0 Loss:   0.2561 Training Acc:    88.87    88.87    88.87%\n",
      "Epoch   3497 Step:      0 Loss:   0.2561 Training Acc:    88.87    88.87    88.87%\n",
      "Epoch   3498 Step:      0 Loss:   0.2560 Training Acc:    88.87    88.87    88.87%\n",
      "Epoch   3499 Step:      0 Loss:   0.2560 Training Acc:    88.87    88.87    88.87%\n",
      "Epoch   3500 Step:      0 Loss:   0.2559 Training Acc:    88.88    88.88    88.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3501 Step:      0 Loss:   0.2559 Training Acc:    88.88    88.88    88.88%\n",
      "Epoch   3502 Step:      0 Loss:   0.2559 Training Acc:    88.87    88.87    88.87%\n",
      "Epoch   3503 Step:      0 Loss:   0.2558 Training Acc:    88.89    88.89    88.89%\n",
      "Epoch   3504 Step:      0 Loss:   0.2559 Training Acc:    88.88    88.88    88.88%\n",
      "Epoch   3505 Step:      0 Loss:   0.2559 Training Acc:    88.89    88.89    88.89%\n",
      "Epoch   3506 Step:      0 Loss:   0.2561 Training Acc:    88.86    88.86    88.86%\n",
      "Epoch   3507 Step:      0 Loss:   0.2564 Training Acc:    88.84    88.84    88.84%\n",
      "Epoch   3508 Step:      0 Loss:   0.2571 Training Acc:    88.78    88.78    88.78%\n",
      "Epoch   3509 Step:      0 Loss:   0.2583 Training Acc:    88.69    88.69    88.69%\n",
      "Epoch   3510 Step:      0 Loss:   0.2606 Training Acc:    88.50    88.50    88.50%\n",
      "Epoch   3511 Step:      0 Loss:   0.2644 Training Acc:    88.25    88.25    88.25%\n",
      "Epoch   3512 Step:      0 Loss:   0.2714 Training Acc:    87.76    87.76    87.76%\n",
      "Epoch   3513 Step:      0 Loss:   0.2795 Training Acc:    87.22    87.22    87.22%\n",
      "Epoch   3514 Step:      0 Loss:   0.2911 Training Acc:    86.55    86.55    86.55%\n",
      "Epoch   3515 Step:      0 Loss:   0.2896 Training Acc:    86.65    86.65    86.65%\n",
      "Epoch   3516 Step:      0 Loss:   0.2796 Training Acc:    87.27    87.27    87.27%\n",
      "Epoch   3517 Step:      0 Loss:   0.2598 Training Acc:    88.60    88.60    88.60%\n",
      "Epoch   3518 Step:      0 Loss:   0.2611 Training Acc:    88.52    88.52    88.52%\n",
      "Epoch   3519 Step:      0 Loss:   0.2730 Training Acc:    87.68    87.68    87.68%\n",
      "Epoch   3520 Step:      0 Loss:   0.2701 Training Acc:    87.88    87.88    87.88%\n",
      "Epoch   3521 Step:      0 Loss:   0.2604 Training Acc:    88.54    88.54    88.54%\n",
      "Epoch   3522 Step:      0 Loss:   0.2594 Training Acc:    88.60    88.60    88.60%\n",
      "Epoch   3523 Step:      0 Loss:   0.2660 Training Acc:    88.11    88.11    88.11%\n",
      "Epoch   3524 Step:      0 Loss:   0.2642 Training Acc:    88.30    88.30    88.30%\n",
      "Epoch   3525 Step:      0 Loss:   0.2573 Training Acc:    88.77    88.77    88.77%\n",
      "Epoch   3526 Step:      0 Loss:   0.2603 Training Acc:    88.51    88.51    88.51%\n",
      "Epoch   3527 Step:      0 Loss:   0.2636 Training Acc:    88.33    88.33    88.33%\n",
      "Epoch   3528 Step:      0 Loss:   0.2589 Training Acc:    88.58    88.58    88.58%\n",
      "Epoch   3529 Step:      0 Loss:   0.2565 Training Acc:    88.83    88.83    88.83%\n",
      "Epoch   3530 Step:      0 Loss:   0.2596 Training Acc:    88.62    88.62    88.62%\n",
      "Epoch   3531 Step:      0 Loss:   0.2601 Training Acc:    88.50    88.50    88.50%\n",
      "Epoch   3532 Step:      0 Loss:   0.2558 Training Acc:    88.87    88.87    88.87%\n",
      "Epoch   3533 Step:      0 Loss:   0.2568 Training Acc:    88.78    88.78    88.78%\n",
      "Epoch   3534 Step:      0 Loss:   0.2587 Training Acc:    88.62    88.62    88.62%\n",
      "Epoch   3535 Step:      0 Loss:   0.2571 Training Acc:    88.80    88.80    88.80%\n",
      "Epoch   3536 Step:      0 Loss:   0.2550 Training Acc:    88.88    88.88    88.88%\n",
      "Epoch   3537 Step:      0 Loss:   0.2568 Training Acc:    88.74    88.74    88.74%\n",
      "Epoch   3538 Step:      0 Loss:   0.2574 Training Acc:    88.76    88.76    88.76%\n",
      "Epoch   3539 Step:      0 Loss:   0.2551 Training Acc:    88.88    88.88    88.88%\n",
      "Epoch   3540 Step:      0 Loss:   0.2551 Training Acc:    88.85    88.85    88.85%\n",
      "Epoch   3541 Step:      0 Loss:   0.2565 Training Acc:    88.82    88.82    88.82%\n",
      "Epoch   3542 Step:      0 Loss:   0.2554 Training Acc:    88.88    88.88    88.88%\n",
      "Epoch   3543 Step:      0 Loss:   0.2545 Training Acc:    88.92    88.92    88.92%\n",
      "Epoch   3544 Step:      0 Loss:   0.2551 Training Acc:    88.93    88.93    88.93%\n",
      "Epoch   3545 Step:      0 Loss:   0.2555 Training Acc:    88.85    88.85    88.85%\n",
      "Epoch   3546 Step:      0 Loss:   0.2544 Training Acc:    88.96    88.96    88.96%\n",
      "Epoch   3547 Step:      0 Loss:   0.2542 Training Acc:    88.96    88.96    88.96%\n",
      "Epoch   3548 Step:      0 Loss:   0.2548 Training Acc:    88.89    88.89    88.89%\n",
      "Epoch   3549 Step:      0 Loss:   0.2545 Training Acc:    88.94    88.94    88.94%\n",
      "Epoch   3550 Step:      0 Loss:   0.2540 Training Acc:    88.97    88.97    88.97%\n",
      "Epoch   3551 Step:      0 Loss:   0.2540 Training Acc:    88.96    88.96    88.96%\n",
      "Epoch   3552 Step:      0 Loss:   0.2543 Training Acc:    88.93    88.93    88.93%\n",
      "Epoch   3553 Step:      0 Loss:   0.2539 Training Acc:    88.96    88.96    88.96%\n",
      "Epoch   3554 Step:      0 Loss:   0.2536 Training Acc:    89.00    89.00    89.00%\n",
      "Epoch   3555 Step:      0 Loss:   0.2539 Training Acc:    88.97    88.97    88.97%\n",
      "Epoch   3556 Step:      0 Loss:   0.2538 Training Acc:    88.98    88.98    88.98%\n",
      "Epoch   3557 Step:      0 Loss:   0.2535 Training Acc:    89.00    89.00    89.00%\n",
      "Epoch   3558 Step:      0 Loss:   0.2534 Training Acc:    89.01    89.01    89.01%\n",
      "Epoch   3559 Step:      0 Loss:   0.2536 Training Acc:    88.99    88.99    88.99%\n",
      "Epoch   3560 Step:      0 Loss:   0.2534 Training Acc:    89.00    89.00    89.00%\n",
      "Epoch   3561 Step:      0 Loss:   0.2532 Training Acc:    89.02    89.02    89.02%\n",
      "Epoch   3562 Step:      0 Loss:   0.2532 Training Acc:    89.02    89.02    89.02%\n",
      "Epoch   3563 Step:      0 Loss:   0.2533 Training Acc:    89.01    89.01    89.01%\n",
      "Epoch   3564 Step:      0 Loss:   0.2531 Training Acc:    89.01    89.01    89.01%\n",
      "Epoch   3565 Step:      0 Loss:   0.2530 Training Acc:    89.03    89.03    89.03%\n",
      "Epoch   3566 Step:      0 Loss:   0.2530 Training Acc:    89.03    89.03    89.03%\n",
      "Epoch   3567 Step:      0 Loss:   0.2530 Training Acc:    89.01    89.01    89.01%\n",
      "Epoch   3568 Step:      0 Loss:   0.2529 Training Acc:    89.04    89.04    89.04%\n",
      "Epoch   3569 Step:      0 Loss:   0.2528 Training Acc:    89.03    89.03    89.03%\n",
      "Epoch   3570 Step:      0 Loss:   0.2528 Training Acc:    89.02    89.02    89.02%\n",
      "Epoch   3571 Step:      0 Loss:   0.2528 Training Acc:    89.04    89.04    89.04%\n",
      "Epoch   3572 Step:      0 Loss:   0.2527 Training Acc:    89.04    89.04    89.04%\n",
      "Epoch   3573 Step:      0 Loss:   0.2526 Training Acc:    89.03    89.03    89.03%\n",
      "Epoch   3574 Step:      0 Loss:   0.2526 Training Acc:    89.06    89.06    89.06%\n",
      "Epoch   3575 Step:      0 Loss:   0.2525 Training Acc:    89.04    89.04    89.04%\n",
      "Epoch   3576 Step:      0 Loss:   0.2525 Training Acc:    89.05    89.05    89.05%\n",
      "Epoch   3577 Step:      0 Loss:   0.2524 Training Acc:    89.06    89.06    89.06%\n",
      "Epoch   3578 Step:      0 Loss:   0.2524 Training Acc:    89.04    89.04    89.04%\n",
      "Epoch   3579 Step:      0 Loss:   0.2523 Training Acc:    89.07    89.07    89.07%\n",
      "Epoch   3580 Step:      0 Loss:   0.2523 Training Acc:    89.06    89.06    89.06%\n",
      "Epoch   3581 Step:      0 Loss:   0.2522 Training Acc:    89.06    89.06    89.06%\n",
      "Epoch   3582 Step:      0 Loss:   0.2522 Training Acc:    89.07    89.07    89.07%\n",
      "Epoch   3583 Step:      0 Loss:   0.2521 Training Acc:    89.07    89.07    89.07%\n",
      "Epoch   3584 Step:      0 Loss:   0.2521 Training Acc:    89.08    89.08    89.08%\n",
      "Epoch   3585 Step:      0 Loss:   0.2520 Training Acc:    89.07    89.07    89.07%\n",
      "Epoch   3586 Step:      0 Loss:   0.2520 Training Acc:    89.07    89.07    89.07%\n",
      "Epoch   3587 Step:      0 Loss:   0.2519 Training Acc:    89.08    89.08    89.08%\n",
      "Epoch   3588 Step:      0 Loss:   0.2519 Training Acc:    89.07    89.07    89.07%\n",
      "Epoch   3589 Step:      0 Loss:   0.2518 Training Acc:    89.08    89.08    89.08%\n",
      "Epoch   3590 Step:      0 Loss:   0.2518 Training Acc:    89.09    89.09    89.09%\n",
      "Epoch   3591 Step:      0 Loss:   0.2518 Training Acc:    89.08    89.08    89.08%\n",
      "Epoch   3592 Step:      0 Loss:   0.2517 Training Acc:    89.08    89.08    89.08%\n",
      "Epoch   3593 Step:      0 Loss:   0.2517 Training Acc:    89.09    89.09    89.09%\n",
      "Epoch   3594 Step:      0 Loss:   0.2516 Training Acc:    89.09    89.09    89.09%\n",
      "Epoch   3595 Step:      0 Loss:   0.2516 Training Acc:    89.10    89.10    89.10%\n",
      "Epoch   3596 Step:      0 Loss:   0.2515 Training Acc:    89.10    89.10    89.10%\n",
      "Epoch   3597 Step:      0 Loss:   0.2515 Training Acc:    89.10    89.10    89.10%\n",
      "Epoch   3598 Step:      0 Loss:   0.2514 Training Acc:    89.11    89.11    89.11%\n",
      "Epoch   3599 Step:      0 Loss:   0.2514 Training Acc:    89.10    89.10    89.10%\n",
      "Epoch   3600 Step:      0 Loss:   0.2513 Training Acc:    89.10    89.10    89.10%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3601 Step:      0 Loss:   0.2513 Training Acc:    89.11    89.11    89.11%\n",
      "Epoch   3602 Step:      0 Loss:   0.2512 Training Acc:    89.11    89.11    89.11%\n",
      "Epoch   3603 Step:      0 Loss:   0.2512 Training Acc:    89.12    89.12    89.12%\n",
      "Epoch   3604 Step:      0 Loss:   0.2511 Training Acc:    89.11    89.11    89.11%\n",
      "Epoch   3605 Step:      0 Loss:   0.2511 Training Acc:    89.12    89.12    89.12%\n",
      "Epoch   3606 Step:      0 Loss:   0.2511 Training Acc:    89.12    89.12    89.12%\n",
      "Epoch   3607 Step:      0 Loss:   0.2510 Training Acc:    89.12    89.12    89.12%\n",
      "Epoch   3608 Step:      0 Loss:   0.2510 Training Acc:    89.12    89.12    89.12%\n",
      "Epoch   3609 Step:      0 Loss:   0.2509 Training Acc:    89.12    89.12    89.12%\n",
      "Epoch   3610 Step:      0 Loss:   0.2509 Training Acc:    89.13    89.13    89.13%\n",
      "Epoch   3611 Step:      0 Loss:   0.2508 Training Acc:    89.13    89.13    89.13%\n",
      "Epoch   3612 Step:      0 Loss:   0.2508 Training Acc:    89.14    89.14    89.14%\n",
      "Epoch   3613 Step:      0 Loss:   0.2507 Training Acc:    89.14    89.14    89.14%\n",
      "Epoch   3614 Step:      0 Loss:   0.2507 Training Acc:    89.14    89.14    89.14%\n",
      "Epoch   3615 Step:      0 Loss:   0.2506 Training Acc:    89.15    89.15    89.15%\n",
      "Epoch   3616 Step:      0 Loss:   0.2506 Training Acc:    89.15    89.15    89.15%\n",
      "Epoch   3617 Step:      0 Loss:   0.2505 Training Acc:    89.15    89.15    89.15%\n",
      "Epoch   3618 Step:      0 Loss:   0.2505 Training Acc:    89.15    89.15    89.15%\n",
      "Epoch   3619 Step:      0 Loss:   0.2505 Training Acc:    89.16    89.16    89.16%\n",
      "Epoch   3620 Step:      0 Loss:   0.2504 Training Acc:    89.16    89.16    89.16%\n",
      "Epoch   3621 Step:      0 Loss:   0.2504 Training Acc:    89.16    89.16    89.16%\n",
      "Epoch   3622 Step:      0 Loss:   0.2503 Training Acc:    89.16    89.16    89.16%\n",
      "Epoch   3623 Step:      0 Loss:   0.2503 Training Acc:    89.17    89.17    89.17%\n",
      "Epoch   3624 Step:      0 Loss:   0.2502 Training Acc:    89.16    89.16    89.16%\n",
      "Epoch   3625 Step:      0 Loss:   0.2502 Training Acc:    89.17    89.17    89.17%\n",
      "Epoch   3626 Step:      0 Loss:   0.2501 Training Acc:    89.17    89.17    89.17%\n",
      "Epoch   3627 Step:      0 Loss:   0.2501 Training Acc:    89.17    89.17    89.17%\n",
      "Epoch   3628 Step:      0 Loss:   0.2500 Training Acc:    89.17    89.17    89.17%\n",
      "Epoch   3629 Step:      0 Loss:   0.2500 Training Acc:    89.17    89.17    89.17%\n",
      "Epoch   3630 Step:      0 Loss:   0.2499 Training Acc:    89.17    89.17    89.17%\n",
      "Epoch   3631 Step:      0 Loss:   0.2499 Training Acc:    89.18    89.18    89.18%\n",
      "Epoch   3632 Step:      0 Loss:   0.2498 Training Acc:    89.18    89.18    89.18%\n",
      "Epoch   3633 Step:      0 Loss:   0.2498 Training Acc:    89.18    89.18    89.18%\n",
      "Epoch   3634 Step:      0 Loss:   0.2498 Training Acc:    89.18    89.18    89.18%\n",
      "Epoch   3635 Step:      0 Loss:   0.2497 Training Acc:    89.18    89.18    89.18%\n",
      "Epoch   3636 Step:      0 Loss:   0.2497 Training Acc:    89.19    89.19    89.19%\n",
      "Epoch   3637 Step:      0 Loss:   0.2496 Training Acc:    89.19    89.19    89.19%\n",
      "Epoch   3638 Step:      0 Loss:   0.2496 Training Acc:    89.19    89.19    89.19%\n",
      "Epoch   3639 Step:      0 Loss:   0.2495 Training Acc:    89.20    89.20    89.20%\n",
      "Epoch   3640 Step:      0 Loss:   0.2495 Training Acc:    89.20    89.20    89.20%\n",
      "Epoch   3641 Step:      0 Loss:   0.2494 Training Acc:    89.20    89.20    89.20%\n",
      "Epoch   3642 Step:      0 Loss:   0.2494 Training Acc:    89.20    89.20    89.20%\n",
      "Epoch   3643 Step:      0 Loss:   0.2493 Training Acc:    89.21    89.21    89.21%\n",
      "Epoch   3644 Step:      0 Loss:   0.2493 Training Acc:    89.21    89.21    89.21%\n",
      "Epoch   3645 Step:      0 Loss:   0.2492 Training Acc:    89.21    89.21    89.21%\n",
      "Epoch   3646 Step:      0 Loss:   0.2492 Training Acc:    89.21    89.21    89.21%\n",
      "Epoch   3647 Step:      0 Loss:   0.2491 Training Acc:    89.21    89.21    89.21%\n",
      "Epoch   3648 Step:      0 Loss:   0.2491 Training Acc:    89.22    89.22    89.22%\n",
      "Epoch   3649 Step:      0 Loss:   0.2491 Training Acc:    89.22    89.22    89.22%\n",
      "Epoch   3650 Step:      0 Loss:   0.2490 Training Acc:    89.22    89.22    89.22%\n",
      "Epoch   3651 Step:      0 Loss:   0.2490 Training Acc:    89.23    89.23    89.23%\n",
      "Epoch   3652 Step:      0 Loss:   0.2489 Training Acc:    89.23    89.23    89.23%\n",
      "Epoch   3653 Step:      0 Loss:   0.2489 Training Acc:    89.23    89.23    89.23%\n",
      "Epoch   3654 Step:      0 Loss:   0.2488 Training Acc:    89.24    89.24    89.24%\n",
      "Epoch   3655 Step:      0 Loss:   0.2488 Training Acc:    89.24    89.24    89.24%\n",
      "Epoch   3656 Step:      0 Loss:   0.2487 Training Acc:    89.24    89.24    89.24%\n",
      "Epoch   3657 Step:      0 Loss:   0.2487 Training Acc:    89.24    89.24    89.24%\n",
      "Epoch   3658 Step:      0 Loss:   0.2486 Training Acc:    89.25    89.25    89.25%\n",
      "Epoch   3659 Step:      0 Loss:   0.2486 Training Acc:    89.24    89.24    89.24%\n",
      "Epoch   3660 Step:      0 Loss:   0.2485 Training Acc:    89.25    89.25    89.25%\n",
      "Epoch   3661 Step:      0 Loss:   0.2485 Training Acc:    89.25    89.25    89.25%\n",
      "Epoch   3662 Step:      0 Loss:   0.2484 Training Acc:    89.25    89.25    89.25%\n",
      "Epoch   3663 Step:      0 Loss:   0.2484 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3664 Step:      0 Loss:   0.2483 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3665 Step:      0 Loss:   0.2483 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3666 Step:      0 Loss:   0.2483 Training Acc:    89.25    89.25    89.25%\n",
      "Epoch   3667 Step:      0 Loss:   0.2482 Training Acc:    89.27    89.27    89.27%\n",
      "Epoch   3668 Step:      0 Loss:   0.2482 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3669 Step:      0 Loss:   0.2481 Training Acc:    89.27    89.27    89.27%\n",
      "Epoch   3670 Step:      0 Loss:   0.2481 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3671 Step:      0 Loss:   0.2481 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3672 Step:      0 Loss:   0.2481 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3673 Step:      0 Loss:   0.2481 Training Acc:    89.25    89.25    89.25%\n",
      "Epoch   3674 Step:      0 Loss:   0.2482 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3675 Step:      0 Loss:   0.2484 Training Acc:    89.22    89.22    89.22%\n",
      "Epoch   3676 Step:      0 Loss:   0.2488 Training Acc:    89.21    89.21    89.21%\n",
      "Epoch   3677 Step:      0 Loss:   0.2494 Training Acc:    89.11    89.11    89.11%\n",
      "Epoch   3678 Step:      0 Loss:   0.2504 Training Acc:    89.14    89.14    89.14%\n",
      "Epoch   3679 Step:      0 Loss:   0.2522 Training Acc:    88.87    88.87    88.87%\n",
      "Epoch   3680 Step:      0 Loss:   0.2543 Training Acc:    88.87    88.87    88.87%\n",
      "Epoch   3681 Step:      0 Loss:   0.2573 Training Acc:    88.52    88.52    88.52%\n",
      "Epoch   3682 Step:      0 Loss:   0.2584 Training Acc:    88.62    88.62    88.62%\n",
      "Epoch   3683 Step:      0 Loss:   0.2579 Training Acc:    88.48    88.48    88.48%\n",
      "Epoch   3684 Step:      0 Loss:   0.2534 Training Acc:    88.94    88.94    88.94%\n",
      "Epoch   3685 Step:      0 Loss:   0.2490 Training Acc:    89.13    89.13    89.13%\n",
      "Epoch   3686 Step:      0 Loss:   0.2475 Training Acc:    89.27    89.27    89.27%\n",
      "Epoch   3687 Step:      0 Loss:   0.2494 Training Acc:    89.19    89.19    89.19%\n",
      "Epoch   3688 Step:      0 Loss:   0.2521 Training Acc:    88.88    88.88    88.88%\n",
      "Epoch   3689 Step:      0 Loss:   0.2524 Training Acc:    89.00    89.00    89.00%\n",
      "Epoch   3690 Step:      0 Loss:   0.2504 Training Acc:    88.99    88.99    88.99%\n",
      "Epoch   3691 Step:      0 Loss:   0.2479 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3692 Step:      0 Loss:   0.2473 Training Acc:    89.29    89.29    89.29%\n",
      "Epoch   3693 Step:      0 Loss:   0.2486 Training Acc:    89.15    89.15    89.15%\n",
      "Epoch   3694 Step:      0 Loss:   0.2499 Training Acc:    89.14    89.14    89.14%\n",
      "Epoch   3695 Step:      0 Loss:   0.2498 Training Acc:    89.04    89.04    89.04%\n",
      "Epoch   3696 Step:      0 Loss:   0.2482 Training Acc:    89.24    89.24    89.24%\n",
      "Epoch   3697 Step:      0 Loss:   0.2470 Training Acc:    89.30    89.30    89.30%\n",
      "Epoch   3698 Step:      0 Loss:   0.2471 Training Acc:    89.27    89.27    89.27%\n",
      "Epoch   3699 Step:      0 Loss:   0.2480 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3700 Step:      0 Loss:   0.2485 Training Acc:    89.15    89.15    89.15%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3701 Step:      0 Loss:   0.2480 Training Acc:    89.25    89.25    89.25%\n",
      "Epoch   3702 Step:      0 Loss:   0.2471 Training Acc:    89.31    89.31    89.31%\n",
      "Epoch   3703 Step:      0 Loss:   0.2466 Training Acc:    89.32    89.32    89.32%\n",
      "Epoch   3704 Step:      0 Loss:   0.2469 Training Acc:    89.33    89.33    89.33%\n",
      "Epoch   3705 Step:      0 Loss:   0.2473 Training Acc:    89.26    89.26    89.26%\n",
      "Epoch   3706 Step:      0 Loss:   0.2474 Training Acc:    89.27    89.27    89.27%\n",
      "Epoch   3707 Step:      0 Loss:   0.2470 Training Acc:    89.30    89.30    89.30%\n",
      "Epoch   3708 Step:      0 Loss:   0.2465 Training Acc:    89.33    89.33    89.33%\n",
      "Epoch   3709 Step:      0 Loss:   0.2464 Training Acc:    89.35    89.35    89.35%\n",
      "Epoch   3710 Step:      0 Loss:   0.2465 Training Acc:    89.32    89.32    89.32%\n",
      "Epoch   3711 Step:      0 Loss:   0.2467 Training Acc:    89.33    89.33    89.33%\n",
      "Epoch   3712 Step:      0 Loss:   0.2467 Training Acc:    89.32    89.32    89.32%\n",
      "Epoch   3713 Step:      0 Loss:   0.2464 Training Acc:    89.34    89.34    89.34%\n",
      "Epoch   3714 Step:      0 Loss:   0.2461 Training Acc:    89.36    89.36    89.36%\n",
      "Epoch   3715 Step:      0 Loss:   0.2461 Training Acc:    89.36    89.36    89.36%\n",
      "Epoch   3716 Step:      0 Loss:   0.2461 Training Acc:    89.36    89.36    89.36%\n",
      "Epoch   3717 Step:      0 Loss:   0.2462 Training Acc:    89.34    89.34    89.34%\n",
      "Epoch   3718 Step:      0 Loss:   0.2462 Training Acc:    89.35    89.35    89.35%\n",
      "Epoch   3719 Step:      0 Loss:   0.2460 Training Acc:    89.36    89.36    89.36%\n",
      "Epoch   3720 Step:      0 Loss:   0.2458 Training Acc:    89.37    89.37    89.37%\n",
      "Epoch   3721 Step:      0 Loss:   0.2458 Training Acc:    89.38    89.38    89.38%\n",
      "Epoch   3722 Step:      0 Loss:   0.2458 Training Acc:    89.37    89.37    89.37%\n",
      "Epoch   3723 Step:      0 Loss:   0.2458 Training Acc:    89.38    89.38    89.38%\n",
      "Epoch   3724 Step:      0 Loss:   0.2458 Training Acc:    89.37    89.37    89.37%\n",
      "Epoch   3725 Step:      0 Loss:   0.2457 Training Acc:    89.37    89.37    89.37%\n",
      "Epoch   3726 Step:      0 Loss:   0.2456 Training Acc:    89.39    89.39    89.39%\n",
      "Epoch   3727 Step:      0 Loss:   0.2455 Training Acc:    89.40    89.40    89.40%\n",
      "Epoch   3728 Step:      0 Loss:   0.2454 Training Acc:    89.40    89.40    89.40%\n",
      "Epoch   3729 Step:      0 Loss:   0.2454 Training Acc:    89.39    89.39    89.39%\n",
      "Epoch   3730 Step:      0 Loss:   0.2454 Training Acc:    89.39    89.39    89.39%\n",
      "Epoch   3731 Step:      0 Loss:   0.2454 Training Acc:    89.40    89.40    89.40%\n",
      "Epoch   3732 Step:      0 Loss:   0.2453 Training Acc:    89.40    89.40    89.40%\n",
      "Epoch   3733 Step:      0 Loss:   0.2452 Training Acc:    89.41    89.41    89.41%\n",
      "Epoch   3734 Step:      0 Loss:   0.2451 Training Acc:    89.42    89.42    89.42%\n",
      "Epoch   3735 Step:      0 Loss:   0.2451 Training Acc:    89.42    89.42    89.42%\n",
      "Epoch   3736 Step:      0 Loss:   0.2451 Training Acc:    89.42    89.42    89.42%\n",
      "Epoch   3737 Step:      0 Loss:   0.2451 Training Acc:    89.41    89.41    89.41%\n",
      "Epoch   3738 Step:      0 Loss:   0.2450 Training Acc:    89.42    89.42    89.42%\n",
      "Epoch   3739 Step:      0 Loss:   0.2449 Training Acc:    89.43    89.43    89.43%\n",
      "Epoch   3740 Step:      0 Loss:   0.2449 Training Acc:    89.43    89.43    89.43%\n",
      "Epoch   3741 Step:      0 Loss:   0.2448 Training Acc:    89.43    89.43    89.43%\n",
      "Epoch   3742 Step:      0 Loss:   0.2448 Training Acc:    89.43    89.43    89.43%\n",
      "Epoch   3743 Step:      0 Loss:   0.2447 Training Acc:    89.44    89.44    89.44%\n",
      "Epoch   3744 Step:      0 Loss:   0.2447 Training Acc:    89.44    89.44    89.44%\n",
      "Epoch   3745 Step:      0 Loss:   0.2447 Training Acc:    89.44    89.44    89.44%\n",
      "Epoch   3746 Step:      0 Loss:   0.2446 Training Acc:    89.44    89.44    89.44%\n",
      "Epoch   3747 Step:      0 Loss:   0.2445 Training Acc:    89.45    89.45    89.45%\n",
      "Epoch   3748 Step:      0 Loss:   0.2445 Training Acc:    89.45    89.45    89.45%\n",
      "Epoch   3749 Step:      0 Loss:   0.2445 Training Acc:    89.45    89.45    89.45%\n",
      "Epoch   3750 Step:      0 Loss:   0.2444 Training Acc:    89.46    89.46    89.46%\n",
      "Epoch   3751 Step:      0 Loss:   0.2444 Training Acc:    89.45    89.45    89.45%\n",
      "Epoch   3752 Step:      0 Loss:   0.2443 Training Acc:    89.46    89.46    89.46%\n",
      "Epoch   3753 Step:      0 Loss:   0.2443 Training Acc:    89.46    89.46    89.46%\n",
      "Epoch   3754 Step:      0 Loss:   0.2442 Training Acc:    89.46    89.46    89.46%\n",
      "Epoch   3755 Step:      0 Loss:   0.2442 Training Acc:    89.47    89.47    89.47%\n",
      "Epoch   3756 Step:      0 Loss:   0.2441 Training Acc:    89.47    89.47    89.47%\n",
      "Epoch   3757 Step:      0 Loss:   0.2441 Training Acc:    89.47    89.47    89.47%\n",
      "Epoch   3758 Step:      0 Loss:   0.2440 Training Acc:    89.48    89.48    89.48%\n",
      "Epoch   3759 Step:      0 Loss:   0.2440 Training Acc:    89.48    89.48    89.48%\n",
      "Epoch   3760 Step:      0 Loss:   0.2440 Training Acc:    89.48    89.48    89.48%\n",
      "Epoch   3761 Step:      0 Loss:   0.2439 Training Acc:    89.48    89.48    89.48%\n",
      "Epoch   3762 Step:      0 Loss:   0.2439 Training Acc:    89.49    89.49    89.49%\n",
      "Epoch   3763 Step:      0 Loss:   0.2438 Training Acc:    89.48    89.48    89.48%\n",
      "Epoch   3764 Step:      0 Loss:   0.2438 Training Acc:    89.49    89.49    89.49%\n",
      "Epoch   3765 Step:      0 Loss:   0.2437 Training Acc:    89.49    89.49    89.49%\n",
      "Epoch   3766 Step:      0 Loss:   0.2437 Training Acc:    89.50    89.50    89.50%\n",
      "Epoch   3767 Step:      0 Loss:   0.2436 Training Acc:    89.50    89.50    89.50%\n",
      "Epoch   3768 Step:      0 Loss:   0.2436 Training Acc:    89.50    89.50    89.50%\n",
      "Epoch   3769 Step:      0 Loss:   0.2435 Training Acc:    89.50    89.50    89.50%\n",
      "Epoch   3770 Step:      0 Loss:   0.2435 Training Acc:    89.50    89.50    89.50%\n",
      "Epoch   3771 Step:      0 Loss:   0.2434 Training Acc:    89.51    89.51    89.51%\n",
      "Epoch   3772 Step:      0 Loss:   0.2434 Training Acc:    89.51    89.51    89.51%\n",
      "Epoch   3773 Step:      0 Loss:   0.2433 Training Acc:    89.52    89.52    89.52%\n",
      "Epoch   3774 Step:      0 Loss:   0.2433 Training Acc:    89.52    89.52    89.52%\n",
      "Epoch   3775 Step:      0 Loss:   0.2433 Training Acc:    89.52    89.52    89.52%\n",
      "Epoch   3776 Step:      0 Loss:   0.2432 Training Acc:    89.53    89.53    89.53%\n",
      "Epoch   3777 Step:      0 Loss:   0.2432 Training Acc:    89.52    89.52    89.52%\n",
      "Epoch   3778 Step:      0 Loss:   0.2431 Training Acc:    89.53    89.53    89.53%\n",
      "Epoch   3779 Step:      0 Loss:   0.2431 Training Acc:    89.52    89.52    89.52%\n",
      "Epoch   3780 Step:      0 Loss:   0.2430 Training Acc:    89.53    89.53    89.53%\n",
      "Epoch   3781 Step:      0 Loss:   0.2430 Training Acc:    89.53    89.53    89.53%\n",
      "Epoch   3782 Step:      0 Loss:   0.2429 Training Acc:    89.54    89.54    89.54%\n",
      "Epoch   3783 Step:      0 Loss:   0.2429 Training Acc:    89.54    89.54    89.54%\n",
      "Epoch   3784 Step:      0 Loss:   0.2428 Training Acc:    89.54    89.54    89.54%\n",
      "Epoch   3785 Step:      0 Loss:   0.2428 Training Acc:    89.54    89.54    89.54%\n",
      "Epoch   3786 Step:      0 Loss:   0.2427 Training Acc:    89.55    89.55    89.55%\n",
      "Epoch   3787 Step:      0 Loss:   0.2427 Training Acc:    89.55    89.55    89.55%\n",
      "Epoch   3788 Step:      0 Loss:   0.2427 Training Acc:    89.55    89.55    89.55%\n",
      "Epoch   3789 Step:      0 Loss:   0.2426 Training Acc:    89.55    89.55    89.55%\n",
      "Epoch   3790 Step:      0 Loss:   0.2426 Training Acc:    89.55    89.55    89.55%\n",
      "Epoch   3791 Step:      0 Loss:   0.2425 Training Acc:    89.55    89.55    89.55%\n",
      "Epoch   3792 Step:      0 Loss:   0.2425 Training Acc:    89.56    89.56    89.56%\n",
      "Epoch   3793 Step:      0 Loss:   0.2424 Training Acc:    89.56    89.56    89.56%\n",
      "Epoch   3794 Step:      0 Loss:   0.2424 Training Acc:    89.56    89.56    89.56%\n",
      "Epoch   3795 Step:      0 Loss:   0.2423 Training Acc:    89.56    89.56    89.56%\n",
      "Epoch   3796 Step:      0 Loss:   0.2423 Training Acc:    89.56    89.56    89.56%\n",
      "Epoch   3797 Step:      0 Loss:   0.2422 Training Acc:    89.57    89.57    89.57%\n",
      "Epoch   3798 Step:      0 Loss:   0.2422 Training Acc:    89.57    89.57    89.57%\n",
      "Epoch   3799 Step:      0 Loss:   0.2421 Training Acc:    89.57    89.57    89.57%\n",
      "Epoch   3800 Step:      0 Loss:   0.2421 Training Acc:    89.57    89.57    89.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3801 Step:      0 Loss:   0.2421 Training Acc:    89.58    89.58    89.58%\n",
      "Epoch   3802 Step:      0 Loss:   0.2420 Training Acc:    89.58    89.58    89.58%\n",
      "Epoch   3803 Step:      0 Loss:   0.2420 Training Acc:    89.58    89.58    89.58%\n",
      "Epoch   3804 Step:      0 Loss:   0.2419 Training Acc:    89.58    89.58    89.58%\n",
      "Epoch   3805 Step:      0 Loss:   0.2419 Training Acc:    89.59    89.59    89.59%\n",
      "Epoch   3806 Step:      0 Loss:   0.2418 Training Acc:    89.58    89.58    89.58%\n",
      "Epoch   3807 Step:      0 Loss:   0.2418 Training Acc:    89.59    89.59    89.59%\n",
      "Epoch   3808 Step:      0 Loss:   0.2417 Training Acc:    89.59    89.59    89.59%\n",
      "Epoch   3809 Step:      0 Loss:   0.2417 Training Acc:    89.60    89.60    89.60%\n",
      "Epoch   3810 Step:      0 Loss:   0.2416 Training Acc:    89.59    89.59    89.59%\n",
      "Epoch   3811 Step:      0 Loss:   0.2416 Training Acc:    89.60    89.60    89.60%\n",
      "Epoch   3812 Step:      0 Loss:   0.2415 Training Acc:    89.60    89.60    89.60%\n",
      "Epoch   3813 Step:      0 Loss:   0.2415 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3814 Step:      0 Loss:   0.2414 Training Acc:    89.60    89.60    89.60%\n",
      "Epoch   3815 Step:      0 Loss:   0.2414 Training Acc:    89.62    89.62    89.62%\n",
      "Epoch   3816 Step:      0 Loss:   0.2413 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3817 Step:      0 Loss:   0.2413 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3818 Step:      0 Loss:   0.2412 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3819 Step:      0 Loss:   0.2412 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3820 Step:      0 Loss:   0.2412 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3821 Step:      0 Loss:   0.2411 Training Acc:    89.62    89.62    89.62%\n",
      "Epoch   3822 Step:      0 Loss:   0.2411 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3823 Step:      0 Loss:   0.2410 Training Acc:    89.62    89.62    89.62%\n",
      "Epoch   3824 Step:      0 Loss:   0.2410 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3825 Step:      0 Loss:   0.2409 Training Acc:    89.62    89.62    89.62%\n",
      "Epoch   3826 Step:      0 Loss:   0.2409 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3827 Step:      0 Loss:   0.2409 Training Acc:    89.63    89.63    89.63%\n",
      "Epoch   3828 Step:      0 Loss:   0.2408 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3829 Step:      0 Loss:   0.2408 Training Acc:    89.63    89.63    89.63%\n",
      "Epoch   3830 Step:      0 Loss:   0.2408 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3831 Step:      0 Loss:   0.2409 Training Acc:    89.62    89.62    89.62%\n",
      "Epoch   3832 Step:      0 Loss:   0.2409 Training Acc:    89.60    89.60    89.60%\n",
      "Epoch   3833 Step:      0 Loss:   0.2411 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3834 Step:      0 Loss:   0.2413 Training Acc:    89.57    89.57    89.57%\n",
      "Epoch   3835 Step:      0 Loss:   0.2416 Training Acc:    89.59    89.59    89.59%\n",
      "Epoch   3836 Step:      0 Loss:   0.2422 Training Acc:    89.47    89.47    89.47%\n",
      "Epoch   3837 Step:      0 Loss:   0.2430 Training Acc:    89.51    89.51    89.51%\n",
      "Epoch   3838 Step:      0 Loss:   0.2442 Training Acc:    89.31    89.31    89.31%\n",
      "Epoch   3839 Step:      0 Loss:   0.2456 Training Acc:    89.32    89.32    89.32%\n",
      "Epoch   3840 Step:      0 Loss:   0.2473 Training Acc:    89.07    89.07    89.07%\n",
      "Epoch   3841 Step:      0 Loss:   0.2484 Training Acc:    89.13    89.13    89.13%\n",
      "Epoch   3842 Step:      0 Loss:   0.2488 Training Acc:    88.98    88.98    88.98%\n",
      "Epoch   3843 Step:      0 Loss:   0.2474 Training Acc:    89.19    89.19    89.19%\n",
      "Epoch   3844 Step:      0 Loss:   0.2451 Training Acc:    89.27    89.27    89.27%\n",
      "Epoch   3845 Step:      0 Loss:   0.2427 Training Acc:    89.49    89.49    89.49%\n",
      "Epoch   3846 Step:      0 Loss:   0.2417 Training Acc:    89.59    89.59    89.59%\n",
      "Epoch   3847 Step:      0 Loss:   0.2423 Training Acc:    89.45    89.45    89.45%\n",
      "Epoch   3848 Step:      0 Loss:   0.2432 Training Acc:    89.49    89.49    89.49%\n",
      "Epoch   3849 Step:      0 Loss:   0.2438 Training Acc:    89.31    89.31    89.31%\n",
      "Epoch   3850 Step:      0 Loss:   0.2428 Training Acc:    89.51    89.51    89.51%\n",
      "Epoch   3851 Step:      0 Loss:   0.2412 Training Acc:    89.52    89.52    89.52%\n",
      "Epoch   3852 Step:      0 Loss:   0.2401 Training Acc:    89.66    89.66    89.66%\n",
      "Epoch   3853 Step:      0 Loss:   0.2400 Training Acc:    89.69    89.69    89.69%\n",
      "Epoch   3854 Step:      0 Loss:   0.2409 Training Acc:    89.54    89.54    89.54%\n",
      "Epoch   3855 Step:      0 Loss:   0.2417 Training Acc:    89.59    89.59    89.59%\n",
      "Epoch   3856 Step:      0 Loss:   0.2419 Training Acc:    89.45    89.45    89.45%\n",
      "Epoch   3857 Step:      0 Loss:   0.2410 Training Acc:    89.62    89.62    89.62%\n",
      "Epoch   3858 Step:      0 Loss:   0.2400 Training Acc:    89.63    89.63    89.63%\n",
      "Epoch   3859 Step:      0 Loss:   0.2394 Training Acc:    89.71    89.71    89.71%\n",
      "Epoch   3860 Step:      0 Loss:   0.2395 Training Acc:    89.69    89.69    89.69%\n",
      "Epoch   3861 Step:      0 Loss:   0.2400 Training Acc:    89.60    89.60    89.60%\n",
      "Epoch   3862 Step:      0 Loss:   0.2403 Training Acc:    89.65    89.65    89.65%\n",
      "Epoch   3863 Step:      0 Loss:   0.2402 Training Acc:    89.60    89.60    89.60%\n",
      "Epoch   3864 Step:      0 Loss:   0.2397 Training Acc:    89.68    89.68    89.68%\n",
      "Epoch   3865 Step:      0 Loss:   0.2393 Training Acc:    89.68    89.68    89.68%\n",
      "Epoch   3866 Step:      0 Loss:   0.2391 Training Acc:    89.71    89.71    89.71%\n",
      "Epoch   3867 Step:      0 Loss:   0.2393 Training Acc:    89.71    89.71    89.71%\n",
      "Epoch   3868 Step:      0 Loss:   0.2395 Training Acc:    89.68    89.68    89.68%\n",
      "Epoch   3869 Step:      0 Loss:   0.2395 Training Acc:    89.70    89.70    89.70%\n",
      "Epoch   3870 Step:      0 Loss:   0.2393 Training Acc:    89.67    89.67    89.67%\n",
      "Epoch   3871 Step:      0 Loss:   0.2390 Training Acc:    89.73    89.73    89.73%\n",
      "Epoch   3872 Step:      0 Loss:   0.2388 Training Acc:    89.72    89.72    89.72%\n",
      "Epoch   3873 Step:      0 Loss:   0.2387 Training Acc:    89.73    89.73    89.73%\n",
      "Epoch   3874 Step:      0 Loss:   0.2387 Training Acc:    89.74    89.74    89.74%\n",
      "Epoch   3875 Step:      0 Loss:   0.2388 Training Acc:    89.71    89.71    89.71%\n",
      "Epoch   3876 Step:      0 Loss:   0.2388 Training Acc:    89.73    89.73    89.73%\n",
      "Epoch   3877 Step:      0 Loss:   0.2387 Training Acc:    89.71    89.71    89.71%\n",
      "Epoch   3878 Step:      0 Loss:   0.2386 Training Acc:    89.75    89.75    89.75%\n",
      "Epoch   3879 Step:      0 Loss:   0.2384 Training Acc:    89.75    89.75    89.75%\n",
      "Epoch   3880 Step:      0 Loss:   0.2384 Training Acc:    89.73    89.73    89.73%\n",
      "Epoch   3881 Step:      0 Loss:   0.2384 Training Acc:    89.77    89.77    89.77%\n",
      "Epoch   3882 Step:      0 Loss:   0.2384 Training Acc:    89.73    89.73    89.73%\n",
      "Epoch   3883 Step:      0 Loss:   0.2384 Training Acc:    89.76    89.76    89.76%\n",
      "Epoch   3884 Step:      0 Loss:   0.2383 Training Acc:    89.74    89.74    89.74%\n",
      "Epoch   3885 Step:      0 Loss:   0.2382 Training Acc:    89.78    89.78    89.78%\n",
      "Epoch   3886 Step:      0 Loss:   0.2381 Training Acc:    89.75    89.75    89.75%\n",
      "Epoch   3887 Step:      0 Loss:   0.2380 Training Acc:    89.79    89.79    89.79%\n",
      "Epoch   3888 Step:      0 Loss:   0.2380 Training Acc:    89.77    89.77    89.77%\n",
      "Epoch   3889 Step:      0 Loss:   0.2380 Training Acc:    89.78    89.78    89.78%\n",
      "Epoch   3890 Step:      0 Loss:   0.2379 Training Acc:    89.78    89.78    89.78%\n",
      "Epoch   3891 Step:      0 Loss:   0.2379 Training Acc:    89.77    89.77    89.77%\n",
      "Epoch   3892 Step:      0 Loss:   0.2378 Training Acc:    89.78    89.78    89.78%\n",
      "Epoch   3893 Step:      0 Loss:   0.2378 Training Acc:    89.78    89.78    89.78%\n",
      "Epoch   3894 Step:      0 Loss:   0.2377 Training Acc:    89.79    89.79    89.79%\n",
      "Epoch   3895 Step:      0 Loss:   0.2376 Training Acc:    89.80    89.80    89.80%\n",
      "Epoch   3896 Step:      0 Loss:   0.2376 Training Acc:    89.80    89.80    89.80%\n",
      "Epoch   3897 Step:      0 Loss:   0.2375 Training Acc:    89.80    89.80    89.80%\n",
      "Epoch   3898 Step:      0 Loss:   0.2375 Training Acc:    89.79    89.79    89.79%\n",
      "Epoch   3899 Step:      0 Loss:   0.2375 Training Acc:    89.80    89.80    89.80%\n",
      "Epoch   3900 Step:      0 Loss:   0.2374 Training Acc:    89.80    89.80    89.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3901 Step:      0 Loss:   0.2374 Training Acc:    89.80    89.80    89.80%\n",
      "Epoch   3902 Step:      0 Loss:   0.2373 Training Acc:    89.81    89.81    89.81%\n",
      "Epoch   3903 Step:      0 Loss:   0.2373 Training Acc:    89.82    89.82    89.82%\n",
      "Epoch   3904 Step:      0 Loss:   0.2372 Training Acc:    89.82    89.82    89.82%\n",
      "Epoch   3905 Step:      0 Loss:   0.2372 Training Acc:    89.81    89.81    89.81%\n",
      "Epoch   3906 Step:      0 Loss:   0.2371 Training Acc:    89.82    89.82    89.82%\n",
      "Epoch   3907 Step:      0 Loss:   0.2371 Training Acc:    89.82    89.82    89.82%\n",
      "Epoch   3908 Step:      0 Loss:   0.2370 Training Acc:    89.83    89.83    89.83%\n",
      "Epoch   3909 Step:      0 Loss:   0.2370 Training Acc:    89.82    89.82    89.82%\n",
      "Epoch   3910 Step:      0 Loss:   0.2369 Training Acc:    89.84    89.84    89.84%\n",
      "Epoch   3911 Step:      0 Loss:   0.2369 Training Acc:    89.83    89.83    89.83%\n",
      "Epoch   3912 Step:      0 Loss:   0.2368 Training Acc:    89.84    89.84    89.84%\n",
      "Epoch   3913 Step:      0 Loss:   0.2368 Training Acc:    89.83    89.83    89.83%\n",
      "Epoch   3914 Step:      0 Loss:   0.2368 Training Acc:    89.85    89.85    89.85%\n",
      "Epoch   3915 Step:      0 Loss:   0.2367 Training Acc:    89.84    89.84    89.84%\n",
      "Epoch   3916 Step:      0 Loss:   0.2367 Training Acc:    89.85    89.85    89.85%\n",
      "Epoch   3917 Step:      0 Loss:   0.2367 Training Acc:    89.84    89.84    89.84%\n",
      "Epoch   3918 Step:      0 Loss:   0.2367 Training Acc:    89.85    89.85    89.85%\n",
      "Epoch   3919 Step:      0 Loss:   0.2367 Training Acc:    89.83    89.83    89.83%\n",
      "Epoch   3920 Step:      0 Loss:   0.2368 Training Acc:    89.84    89.84    89.84%\n",
      "Epoch   3921 Step:      0 Loss:   0.2369 Training Acc:    89.81    89.81    89.81%\n",
      "Epoch   3922 Step:      0 Loss:   0.2372 Training Acc:    89.83    89.83    89.83%\n",
      "Epoch   3923 Step:      0 Loss:   0.2376 Training Acc:    89.76    89.76    89.76%\n",
      "Epoch   3924 Step:      0 Loss:   0.2383 Training Acc:    89.72    89.72    89.72%\n",
      "Epoch   3925 Step:      0 Loss:   0.2395 Training Acc:    89.61    89.61    89.61%\n",
      "Epoch   3926 Step:      0 Loss:   0.2414 Training Acc:    89.51    89.51    89.51%\n",
      "Epoch   3927 Step:      0 Loss:   0.2440 Training Acc:    89.29    89.29    89.29%\n",
      "Epoch   3928 Step:      0 Loss:   0.2481 Training Acc:    89.02    89.02    89.02%\n",
      "Epoch   3929 Step:      0 Loss:   0.2518 Training Acc:    88.75    88.75    88.75%\n",
      "Epoch   3930 Step:      0 Loss:   0.2561 Training Acc:    88.52    88.52    88.52%\n",
      "Epoch   3931 Step:      0 Loss:   0.2549 Training Acc:    88.57    88.57    88.57%\n",
      "Epoch   3932 Step:      0 Loss:   0.2501 Training Acc:    88.92    88.92    88.92%\n",
      "Epoch   3933 Step:      0 Loss:   0.2404 Training Acc:    89.57    89.57    89.57%\n",
      "Epoch   3934 Step:      0 Loss:   0.2371 Training Acc:    89.77    89.77    89.77%\n",
      "Epoch   3935 Step:      0 Loss:   0.2413 Training Acc:    89.52    89.52    89.52%\n",
      "Epoch   3936 Step:      0 Loss:   0.2447 Training Acc:    89.21    89.21    89.21%\n",
      "Epoch   3937 Step:      0 Loss:   0.2424 Training Acc:    89.44    89.44    89.44%\n",
      "Epoch   3938 Step:      0 Loss:   0.2376 Training Acc:    89.74    89.74    89.74%\n",
      "Epoch   3939 Step:      0 Loss:   0.2375 Training Acc:    89.72    89.72    89.72%\n",
      "Epoch   3940 Step:      0 Loss:   0.2403 Training Acc:    89.59    89.59    89.59%\n",
      "Epoch   3941 Step:      0 Loss:   0.2408 Training Acc:    89.46    89.46    89.46%\n",
      "Epoch   3942 Step:      0 Loss:   0.2382 Training Acc:    89.74    89.74    89.74%\n",
      "Epoch   3943 Step:      0 Loss:   0.2363 Training Acc:    89.84    89.84    89.84%\n",
      "Epoch   3944 Step:      0 Loss:   0.2371 Training Acc:    89.74    89.74    89.74%\n",
      "Epoch   3945 Step:      0 Loss:   0.2387 Training Acc:    89.69    89.69    89.69%\n",
      "Epoch   3946 Step:      0 Loss:   0.2382 Training Acc:    89.66    89.66    89.66%\n",
      "Epoch   3947 Step:      0 Loss:   0.2362 Training Acc:    89.89    89.89    89.89%\n",
      "Epoch   3948 Step:      0 Loss:   0.2358 Training Acc:    89.86    89.86    89.86%\n",
      "Epoch   3949 Step:      0 Loss:   0.2369 Training Acc:    89.76    89.76    89.76%\n",
      "Epoch   3950 Step:      0 Loss:   0.2373 Training Acc:    89.81    89.81    89.81%\n",
      "Epoch   3951 Step:      0 Loss:   0.2361 Training Acc:    89.79    89.79    89.79%\n",
      "Epoch   3952 Step:      0 Loss:   0.2353 Training Acc:    89.92    89.92    89.92%\n",
      "Epoch   3953 Step:      0 Loss:   0.2358 Training Acc:    89.91    89.91    89.91%\n",
      "Epoch   3954 Step:      0 Loss:   0.2364 Training Acc:    89.79    89.79    89.79%\n",
      "Epoch   3955 Step:      0 Loss:   0.2360 Training Acc:    89.89    89.89    89.89%\n",
      "Epoch   3956 Step:      0 Loss:   0.2352 Training Acc:    89.92    89.92    89.92%\n",
      "Epoch   3957 Step:      0 Loss:   0.2352 Training Acc:    89.91    89.91    89.91%\n",
      "Epoch   3958 Step:      0 Loss:   0.2356 Training Acc:    89.94    89.94    89.94%\n",
      "Epoch   3959 Step:      0 Loss:   0.2355 Training Acc:    89.87    89.87    89.87%\n",
      "Epoch   3960 Step:      0 Loss:   0.2351 Training Acc:    89.93    89.93    89.93%\n",
      "Epoch   3961 Step:      0 Loss:   0.2349 Training Acc:    89.95    89.95    89.95%\n",
      "Epoch   3962 Step:      0 Loss:   0.2349 Training Acc:    89.89    89.89    89.89%\n",
      "Epoch   3963 Step:      0 Loss:   0.2351 Training Acc:    89.96    89.96    89.96%\n",
      "Epoch   3964 Step:      0 Loss:   0.2350 Training Acc:    89.90    89.90    89.90%\n",
      "Epoch   3965 Step:      0 Loss:   0.2347 Training Acc:    89.95    89.95    89.95%\n",
      "Epoch   3966 Step:      0 Loss:   0.2345 Training Acc:    89.95    89.95    89.95%\n",
      "Epoch   3967 Step:      0 Loss:   0.2346 Training Acc:    89.93    89.93    89.93%\n",
      "Epoch   3968 Step:      0 Loss:   0.2347 Training Acc:    89.95    89.95    89.95%\n",
      "Epoch   3969 Step:      0 Loss:   0.2345 Training Acc:    89.93    89.93    89.93%\n",
      "Epoch   3970 Step:      0 Loss:   0.2343 Training Acc:    89.96    89.96    89.96%\n",
      "Epoch   3971 Step:      0 Loss:   0.2343 Training Acc:    89.97    89.97    89.97%\n",
      "Epoch   3972 Step:      0 Loss:   0.2344 Training Acc:    89.95    89.95    89.95%\n",
      "Epoch   3973 Step:      0 Loss:   0.2343 Training Acc:    89.98    89.98    89.98%\n",
      "Epoch   3974 Step:      0 Loss:   0.2342 Training Acc:    89.95    89.95    89.95%\n",
      "Epoch   3975 Step:      0 Loss:   0.2341 Training Acc:    89.98    89.98    89.98%\n",
      "Epoch   3976 Step:      0 Loss:   0.2341 Training Acc:    89.97    89.97    89.97%\n",
      "Epoch   3977 Step:      0 Loss:   0.2341 Training Acc:    89.97    89.97    89.97%\n",
      "Epoch   3978 Step:      0 Loss:   0.2340 Training Acc:    89.99    89.99    89.99%\n",
      "Epoch   3979 Step:      0 Loss:   0.2340 Training Acc:    89.96    89.96    89.96%\n",
      "Epoch   3980 Step:      0 Loss:   0.2339 Training Acc:    90.00    90.00    90.00%\n",
      "Epoch   3981 Step:      0 Loss:   0.2338 Training Acc:    90.00    90.00    90.00%\n",
      "Epoch   3982 Step:      0 Loss:   0.2338 Training Acc:    89.97    89.97    89.97%\n",
      "Epoch   3983 Step:      0 Loss:   0.2338 Training Acc:    90.01    90.01    90.01%\n",
      "Epoch   3984 Step:      0 Loss:   0.2337 Training Acc:    89.99    89.99    89.99%\n",
      "Epoch   3985 Step:      0 Loss:   0.2336 Training Acc:    90.01    90.01    90.01%\n",
      "Epoch   3986 Step:      0 Loss:   0.2336 Training Acc:    90.01    90.01    90.01%\n",
      "Epoch   3987 Step:      0 Loss:   0.2336 Training Acc:    89.99    89.99    89.99%\n",
      "Epoch   3988 Step:      0 Loss:   0.2335 Training Acc:    90.01    90.01    90.01%\n",
      "Epoch   3989 Step:      0 Loss:   0.2335 Training Acc:    90.01    90.01    90.01%\n",
      "Epoch   3990 Step:      0 Loss:   0.2334 Training Acc:    90.02    90.02    90.02%\n",
      "Epoch   3991 Step:      0 Loss:   0.2334 Training Acc:    90.01    90.01    90.01%\n",
      "Epoch   3992 Step:      0 Loss:   0.2333 Training Acc:    90.01    90.01    90.01%\n",
      "Epoch   3993 Step:      0 Loss:   0.2333 Training Acc:    90.03    90.03    90.03%\n",
      "Epoch   3994 Step:      0 Loss:   0.2332 Training Acc:    90.02    90.02    90.02%\n",
      "Epoch   3995 Step:      0 Loss:   0.2332 Training Acc:    90.02    90.02    90.02%\n",
      "Epoch   3996 Step:      0 Loss:   0.2331 Training Acc:    90.03    90.03    90.03%\n",
      "Epoch   3997 Step:      0 Loss:   0.2331 Training Acc:    90.02    90.02    90.02%\n",
      "Epoch   3998 Step:      0 Loss:   0.2331 Training Acc:    90.03    90.03    90.03%\n",
      "Epoch   3999 Step:      0 Loss:   0.2330 Training Acc:    90.03    90.03    90.03%\n",
      "Epoch   4000 Step:      0 Loss:   0.2330 Training Acc:    90.04    90.04    90.04%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4001 Step:      0 Loss:   0.2329 Training Acc:    90.04    90.04    90.04%\n",
      "Epoch   4002 Step:      0 Loss:   0.2329 Training Acc:    90.03    90.03    90.03%\n",
      "Epoch   4003 Step:      0 Loss:   0.2328 Training Acc:    90.04    90.04    90.04%\n",
      "Epoch   4004 Step:      0 Loss:   0.2328 Training Acc:    90.04    90.04    90.04%\n",
      "Epoch   4005 Step:      0 Loss:   0.2327 Training Acc:    90.05    90.05    90.05%\n",
      "Epoch   4006 Step:      0 Loss:   0.2327 Training Acc:    90.04    90.04    90.04%\n",
      "Epoch   4007 Step:      0 Loss:   0.2326 Training Acc:    90.04    90.04    90.04%\n",
      "Epoch   4008 Step:      0 Loss:   0.2326 Training Acc:    90.05    90.05    90.05%\n",
      "Epoch   4009 Step:      0 Loss:   0.2326 Training Acc:    90.04    90.04    90.04%\n",
      "Epoch   4010 Step:      0 Loss:   0.2325 Training Acc:    90.05    90.05    90.05%\n",
      "Epoch   4011 Step:      0 Loss:   0.2325 Training Acc:    90.06    90.06    90.06%\n",
      "Epoch   4012 Step:      0 Loss:   0.2324 Training Acc:    90.06    90.06    90.06%\n",
      "Epoch   4013 Step:      0 Loss:   0.2324 Training Acc:    90.07    90.07    90.07%\n",
      "Epoch   4014 Step:      0 Loss:   0.2323 Training Acc:    90.06    90.06    90.06%\n",
      "Epoch   4015 Step:      0 Loss:   0.2323 Training Acc:    90.07    90.07    90.07%\n",
      "Epoch   4016 Step:      0 Loss:   0.2323 Training Acc:    90.06    90.06    90.06%\n",
      "Epoch   4017 Step:      0 Loss:   0.2322 Training Acc:    90.07    90.07    90.07%\n",
      "Epoch   4018 Step:      0 Loss:   0.2322 Training Acc:    90.07    90.07    90.07%\n",
      "Epoch   4019 Step:      0 Loss:   0.2321 Training Acc:    90.07    90.07    90.07%\n",
      "Epoch   4020 Step:      0 Loss:   0.2321 Training Acc:    90.07    90.07    90.07%\n",
      "Epoch   4021 Step:      0 Loss:   0.2320 Training Acc:    90.07    90.07    90.07%\n",
      "Epoch   4022 Step:      0 Loss:   0.2320 Training Acc:    90.08    90.08    90.08%\n",
      "Epoch   4023 Step:      0 Loss:   0.2319 Training Acc:    90.09    90.09    90.09%\n",
      "Epoch   4024 Step:      0 Loss:   0.2319 Training Acc:    90.09    90.09    90.09%\n",
      "Epoch   4025 Step:      0 Loss:   0.2319 Training Acc:    90.09    90.09    90.09%\n",
      "Epoch   4026 Step:      0 Loss:   0.2318 Training Acc:    90.09    90.09    90.09%\n",
      "Epoch   4027 Step:      0 Loss:   0.2318 Training Acc:    90.09    90.09    90.09%\n",
      "Epoch   4028 Step:      0 Loss:   0.2317 Training Acc:    90.09    90.09    90.09%\n",
      "Epoch   4029 Step:      0 Loss:   0.2317 Training Acc:    90.10    90.10    90.10%\n",
      "Epoch   4030 Step:      0 Loss:   0.2316 Training Acc:    90.10    90.10    90.10%\n",
      "Epoch   4031 Step:      0 Loss:   0.2316 Training Acc:    90.10    90.10    90.10%\n",
      "Epoch   4032 Step:      0 Loss:   0.2315 Training Acc:    90.10    90.10    90.10%\n",
      "Epoch   4033 Step:      0 Loss:   0.2315 Training Acc:    90.10    90.10    90.10%\n",
      "Epoch   4034 Step:      0 Loss:   0.2315 Training Acc:    90.11    90.11    90.11%\n",
      "Epoch   4035 Step:      0 Loss:   0.2314 Training Acc:    90.11    90.11    90.11%\n",
      "Epoch   4036 Step:      0 Loss:   0.2314 Training Acc:    90.11    90.11    90.11%\n",
      "Epoch   4037 Step:      0 Loss:   0.2313 Training Acc:    90.11    90.11    90.11%\n",
      "Epoch   4038 Step:      0 Loss:   0.2313 Training Acc:    90.11    90.11    90.11%\n",
      "Epoch   4039 Step:      0 Loss:   0.2312 Training Acc:    90.11    90.11    90.11%\n",
      "Epoch   4040 Step:      0 Loss:   0.2312 Training Acc:    90.12    90.12    90.12%\n",
      "Epoch   4041 Step:      0 Loss:   0.2311 Training Acc:    90.12    90.12    90.12%\n",
      "Epoch   4042 Step:      0 Loss:   0.2311 Training Acc:    90.13    90.13    90.13%\n",
      "Epoch   4043 Step:      0 Loss:   0.2311 Training Acc:    90.12    90.12    90.12%\n",
      "Epoch   4044 Step:      0 Loss:   0.2310 Training Acc:    90.13    90.13    90.13%\n",
      "Epoch   4045 Step:      0 Loss:   0.2310 Training Acc:    90.12    90.12    90.12%\n",
      "Epoch   4046 Step:      0 Loss:   0.2309 Training Acc:    90.13    90.13    90.13%\n",
      "Epoch   4047 Step:      0 Loss:   0.2309 Training Acc:    90.12    90.12    90.12%\n",
      "Epoch   4048 Step:      0 Loss:   0.2308 Training Acc:    90.14    90.14    90.14%\n",
      "Epoch   4049 Step:      0 Loss:   0.2308 Training Acc:    90.12    90.12    90.12%\n",
      "Epoch   4050 Step:      0 Loss:   0.2308 Training Acc:    90.14    90.14    90.14%\n",
      "Epoch   4051 Step:      0 Loss:   0.2307 Training Acc:    90.13    90.13    90.13%\n",
      "Epoch   4052 Step:      0 Loss:   0.2307 Training Acc:    90.15    90.15    90.15%\n",
      "Epoch   4053 Step:      0 Loss:   0.2307 Training Acc:    90.13    90.13    90.13%\n",
      "Epoch   4054 Step:      0 Loss:   0.2306 Training Acc:    90.16    90.16    90.16%\n",
      "Epoch   4055 Step:      0 Loss:   0.2306 Training Acc:    90.12    90.12    90.12%\n",
      "Epoch   4056 Step:      0 Loss:   0.2307 Training Acc:    90.17    90.17    90.17%\n",
      "Epoch   4057 Step:      0 Loss:   0.2308 Training Acc:    90.11    90.11    90.11%\n",
      "Epoch   4058 Step:      0 Loss:   0.2310 Training Acc:    90.15    90.15    90.15%\n",
      "Epoch   4059 Step:      0 Loss:   0.2314 Training Acc:    90.05    90.05    90.05%\n",
      "Epoch   4060 Step:      0 Loss:   0.2321 Training Acc:    90.07    90.07    90.07%\n",
      "Epoch   4061 Step:      0 Loss:   0.2334 Training Acc:    89.87    89.87    89.87%\n",
      "Epoch   4062 Step:      0 Loss:   0.2354 Training Acc:    89.85    89.85    89.85%\n",
      "Epoch   4063 Step:      0 Loss:   0.2387 Training Acc:    89.48    89.48    89.48%\n",
      "Epoch   4064 Step:      0 Loss:   0.2424 Training Acc:    89.36    89.36    89.36%\n",
      "Epoch   4065 Step:      0 Loss:   0.2461 Training Acc:    89.03    89.03    89.03%\n",
      "Epoch   4066 Step:      0 Loss:   0.2454 Training Acc:    89.17    89.17    89.17%\n",
      "Epoch   4067 Step:      0 Loss:   0.2405 Training Acc:    89.35    89.35    89.35%\n",
      "Epoch   4068 Step:      0 Loss:   0.2335 Training Acc:    89.97    89.97    89.97%\n",
      "Epoch   4069 Step:      0 Loss:   0.2310 Training Acc:    90.15    90.15    90.15%\n",
      "Epoch   4070 Step:      0 Loss:   0.2337 Training Acc:    89.83    89.83    89.83%\n",
      "Epoch   4071 Step:      0 Loss:   0.2367 Training Acc:    89.77    89.77    89.77%\n",
      "Epoch   4072 Step:      0 Loss:   0.2364 Training Acc:    89.65    89.65    89.65%\n",
      "Epoch   4073 Step:      0 Loss:   0.2328 Training Acc:    90.01    90.01    90.01%\n",
      "Epoch   4074 Step:      0 Loss:   0.2310 Training Acc:    90.14    90.14    90.14%\n",
      "Epoch   4075 Step:      0 Loss:   0.2323 Training Acc:    89.92    89.92    89.92%\n",
      "Epoch   4076 Step:      0 Loss:   0.2338 Training Acc:    89.97    89.97    89.97%\n",
      "Epoch   4077 Step:      0 Loss:   0.2332 Training Acc:    89.85    89.85    89.85%\n",
      "Epoch   4078 Step:      0 Loss:   0.2310 Training Acc:    90.12    90.12    90.12%\n",
      "Epoch   4079 Step:      0 Loss:   0.2303 Training Acc:    90.18    90.18    90.18%\n",
      "Epoch   4080 Step:      0 Loss:   0.2315 Training Acc:    89.98    89.98    89.98%\n",
      "Epoch   4081 Step:      0 Loss:   0.2323 Training Acc:    90.09    90.09    90.09%\n",
      "Epoch   4082 Step:      0 Loss:   0.2316 Training Acc:    89.97    89.97    89.97%\n",
      "Epoch   4083 Step:      0 Loss:   0.2300 Training Acc:    90.19    90.19    90.19%\n",
      "Epoch   4084 Step:      0 Loss:   0.2296 Training Acc:    90.22    90.22    90.22%\n",
      "Epoch   4085 Step:      0 Loss:   0.2306 Training Acc:    90.06    90.06    90.06%\n",
      "Epoch   4086 Step:      0 Loss:   0.2312 Training Acc:    90.15    90.15    90.15%\n",
      "Epoch   4087 Step:      0 Loss:   0.2306 Training Acc:    90.06    90.06    90.06%\n",
      "Epoch   4088 Step:      0 Loss:   0.2295 Training Acc:    90.22    90.22    90.22%\n",
      "Epoch   4089 Step:      0 Loss:   0.2292 Training Acc:    90.23    90.23    90.23%\n",
      "Epoch   4090 Step:      0 Loss:   0.2298 Training Acc:    90.13    90.13    90.13%\n",
      "Epoch   4091 Step:      0 Loss:   0.2302 Training Acc:    90.20    90.20    90.20%\n",
      "Epoch   4092 Step:      0 Loss:   0.2299 Training Acc:    90.13    90.13    90.13%\n",
      "Epoch   4093 Step:      0 Loss:   0.2292 Training Acc:    90.24    90.24    90.24%\n",
      "Epoch   4094 Step:      0 Loss:   0.2290 Training Acc:    90.25    90.25    90.25%\n",
      "Epoch   4095 Step:      0 Loss:   0.2293 Training Acc:    90.17    90.17    90.17%\n",
      "Epoch   4096 Step:      0 Loss:   0.2295 Training Acc:    90.24    90.24    90.24%\n",
      "Epoch   4097 Step:      0 Loss:   0.2293 Training Acc:    90.16    90.16    90.16%\n",
      "Epoch   4098 Step:      0 Loss:   0.2289 Training Acc:    90.25    90.25    90.25%\n",
      "Epoch   4099 Step:      0 Loss:   0.2288 Training Acc:    90.25    90.25    90.25%\n",
      "Epoch   4100 Step:      0 Loss:   0.2289 Training Acc:    90.21    90.21    90.21%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4101 Step:      0 Loss:   0.2290 Training Acc:    90.25    90.25    90.25%\n",
      "Epoch   4102 Step:      0 Loss:   0.2289 Training Acc:    90.20    90.20    90.20%\n",
      "Epoch   4103 Step:      0 Loss:   0.2286 Training Acc:    90.27    90.27    90.27%\n",
      "Epoch   4104 Step:      0 Loss:   0.2285 Training Acc:    90.26    90.26    90.26%\n",
      "Epoch   4105 Step:      0 Loss:   0.2286 Training Acc:    90.23    90.23    90.23%\n",
      "Epoch   4106 Step:      0 Loss:   0.2287 Training Acc:    90.27    90.27    90.27%\n",
      "Epoch   4107 Step:      0 Loss:   0.2286 Training Acc:    90.23    90.23    90.23%\n",
      "Epoch   4108 Step:      0 Loss:   0.2284 Training Acc:    90.28    90.28    90.28%\n",
      "Epoch   4109 Step:      0 Loss:   0.2283 Training Acc:    90.27    90.27    90.27%\n",
      "Epoch   4110 Step:      0 Loss:   0.2283 Training Acc:    90.26    90.26    90.26%\n",
      "Epoch   4111 Step:      0 Loss:   0.2283 Training Acc:    90.28    90.28    90.28%\n",
      "Epoch   4112 Step:      0 Loss:   0.2283 Training Acc:    90.25    90.25    90.25%\n",
      "Epoch   4113 Step:      0 Loss:   0.2282 Training Acc:    90.29    90.29    90.29%\n",
      "Epoch   4114 Step:      0 Loss:   0.2281 Training Acc:    90.27    90.27    90.27%\n",
      "Epoch   4115 Step:      0 Loss:   0.2280 Training Acc:    90.27    90.27    90.27%\n",
      "Epoch   4116 Step:      0 Loss:   0.2280 Training Acc:    90.29    90.29    90.29%\n",
      "Epoch   4117 Step:      0 Loss:   0.2280 Training Acc:    90.26    90.26    90.26%\n",
      "Epoch   4118 Step:      0 Loss:   0.2280 Training Acc:    90.30    90.30    90.30%\n",
      "Epoch   4119 Step:      0 Loss:   0.2279 Training Acc:    90.28    90.28    90.28%\n",
      "Epoch   4120 Step:      0 Loss:   0.2278 Training Acc:    90.29    90.29    90.29%\n",
      "Epoch   4121 Step:      0 Loss:   0.2278 Training Acc:    90.30    90.30    90.30%\n",
      "Epoch   4122 Step:      0 Loss:   0.2278 Training Acc:    90.27    90.27    90.27%\n",
      "Epoch   4123 Step:      0 Loss:   0.2277 Training Acc:    90.31    90.31    90.31%\n",
      "Epoch   4124 Step:      0 Loss:   0.2277 Training Acc:    90.28    90.28    90.28%\n",
      "Epoch   4125 Step:      0 Loss:   0.2276 Training Acc:    90.31    90.31    90.31%\n",
      "Epoch   4126 Step:      0 Loss:   0.2276 Training Acc:    90.31    90.31    90.31%\n",
      "Epoch   4127 Step:      0 Loss:   0.2275 Training Acc:    90.30    90.30    90.30%\n",
      "Epoch   4128 Step:      0 Loss:   0.2275 Training Acc:    90.32    90.32    90.32%\n",
      "Epoch   4129 Step:      0 Loss:   0.2275 Training Acc:    90.29    90.29    90.29%\n",
      "Epoch   4130 Step:      0 Loss:   0.2274 Training Acc:    90.32    90.32    90.32%\n",
      "Epoch   4131 Step:      0 Loss:   0.2273 Training Acc:    90.31    90.31    90.31%\n",
      "Epoch   4132 Step:      0 Loss:   0.2273 Training Acc:    90.32    90.32    90.32%\n",
      "Epoch   4133 Step:      0 Loss:   0.2273 Training Acc:    90.33    90.33    90.33%\n",
      "Epoch   4134 Step:      0 Loss:   0.2272 Training Acc:    90.32    90.32    90.32%\n",
      "Epoch   4135 Step:      0 Loss:   0.2272 Training Acc:    90.34    90.34    90.34%\n",
      "Epoch   4136 Step:      0 Loss:   0.2271 Training Acc:    90.32    90.32    90.32%\n",
      "Epoch   4137 Step:      0 Loss:   0.2271 Training Acc:    90.33    90.33    90.33%\n",
      "Epoch   4138 Step:      0 Loss:   0.2271 Training Acc:    90.34    90.34    90.34%\n",
      "Epoch   4139 Step:      0 Loss:   0.2270 Training Acc:    90.33    90.33    90.33%\n",
      "Epoch   4140 Step:      0 Loss:   0.2270 Training Acc:    90.35    90.35    90.35%\n",
      "Epoch   4141 Step:      0 Loss:   0.2269 Training Acc:    90.33    90.33    90.33%\n",
      "Epoch   4142 Step:      0 Loss:   0.2269 Training Acc:    90.35    90.35    90.35%\n",
      "Epoch   4143 Step:      0 Loss:   0.2268 Training Acc:    90.34    90.34    90.34%\n",
      "Epoch   4144 Step:      0 Loss:   0.2268 Training Acc:    90.34    90.34    90.34%\n",
      "Epoch   4145 Step:      0 Loss:   0.2268 Training Acc:    90.35    90.35    90.35%\n",
      "Epoch   4146 Step:      0 Loss:   0.2267 Training Acc:    90.34    90.34    90.34%\n",
      "Epoch   4147 Step:      0 Loss:   0.2267 Training Acc:    90.36    90.36    90.36%\n",
      "Epoch   4148 Step:      0 Loss:   0.2266 Training Acc:    90.35    90.35    90.35%\n",
      "Epoch   4149 Step:      0 Loss:   0.2266 Training Acc:    90.36    90.36    90.36%\n",
      "Epoch   4150 Step:      0 Loss:   0.2265 Training Acc:    90.36    90.36    90.36%\n",
      "Epoch   4151 Step:      0 Loss:   0.2265 Training Acc:    90.36    90.36    90.36%\n",
      "Epoch   4152 Step:      0 Loss:   0.2265 Training Acc:    90.37    90.37    90.37%\n",
      "Epoch   4153 Step:      0 Loss:   0.2264 Training Acc:    90.36    90.36    90.36%\n",
      "Epoch   4154 Step:      0 Loss:   0.2264 Training Acc:    90.37    90.37    90.37%\n",
      "Epoch   4155 Step:      0 Loss:   0.2263 Training Acc:    90.37    90.37    90.37%\n",
      "Epoch   4156 Step:      0 Loss:   0.2263 Training Acc:    90.37    90.37    90.37%\n",
      "Epoch   4157 Step:      0 Loss:   0.2263 Training Acc:    90.38    90.38    90.38%\n",
      "Epoch   4158 Step:      0 Loss:   0.2262 Training Acc:    90.37    90.37    90.37%\n",
      "Epoch   4159 Step:      0 Loss:   0.2262 Training Acc:    90.39    90.39    90.39%\n",
      "Epoch   4160 Step:      0 Loss:   0.2261 Training Acc:    90.37    90.37    90.37%\n",
      "Epoch   4161 Step:      0 Loss:   0.2261 Training Acc:    90.39    90.39    90.39%\n",
      "Epoch   4162 Step:      0 Loss:   0.2260 Training Acc:    90.38    90.38    90.38%\n",
      "Epoch   4163 Step:      0 Loss:   0.2260 Training Acc:    90.39    90.39    90.39%\n",
      "Epoch   4164 Step:      0 Loss:   0.2260 Training Acc:    90.39    90.39    90.39%\n",
      "Epoch   4165 Step:      0 Loss:   0.2259 Training Acc:    90.39    90.39    90.39%\n",
      "Epoch   4166 Step:      0 Loss:   0.2259 Training Acc:    90.40    90.40    90.40%\n",
      "Epoch   4167 Step:      0 Loss:   0.2258 Training Acc:    90.39    90.39    90.39%\n",
      "Epoch   4168 Step:      0 Loss:   0.2258 Training Acc:    90.40    90.40    90.40%\n",
      "Epoch   4169 Step:      0 Loss:   0.2257 Training Acc:    90.40    90.40    90.40%\n",
      "Epoch   4170 Step:      0 Loss:   0.2257 Training Acc:    90.40    90.40    90.40%\n",
      "Epoch   4171 Step:      0 Loss:   0.2257 Training Acc:    90.40    90.40    90.40%\n",
      "Epoch   4172 Step:      0 Loss:   0.2256 Training Acc:    90.40    90.40    90.40%\n",
      "Epoch   4173 Step:      0 Loss:   0.2256 Training Acc:    90.41    90.41    90.41%\n",
      "Epoch   4174 Step:      0 Loss:   0.2255 Training Acc:    90.40    90.40    90.40%\n",
      "Epoch   4175 Step:      0 Loss:   0.2255 Training Acc:    90.41    90.41    90.41%\n",
      "Epoch   4176 Step:      0 Loss:   0.2254 Training Acc:    90.41    90.41    90.41%\n",
      "Epoch   4177 Step:      0 Loss:   0.2254 Training Acc:    90.42    90.42    90.42%\n",
      "Epoch   4178 Step:      0 Loss:   0.2254 Training Acc:    90.41    90.41    90.41%\n",
      "Epoch   4179 Step:      0 Loss:   0.2253 Training Acc:    90.42    90.42    90.42%\n",
      "Epoch   4180 Step:      0 Loss:   0.2253 Training Acc:    90.42    90.42    90.42%\n",
      "Epoch   4181 Step:      0 Loss:   0.2252 Training Acc:    90.42    90.42    90.42%\n",
      "Epoch   4182 Step:      0 Loss:   0.2252 Training Acc:    90.43    90.43    90.43%\n",
      "Epoch   4183 Step:      0 Loss:   0.2251 Training Acc:    90.43    90.43    90.43%\n",
      "Epoch   4184 Step:      0 Loss:   0.2251 Training Acc:    90.43    90.43    90.43%\n",
      "Epoch   4185 Step:      0 Loss:   0.2251 Training Acc:    90.43    90.43    90.43%\n",
      "Epoch   4186 Step:      0 Loss:   0.2250 Training Acc:    90.43    90.43    90.43%\n",
      "Epoch   4187 Step:      0 Loss:   0.2250 Training Acc:    90.44    90.44    90.44%\n",
      "Epoch   4188 Step:      0 Loss:   0.2249 Training Acc:    90.43    90.43    90.43%\n",
      "Epoch   4189 Step:      0 Loss:   0.2249 Training Acc:    90.45    90.45    90.45%\n",
      "Epoch   4190 Step:      0 Loss:   0.2249 Training Acc:    90.44    90.44    90.44%\n",
      "Epoch   4191 Step:      0 Loss:   0.2248 Training Acc:    90.45    90.45    90.45%\n",
      "Epoch   4192 Step:      0 Loss:   0.2248 Training Acc:    90.44    90.44    90.44%\n",
      "Epoch   4193 Step:      0 Loss:   0.2247 Training Acc:    90.45    90.45    90.45%\n",
      "Epoch   4194 Step:      0 Loss:   0.2247 Training Acc:    90.44    90.44    90.44%\n",
      "Epoch   4195 Step:      0 Loss:   0.2247 Training Acc:    90.45    90.45    90.45%\n",
      "Epoch   4196 Step:      0 Loss:   0.2246 Training Acc:    90.45    90.45    90.45%\n",
      "Epoch   4197 Step:      0 Loss:   0.2246 Training Acc:    90.47    90.47    90.47%\n",
      "Epoch   4198 Step:      0 Loss:   0.2246 Training Acc:    90.44    90.44    90.44%\n",
      "Epoch   4199 Step:      0 Loss:   0.2246 Training Acc:    90.46    90.46    90.46%\n",
      "Epoch   4200 Step:      0 Loss:   0.2247 Training Acc:    90.44    90.44    90.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4201 Step:      0 Loss:   0.2248 Training Acc:    90.46    90.46    90.46%\n",
      "Epoch   4202 Step:      0 Loss:   0.2250 Training Acc:    90.40    90.40    90.40%\n",
      "Epoch   4203 Step:      0 Loss:   0.2254 Training Acc:    90.42    90.42    90.42%\n",
      "Epoch   4204 Step:      0 Loss:   0.2260 Training Acc:    90.33    90.33    90.33%\n",
      "Epoch   4205 Step:      0 Loss:   0.2271 Training Acc:    90.29    90.29    90.29%\n",
      "Epoch   4206 Step:      0 Loss:   0.2286 Training Acc:    90.13    90.13    90.13%\n",
      "Epoch   4207 Step:      0 Loss:   0.2313 Training Acc:    89.96    89.96    89.96%\n",
      "Epoch   4208 Step:      0 Loss:   0.2345 Training Acc:    89.76    89.76    89.76%\n",
      "Epoch   4209 Step:      0 Loss:   0.2399 Training Acc:    89.34    89.34    89.34%\n",
      "Epoch   4210 Step:      0 Loss:   0.2440 Training Acc:    89.15    89.15    89.15%\n",
      "Epoch   4211 Step:      0 Loss:   0.2497 Training Acc:    88.72    88.72    88.72%\n",
      "Epoch   4212 Step:      0 Loss:   0.2484 Training Acc:    88.91    88.91    88.91%\n",
      "Epoch   4213 Step:      0 Loss:   0.2460 Training Acc:    89.04    89.04    89.04%\n",
      "Epoch   4214 Step:      0 Loss:   0.2335 Training Acc:    89.83    89.83    89.83%\n",
      "Epoch   4215 Step:      0 Loss:   0.2257 Training Acc:    90.42    90.42    90.42%\n",
      "Epoch   4216 Step:      0 Loss:   0.2294 Training Acc:    90.08    90.08    90.08%\n",
      "Epoch   4217 Step:      0 Loss:   0.2333 Training Acc:    89.85    89.85    89.85%\n",
      "Epoch   4218 Step:      0 Loss:   0.2317 Training Acc:    89.95    89.95    89.95%\n",
      "Epoch   4219 Step:      0 Loss:   0.2267 Training Acc:    90.28    90.28    90.28%\n",
      "Epoch   4220 Step:      0 Loss:   0.2263 Training Acc:    90.31    90.31    90.31%\n",
      "Epoch   4221 Step:      0 Loss:   0.2284 Training Acc:    90.20    90.20    90.20%\n",
      "Epoch   4222 Step:      0 Loss:   0.2289 Training Acc:    90.12    90.12    90.12%\n",
      "Epoch   4223 Step:      0 Loss:   0.2266 Training Acc:    90.26    90.26    90.26%\n",
      "Epoch   4224 Step:      0 Loss:   0.2251 Training Acc:    90.44    90.44    90.44%\n",
      "Epoch   4225 Step:      0 Loss:   0.2260 Training Acc:    90.27    90.27    90.27%\n",
      "Epoch   4226 Step:      0 Loss:   0.2270 Training Acc:    90.26    90.26    90.26%\n",
      "Epoch   4227 Step:      0 Loss:   0.2256 Training Acc:    90.33    90.33    90.33%\n",
      "Epoch   4228 Step:      0 Loss:   0.2241 Training Acc:    90.46    90.46    90.46%\n",
      "Epoch   4229 Step:      0 Loss:   0.2248 Training Acc:    90.45    90.45    90.45%\n",
      "Epoch   4230 Step:      0 Loss:   0.2258 Training Acc:    90.30    90.30    90.30%\n",
      "Epoch   4231 Step:      0 Loss:   0.2248 Training Acc:    90.42    90.42    90.42%\n",
      "Epoch   4232 Step:      0 Loss:   0.2236 Training Acc:    90.50    90.50    90.50%\n",
      "Epoch   4233 Step:      0 Loss:   0.2240 Training Acc:    90.45    90.45    90.45%\n",
      "Epoch   4234 Step:      0 Loss:   0.2245 Training Acc:    90.48    90.48    90.48%\n",
      "Epoch   4235 Step:      0 Loss:   0.2243 Training Acc:    90.41    90.41    90.41%\n",
      "Epoch   4236 Step:      0 Loss:   0.2238 Training Acc:    90.51    90.51    90.51%\n",
      "Epoch   4237 Step:      0 Loss:   0.2233 Training Acc:    90.51    90.51    90.51%\n",
      "Epoch   4238 Step:      0 Loss:   0.2237 Training Acc:    90.46    90.46    90.46%\n",
      "Epoch   4239 Step:      0 Loss:   0.2240 Training Acc:    90.50    90.50    90.50%\n",
      "Epoch   4240 Step:      0 Loss:   0.2234 Training Acc:    90.49    90.49    90.49%\n",
      "Epoch   4241 Step:      0 Loss:   0.2231 Training Acc:    90.53    90.53    90.53%\n",
      "Epoch   4242 Step:      0 Loss:   0.2233 Training Acc:    90.53    90.53    90.53%\n",
      "Epoch   4243 Step:      0 Loss:   0.2234 Training Acc:    90.49    90.49    90.49%\n",
      "Epoch   4244 Step:      0 Loss:   0.2232 Training Acc:    90.53    90.53    90.53%\n",
      "Epoch   4245 Step:      0 Loss:   0.2229 Training Acc:    90.54    90.54    90.54%\n",
      "Epoch   4246 Step:      0 Loss:   0.2228 Training Acc:    90.54    90.54    90.54%\n",
      "Epoch   4247 Step:      0 Loss:   0.2230 Training Acc:    90.54    90.54    90.54%\n",
      "Epoch   4248 Step:      0 Loss:   0.2229 Training Acc:    90.53    90.53    90.53%\n",
      "Epoch   4249 Step:      0 Loss:   0.2228 Training Acc:    90.56    90.56    90.56%\n",
      "Epoch   4250 Step:      0 Loss:   0.2225 Training Acc:    90.57    90.57    90.57%\n",
      "Epoch   4251 Step:      0 Loss:   0.2226 Training Acc:    90.54    90.54    90.54%\n",
      "Epoch   4252 Step:      0 Loss:   0.2227 Training Acc:    90.56    90.56    90.56%\n",
      "Epoch   4253 Step:      0 Loss:   0.2225 Training Acc:    90.53    90.53    90.53%\n",
      "Epoch   4254 Step:      0 Loss:   0.2224 Training Acc:    90.57    90.57    90.57%\n",
      "Epoch   4255 Step:      0 Loss:   0.2224 Training Acc:    90.57    90.57    90.57%\n",
      "Epoch   4256 Step:      0 Loss:   0.2224 Training Acc:    90.55    90.55    90.55%\n",
      "Epoch   4257 Step:      0 Loss:   0.2224 Training Acc:    90.59    90.59    90.59%\n",
      "Epoch   4258 Step:      0 Loss:   0.2222 Training Acc:    90.58    90.58    90.58%\n",
      "Epoch   4259 Step:      0 Loss:   0.2221 Training Acc:    90.59    90.59    90.59%\n",
      "Epoch   4260 Step:      0 Loss:   0.2221 Training Acc:    90.60    90.60    90.60%\n",
      "Epoch   4261 Step:      0 Loss:   0.2221 Training Acc:    90.57    90.57    90.57%\n",
      "Epoch   4262 Step:      0 Loss:   0.2221 Training Acc:    90.60    90.60    90.60%\n",
      "Epoch   4263 Step:      0 Loss:   0.2220 Training Acc:    90.59    90.59    90.59%\n",
      "Epoch   4264 Step:      0 Loss:   0.2219 Training Acc:    90.59    90.59    90.59%\n",
      "Epoch   4265 Step:      0 Loss:   0.2219 Training Acc:    90.60    90.60    90.60%\n",
      "Epoch   4266 Step:      0 Loss:   0.2219 Training Acc:    90.59    90.59    90.59%\n",
      "Epoch   4267 Step:      0 Loss:   0.2218 Training Acc:    90.61    90.61    90.61%\n",
      "Epoch   4268 Step:      0 Loss:   0.2218 Training Acc:    90.60    90.60    90.60%\n",
      "Epoch   4269 Step:      0 Loss:   0.2217 Training Acc:    90.61    90.61    90.61%\n",
      "Epoch   4270 Step:      0 Loss:   0.2217 Training Acc:    90.61    90.61    90.61%\n",
      "Epoch   4271 Step:      0 Loss:   0.2217 Training Acc:    90.60    90.60    90.60%\n",
      "Epoch   4272 Step:      0 Loss:   0.2216 Training Acc:    90.62    90.62    90.62%\n",
      "Epoch   4273 Step:      0 Loss:   0.2216 Training Acc:    90.61    90.61    90.61%\n",
      "Epoch   4274 Step:      0 Loss:   0.2215 Training Acc:    90.62    90.62    90.62%\n",
      "Epoch   4275 Step:      0 Loss:   0.2215 Training Acc:    90.64    90.64    90.64%\n",
      "Epoch   4276 Step:      0 Loss:   0.2215 Training Acc:    90.61    90.61    90.61%\n",
      "Epoch   4277 Step:      0 Loss:   0.2214 Training Acc:    90.63    90.63    90.63%\n",
      "Epoch   4278 Step:      0 Loss:   0.2214 Training Acc:    90.64    90.64    90.64%\n",
      "Epoch   4279 Step:      0 Loss:   0.2213 Training Acc:    90.63    90.63    90.63%\n",
      "Epoch   4280 Step:      0 Loss:   0.2213 Training Acc:    90.64    90.64    90.64%\n",
      "Epoch   4281 Step:      0 Loss:   0.2213 Training Acc:    90.63    90.63    90.63%\n",
      "Epoch   4282 Step:      0 Loss:   0.2212 Training Acc:    90.64    90.64    90.64%\n",
      "Epoch   4283 Step:      0 Loss:   0.2212 Training Acc:    90.64    90.64    90.64%\n",
      "Epoch   4284 Step:      0 Loss:   0.2211 Training Acc:    90.64    90.64    90.64%\n",
      "Epoch   4285 Step:      0 Loss:   0.2211 Training Acc:    90.65    90.65    90.65%\n",
      "Epoch   4286 Step:      0 Loss:   0.2211 Training Acc:    90.64    90.64    90.64%\n",
      "Epoch   4287 Step:      0 Loss:   0.2210 Training Acc:    90.65    90.65    90.65%\n",
      "Epoch   4288 Step:      0 Loss:   0.2210 Training Acc:    90.65    90.65    90.65%\n",
      "Epoch   4289 Step:      0 Loss:   0.2209 Training Acc:    90.66    90.66    90.66%\n",
      "Epoch   4290 Step:      0 Loss:   0.2209 Training Acc:    90.66    90.66    90.66%\n",
      "Epoch   4291 Step:      0 Loss:   0.2208 Training Acc:    90.65    90.65    90.65%\n",
      "Epoch   4292 Step:      0 Loss:   0.2208 Training Acc:    90.67    90.67    90.67%\n",
      "Epoch   4293 Step:      0 Loss:   0.2208 Training Acc:    90.66    90.66    90.66%\n",
      "Epoch   4294 Step:      0 Loss:   0.2207 Training Acc:    90.67    90.67    90.67%\n",
      "Epoch   4295 Step:      0 Loss:   0.2207 Training Acc:    90.67    90.67    90.67%\n",
      "Epoch   4296 Step:      0 Loss:   0.2206 Training Acc:    90.66    90.66    90.66%\n",
      "Epoch   4297 Step:      0 Loss:   0.2206 Training Acc:    90.67    90.67    90.67%\n",
      "Epoch   4298 Step:      0 Loss:   0.2206 Training Acc:    90.68    90.68    90.68%\n",
      "Epoch   4299 Step:      0 Loss:   0.2205 Training Acc:    90.68    90.68    90.68%\n",
      "Epoch   4300 Step:      0 Loss:   0.2205 Training Acc:    90.68    90.68    90.68%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4301 Step:      0 Loss:   0.2204 Training Acc:    90.68    90.68    90.68%\n",
      "Epoch   4302 Step:      0 Loss:   0.2204 Training Acc:    90.68    90.68    90.68%\n",
      "Epoch   4303 Step:      0 Loss:   0.2204 Training Acc:    90.68    90.68    90.68%\n",
      "Epoch   4304 Step:      0 Loss:   0.2203 Training Acc:    90.68    90.68    90.68%\n",
      "Epoch   4305 Step:      0 Loss:   0.2203 Training Acc:    90.69    90.69    90.69%\n",
      "Epoch   4306 Step:      0 Loss:   0.2202 Training Acc:    90.69    90.69    90.69%\n",
      "Epoch   4307 Step:      0 Loss:   0.2202 Training Acc:    90.69    90.69    90.69%\n",
      "Epoch   4308 Step:      0 Loss:   0.2202 Training Acc:    90.69    90.69    90.69%\n",
      "Epoch   4309 Step:      0 Loss:   0.2201 Training Acc:    90.70    90.70    90.70%\n",
      "Epoch   4310 Step:      0 Loss:   0.2201 Training Acc:    90.69    90.69    90.69%\n",
      "Epoch   4311 Step:      0 Loss:   0.2200 Training Acc:    90.70    90.70    90.70%\n",
      "Epoch   4312 Step:      0 Loss:   0.2200 Training Acc:    90.70    90.70    90.70%\n",
      "Epoch   4313 Step:      0 Loss:   0.2200 Training Acc:    90.70    90.70    90.70%\n",
      "Epoch   4314 Step:      0 Loss:   0.2199 Training Acc:    90.71    90.71    90.71%\n",
      "Epoch   4315 Step:      0 Loss:   0.2199 Training Acc:    90.70    90.70    90.70%\n",
      "Epoch   4316 Step:      0 Loss:   0.2198 Training Acc:    90.71    90.71    90.71%\n",
      "Epoch   4317 Step:      0 Loss:   0.2198 Training Acc:    90.71    90.71    90.71%\n",
      "Epoch   4318 Step:      0 Loss:   0.2198 Training Acc:    90.71    90.71    90.71%\n",
      "Epoch   4319 Step:      0 Loss:   0.2197 Training Acc:    90.72    90.72    90.72%\n",
      "Epoch   4320 Step:      0 Loss:   0.2197 Training Acc:    90.71    90.71    90.71%\n",
      "Epoch   4321 Step:      0 Loss:   0.2196 Training Acc:    90.72    90.72    90.72%\n",
      "Epoch   4322 Step:      0 Loss:   0.2196 Training Acc:    90.72    90.72    90.72%\n",
      "Epoch   4323 Step:      0 Loss:   0.2196 Training Acc:    90.72    90.72    90.72%\n",
      "Epoch   4324 Step:      0 Loss:   0.2195 Training Acc:    90.72    90.72    90.72%\n",
      "Epoch   4325 Step:      0 Loss:   0.2195 Training Acc:    90.72    90.72    90.72%\n",
      "Epoch   4326 Step:      0 Loss:   0.2194 Training Acc:    90.73    90.73    90.73%\n",
      "Epoch   4327 Step:      0 Loss:   0.2194 Training Acc:    90.73    90.73    90.73%\n",
      "Epoch   4328 Step:      0 Loss:   0.2194 Training Acc:    90.73    90.73    90.73%\n",
      "Epoch   4329 Step:      0 Loss:   0.2193 Training Acc:    90.73    90.73    90.73%\n",
      "Epoch   4330 Step:      0 Loss:   0.2193 Training Acc:    90.73    90.73    90.73%\n",
      "Epoch   4331 Step:      0 Loss:   0.2192 Training Acc:    90.74    90.74    90.74%\n",
      "Epoch   4332 Step:      0 Loss:   0.2192 Training Acc:    90.74    90.74    90.74%\n",
      "Epoch   4333 Step:      0 Loss:   0.2192 Training Acc:    90.74    90.74    90.74%\n",
      "Epoch   4334 Step:      0 Loss:   0.2191 Training Acc:    90.74    90.74    90.74%\n",
      "Epoch   4335 Step:      0 Loss:   0.2191 Training Acc:    90.74    90.74    90.74%\n",
      "Epoch   4336 Step:      0 Loss:   0.2190 Training Acc:    90.75    90.75    90.75%\n",
      "Epoch   4337 Step:      0 Loss:   0.2190 Training Acc:    90.75    90.75    90.75%\n",
      "Epoch   4338 Step:      0 Loss:   0.2190 Training Acc:    90.75    90.75    90.75%\n",
      "Epoch   4339 Step:      0 Loss:   0.2189 Training Acc:    90.76    90.76    90.76%\n",
      "Epoch   4340 Step:      0 Loss:   0.2189 Training Acc:    90.76    90.76    90.76%\n",
      "Epoch   4341 Step:      0 Loss:   0.2188 Training Acc:    90.76    90.76    90.76%\n",
      "Epoch   4342 Step:      0 Loss:   0.2188 Training Acc:    90.76    90.76    90.76%\n",
      "Epoch   4343 Step:      0 Loss:   0.2188 Training Acc:    90.77    90.77    90.77%\n",
      "Epoch   4344 Step:      0 Loss:   0.2187 Training Acc:    90.77    90.77    90.77%\n",
      "Epoch   4345 Step:      0 Loss:   0.2187 Training Acc:    90.77    90.77    90.77%\n",
      "Epoch   4346 Step:      0 Loss:   0.2186 Training Acc:    90.77    90.77    90.77%\n",
      "Epoch   4347 Step:      0 Loss:   0.2186 Training Acc:    90.78    90.78    90.78%\n",
      "Epoch   4348 Step:      0 Loss:   0.2186 Training Acc:    90.78    90.78    90.78%\n",
      "Epoch   4349 Step:      0 Loss:   0.2185 Training Acc:    90.79    90.79    90.79%\n",
      "Epoch   4350 Step:      0 Loss:   0.2185 Training Acc:    90.78    90.78    90.78%\n",
      "Epoch   4351 Step:      0 Loss:   0.2184 Training Acc:    90.79    90.79    90.79%\n",
      "Epoch   4352 Step:      0 Loss:   0.2184 Training Acc:    90.79    90.79    90.79%\n",
      "Epoch   4353 Step:      0 Loss:   0.2184 Training Acc:    90.79    90.79    90.79%\n",
      "Epoch   4354 Step:      0 Loss:   0.2183 Training Acc:    90.80    90.80    90.80%\n",
      "Epoch   4355 Step:      0 Loss:   0.2183 Training Acc:    90.80    90.80    90.80%\n",
      "Epoch   4356 Step:      0 Loss:   0.2182 Training Acc:    90.80    90.80    90.80%\n",
      "Epoch   4357 Step:      0 Loss:   0.2182 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4358 Step:      0 Loss:   0.2182 Training Acc:    90.80    90.80    90.80%\n",
      "Epoch   4359 Step:      0 Loss:   0.2181 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4360 Step:      0 Loss:   0.2181 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4361 Step:      0 Loss:   0.2180 Training Acc:    90.82    90.82    90.82%\n",
      "Epoch   4362 Step:      0 Loss:   0.2180 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4363 Step:      0 Loss:   0.2180 Training Acc:    90.82    90.82    90.82%\n",
      "Epoch   4364 Step:      0 Loss:   0.2180 Training Acc:    90.82    90.82    90.82%\n",
      "Epoch   4365 Step:      0 Loss:   0.2179 Training Acc:    90.82    90.82    90.82%\n",
      "Epoch   4366 Step:      0 Loss:   0.2179 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4367 Step:      0 Loss:   0.2179 Training Acc:    90.82    90.82    90.82%\n",
      "Epoch   4368 Step:      0 Loss:   0.2180 Training Acc:    90.79    90.79    90.79%\n",
      "Epoch   4369 Step:      0 Loss:   0.2180 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4370 Step:      0 Loss:   0.2182 Training Acc:    90.76    90.76    90.76%\n",
      "Epoch   4371 Step:      0 Loss:   0.2185 Training Acc:    90.77    90.77    90.77%\n",
      "Epoch   4372 Step:      0 Loss:   0.2191 Training Acc:    90.67    90.67    90.67%\n",
      "Epoch   4373 Step:      0 Loss:   0.2199 Training Acc:    90.71    90.71    90.71%\n",
      "Epoch   4374 Step:      0 Loss:   0.2214 Training Acc:    90.47    90.47    90.47%\n",
      "Epoch   4375 Step:      0 Loss:   0.2234 Training Acc:    90.47    90.47    90.47%\n",
      "Epoch   4376 Step:      0 Loss:   0.2266 Training Acc:    90.11    90.11    90.11%\n",
      "Epoch   4377 Step:      0 Loss:   0.2292 Training Acc:    90.07    90.07    90.07%\n",
      "Epoch   4378 Step:      0 Loss:   0.2312 Training Acc:    89.82    89.82    89.82%\n",
      "Epoch   4379 Step:      0 Loss:   0.2288 Training Acc:    90.11    90.11    90.11%\n",
      "Epoch   4380 Step:      0 Loss:   0.2239 Training Acc:    90.27    90.27    90.27%\n",
      "Epoch   4381 Step:      0 Loss:   0.2188 Training Acc:    90.76    90.76    90.76%\n",
      "Epoch   4382 Step:      0 Loss:   0.2181 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4383 Step:      0 Loss:   0.2209 Training Acc:    90.49    90.49    90.49%\n",
      "Epoch   4384 Step:      0 Loss:   0.2232 Training Acc:    90.48    90.48    90.48%\n",
      "Epoch   4385 Step:      0 Loss:   0.2226 Training Acc:    90.37    90.37    90.37%\n",
      "Epoch   4386 Step:      0 Loss:   0.2194 Training Acc:    90.71    90.71    90.71%\n",
      "Epoch   4387 Step:      0 Loss:   0.2175 Training Acc:    90.82    90.82    90.82%\n",
      "Epoch   4388 Step:      0 Loss:   0.2185 Training Acc:    90.70    90.70    90.70%\n",
      "Epoch   4389 Step:      0 Loss:   0.2202 Training Acc:    90.69    90.69    90.69%\n",
      "Epoch   4390 Step:      0 Loss:   0.2205 Training Acc:    90.53    90.53    90.53%\n",
      "Epoch   4391 Step:      0 Loss:   0.2187 Training Acc:    90.76    90.76    90.76%\n",
      "Epoch   4392 Step:      0 Loss:   0.2171 Training Acc:    90.85    90.85    90.85%\n",
      "Epoch   4393 Step:      0 Loss:   0.2173 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4394 Step:      0 Loss:   0.2185 Training Acc:    90.80    90.80    90.80%\n",
      "Epoch   4395 Step:      0 Loss:   0.2191 Training Acc:    90.63    90.63    90.63%\n",
      "Epoch   4396 Step:      0 Loss:   0.2182 Training Acc:    90.80    90.80    90.80%\n",
      "Epoch   4397 Step:      0 Loss:   0.2170 Training Acc:    90.83    90.83    90.83%\n",
      "Epoch   4398 Step:      0 Loss:   0.2167 Training Acc:    90.86    90.86    90.86%\n",
      "Epoch   4399 Step:      0 Loss:   0.2173 Training Acc:    90.84    90.84    90.84%\n",
      "Epoch   4400 Step:      0 Loss:   0.2179 Training Acc:    90.72    90.72    90.72%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4401 Step:      0 Loss:   0.2176 Training Acc:    90.83    90.83    90.83%\n",
      "Epoch   4402 Step:      0 Loss:   0.2169 Training Acc:    90.84    90.84    90.84%\n",
      "Epoch   4403 Step:      0 Loss:   0.2165 Training Acc:    90.89    90.89    90.89%\n",
      "Epoch   4404 Step:      0 Loss:   0.2167 Training Acc:    90.88    90.88    90.88%\n",
      "Epoch   4405 Step:      0 Loss:   0.2171 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4406 Step:      0 Loss:   0.2170 Training Acc:    90.85    90.85    90.85%\n",
      "Epoch   4407 Step:      0 Loss:   0.2166 Training Acc:    90.85    90.85    90.85%\n",
      "Epoch   4408 Step:      0 Loss:   0.2163 Training Acc:    90.90    90.90    90.90%\n",
      "Epoch   4409 Step:      0 Loss:   0.2163 Training Acc:    90.89    90.89    90.89%\n",
      "Epoch   4410 Step:      0 Loss:   0.2165 Training Acc:    90.87    90.87    90.87%\n",
      "Epoch   4411 Step:      0 Loss:   0.2166 Training Acc:    90.88    90.88    90.88%\n",
      "Epoch   4412 Step:      0 Loss:   0.2164 Training Acc:    90.86    90.86    90.86%\n",
      "Epoch   4413 Step:      0 Loss:   0.2161 Training Acc:    90.91    90.91    90.91%\n",
      "Epoch   4414 Step:      0 Loss:   0.2160 Training Acc:    90.91    90.91    90.91%\n",
      "Epoch   4415 Step:      0 Loss:   0.2161 Training Acc:    90.89    90.89    90.89%\n",
      "Epoch   4416 Step:      0 Loss:   0.2162 Training Acc:    90.92    90.92    90.92%\n",
      "Epoch   4417 Step:      0 Loss:   0.2161 Training Acc:    90.89    90.89    90.89%\n",
      "Epoch   4418 Step:      0 Loss:   0.2160 Training Acc:    90.92    90.92    90.92%\n",
      "Epoch   4419 Step:      0 Loss:   0.2158 Training Acc:    90.91    90.91    90.91%\n",
      "Epoch   4420 Step:      0 Loss:   0.2158 Training Acc:    90.90    90.90    90.90%\n",
      "Epoch   4421 Step:      0 Loss:   0.2158 Training Acc:    90.93    90.93    90.93%\n",
      "Epoch   4422 Step:      0 Loss:   0.2158 Training Acc:    90.91    90.91    90.91%\n",
      "Epoch   4423 Step:      0 Loss:   0.2158 Training Acc:    90.93    90.93    90.93%\n",
      "Epoch   4424 Step:      0 Loss:   0.2157 Training Acc:    90.92    90.92    90.92%\n",
      "Epoch   4425 Step:      0 Loss:   0.2156 Training Acc:    90.93    90.93    90.93%\n",
      "Epoch   4426 Step:      0 Loss:   0.2155 Training Acc:    90.93    90.93    90.93%\n",
      "Epoch   4427 Step:      0 Loss:   0.2155 Training Acc:    90.92    90.92    90.92%\n",
      "Epoch   4428 Step:      0 Loss:   0.2155 Training Acc:    90.94    90.94    90.94%\n",
      "Epoch   4429 Step:      0 Loss:   0.2155 Training Acc:    90.93    90.93    90.93%\n",
      "Epoch   4430 Step:      0 Loss:   0.2154 Training Acc:    90.95    90.95    90.95%\n",
      "Epoch   4431 Step:      0 Loss:   0.2153 Training Acc:    90.95    90.95    90.95%\n",
      "Epoch   4432 Step:      0 Loss:   0.2153 Training Acc:    90.95    90.95    90.95%\n",
      "Epoch   4433 Step:      0 Loss:   0.2153 Training Acc:    90.95    90.95    90.95%\n",
      "Epoch   4434 Step:      0 Loss:   0.2153 Training Acc:    90.94    90.94    90.94%\n",
      "Epoch   4435 Step:      0 Loss:   0.2152 Training Acc:    90.95    90.95    90.95%\n",
      "Epoch   4436 Step:      0 Loss:   0.2152 Training Acc:    90.95    90.95    90.95%\n",
      "Epoch   4437 Step:      0 Loss:   0.2151 Training Acc:    90.96    90.96    90.96%\n",
      "Epoch   4438 Step:      0 Loss:   0.2151 Training Acc:    90.96    90.96    90.96%\n",
      "Epoch   4439 Step:      0 Loss:   0.2150 Training Acc:    90.96    90.96    90.96%\n",
      "Epoch   4440 Step:      0 Loss:   0.2150 Training Acc:    90.96    90.96    90.96%\n",
      "Epoch   4441 Step:      0 Loss:   0.2150 Training Acc:    90.96    90.96    90.96%\n",
      "Epoch   4442 Step:      0 Loss:   0.2149 Training Acc:    90.97    90.97    90.97%\n",
      "Epoch   4443 Step:      0 Loss:   0.2149 Training Acc:    90.98    90.98    90.98%\n",
      "Epoch   4444 Step:      0 Loss:   0.2148 Training Acc:    90.98    90.98    90.98%\n",
      "Epoch   4445 Step:      0 Loss:   0.2148 Training Acc:    90.98    90.98    90.98%\n",
      "Epoch   4446 Step:      0 Loss:   0.2148 Training Acc:    90.97    90.97    90.97%\n",
      "Epoch   4447 Step:      0 Loss:   0.2147 Training Acc:    90.98    90.98    90.98%\n",
      "Epoch   4448 Step:      0 Loss:   0.2147 Training Acc:    90.98    90.98    90.98%\n",
      "Epoch   4449 Step:      0 Loss:   0.2146 Training Acc:    90.98    90.98    90.98%\n",
      "Epoch   4450 Step:      0 Loss:   0.2146 Training Acc:    90.99    90.99    90.99%\n",
      "Epoch   4451 Step:      0 Loss:   0.2146 Training Acc:    90.99    90.99    90.99%\n",
      "Epoch   4452 Step:      0 Loss:   0.2145 Training Acc:    91.00    91.00    91.00%\n",
      "Epoch   4453 Step:      0 Loss:   0.2145 Training Acc:    91.00    91.00    91.00%\n",
      "Epoch   4454 Step:      0 Loss:   0.2144 Training Acc:    91.00    91.00    91.00%\n",
      "Epoch   4455 Step:      0 Loss:   0.2144 Training Acc:    91.00    91.00    91.00%\n",
      "Epoch   4456 Step:      0 Loss:   0.2144 Training Acc:    91.01    91.01    91.01%\n",
      "Epoch   4457 Step:      0 Loss:   0.2143 Training Acc:    91.01    91.01    91.01%\n",
      "Epoch   4458 Step:      0 Loss:   0.2143 Training Acc:    91.02    91.02    91.02%\n",
      "Epoch   4459 Step:      0 Loss:   0.2142 Training Acc:    91.01    91.01    91.01%\n",
      "Epoch   4460 Step:      0 Loss:   0.2142 Training Acc:    91.02    91.02    91.02%\n",
      "Epoch   4461 Step:      0 Loss:   0.2142 Training Acc:    91.01    91.01    91.01%\n",
      "Epoch   4462 Step:      0 Loss:   0.2141 Training Acc:    91.02    91.02    91.02%\n",
      "Epoch   4463 Step:      0 Loss:   0.2141 Training Acc:    91.02    91.02    91.02%\n",
      "Epoch   4464 Step:      0 Loss:   0.2140 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4465 Step:      0 Loss:   0.2140 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4466 Step:      0 Loss:   0.2140 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4467 Step:      0 Loss:   0.2139 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4468 Step:      0 Loss:   0.2139 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4469 Step:      0 Loss:   0.2139 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4470 Step:      0 Loss:   0.2138 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4471 Step:      0 Loss:   0.2138 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4472 Step:      0 Loss:   0.2137 Training Acc:    91.04    91.04    91.04%\n",
      "Epoch   4473 Step:      0 Loss:   0.2137 Training Acc:    91.04    91.04    91.04%\n",
      "Epoch   4474 Step:      0 Loss:   0.2137 Training Acc:    91.04    91.04    91.04%\n",
      "Epoch   4475 Step:      0 Loss:   0.2136 Training Acc:    91.04    91.04    91.04%\n",
      "Epoch   4476 Step:      0 Loss:   0.2136 Training Acc:    91.04    91.04    91.04%\n",
      "Epoch   4477 Step:      0 Loss:   0.2135 Training Acc:    91.04    91.04    91.04%\n",
      "Epoch   4478 Step:      0 Loss:   0.2135 Training Acc:    91.05    91.05    91.05%\n",
      "Epoch   4479 Step:      0 Loss:   0.2135 Training Acc:    91.05    91.05    91.05%\n",
      "Epoch   4480 Step:      0 Loss:   0.2134 Training Acc:    91.05    91.05    91.05%\n",
      "Epoch   4481 Step:      0 Loss:   0.2134 Training Acc:    91.05    91.05    91.05%\n",
      "Epoch   4482 Step:      0 Loss:   0.2133 Training Acc:    91.06    91.06    91.06%\n",
      "Epoch   4483 Step:      0 Loss:   0.2133 Training Acc:    91.05    91.05    91.05%\n",
      "Epoch   4484 Step:      0 Loss:   0.2133 Training Acc:    91.06    91.06    91.06%\n",
      "Epoch   4485 Step:      0 Loss:   0.2132 Training Acc:    91.05    91.05    91.05%\n",
      "Epoch   4486 Step:      0 Loss:   0.2132 Training Acc:    91.07    91.07    91.07%\n",
      "Epoch   4487 Step:      0 Loss:   0.2132 Training Acc:    91.06    91.06    91.06%\n",
      "Epoch   4488 Step:      0 Loss:   0.2131 Training Acc:    91.07    91.07    91.07%\n",
      "Epoch   4489 Step:      0 Loss:   0.2131 Training Acc:    91.06    91.06    91.06%\n",
      "Epoch   4490 Step:      0 Loss:   0.2131 Training Acc:    91.08    91.08    91.08%\n",
      "Epoch   4491 Step:      0 Loss:   0.2131 Training Acc:    91.06    91.06    91.06%\n",
      "Epoch   4492 Step:      0 Loss:   0.2131 Training Acc:    91.08    91.08    91.08%\n",
      "Epoch   4493 Step:      0 Loss:   0.2132 Training Acc:    91.05    91.05    91.05%\n",
      "Epoch   4494 Step:      0 Loss:   0.2134 Training Acc:    91.05    91.05    91.05%\n",
      "Epoch   4495 Step:      0 Loss:   0.2136 Training Acc:    91.01    91.01    91.01%\n",
      "Epoch   4496 Step:      0 Loss:   0.2141 Training Acc:    91.00    91.00    91.00%\n",
      "Epoch   4497 Step:      0 Loss:   0.2148 Training Acc:    90.92    90.92    90.92%\n",
      "Epoch   4498 Step:      0 Loss:   0.2162 Training Acc:    90.82    90.82    90.82%\n",
      "Epoch   4499 Step:      0 Loss:   0.2180 Training Acc:    90.69    90.69    90.69%\n",
      "Epoch   4500 Step:      0 Loss:   0.2213 Training Acc:    90.42    90.42    90.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4501 Step:      0 Loss:   0.2249 Training Acc:    90.23    90.23    90.23%\n",
      "Epoch   4502 Step:      0 Loss:   0.2310 Training Acc:    89.77    89.77    89.77%\n",
      "Epoch   4503 Step:      0 Loss:   0.2346 Training Acc:    89.59    89.59    89.59%\n",
      "Epoch   4504 Step:      0 Loss:   0.2390 Training Acc:    89.29    89.29    89.29%\n",
      "Epoch   4505 Step:      0 Loss:   0.2342 Training Acc:    89.64    89.64    89.64%\n",
      "Epoch   4506 Step:      0 Loss:   0.2251 Training Acc:    90.20    90.20    90.20%\n",
      "Epoch   4507 Step:      0 Loss:   0.2142 Training Acc:    90.98    90.98    90.98%\n",
      "Epoch   4508 Step:      0 Loss:   0.2162 Training Acc:    90.83    90.83    90.83%\n",
      "Epoch   4509 Step:      0 Loss:   0.2240 Training Acc:    90.27    90.27    90.27%\n",
      "Epoch   4510 Step:      0 Loss:   0.2234 Training Acc:    90.34    90.34    90.34%\n",
      "Epoch   4511 Step:      0 Loss:   0.2169 Training Acc:    90.74    90.74    90.74%\n",
      "Epoch   4512 Step:      0 Loss:   0.2145 Training Acc:    90.97    90.97    90.97%\n",
      "Epoch   4513 Step:      0 Loss:   0.2180 Training Acc:    90.66    90.66    90.66%\n",
      "Epoch   4514 Step:      0 Loss:   0.2195 Training Acc:    90.58    90.58    90.58%\n",
      "Epoch   4515 Step:      0 Loss:   0.2161 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4516 Step:      0 Loss:   0.2143 Training Acc:    90.90    90.90    90.90%\n",
      "Epoch   4517 Step:      0 Loss:   0.2155 Training Acc:    90.88    90.88    90.88%\n",
      "Epoch   4518 Step:      0 Loss:   0.2162 Training Acc:    90.76    90.76    90.76%\n",
      "Epoch   4519 Step:      0 Loss:   0.2155 Training Acc:    90.88    90.88    90.88%\n",
      "Epoch   4520 Step:      0 Loss:   0.2137 Training Acc:    91.00    91.00    91.00%\n",
      "Epoch   4521 Step:      0 Loss:   0.2131 Training Acc:    91.02    91.02    91.02%\n",
      "Epoch   4522 Step:      0 Loss:   0.2148 Training Acc:    90.93    90.93    90.93%\n",
      "Epoch   4523 Step:      0 Loss:   0.2147 Training Acc:    90.89    90.89    90.89%\n",
      "Epoch   4524 Step:      0 Loss:   0.2127 Training Acc:    91.08    91.08    91.08%\n",
      "Epoch   4525 Step:      0 Loss:   0.2125 Training Acc:    91.04    91.04    91.04%\n",
      "Epoch   4526 Step:      0 Loss:   0.2136 Training Acc:    90.97    90.97    90.97%\n",
      "Epoch   4527 Step:      0 Loss:   0.2132 Training Acc:    91.05    91.05    91.05%\n",
      "Epoch   4528 Step:      0 Loss:   0.2125 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4529 Step:      0 Loss:   0.2123 Training Acc:    91.09    91.09    91.09%\n",
      "Epoch   4530 Step:      0 Loss:   0.2124 Training Acc:    91.09    91.09    91.09%\n",
      "Epoch   4531 Step:      0 Loss:   0.2128 Training Acc:    91.02    91.02    91.02%\n",
      "Epoch   4532 Step:      0 Loss:   0.2123 Training Acc:    91.12    91.12    91.12%\n",
      "Epoch   4533 Step:      0 Loss:   0.2117 Training Acc:    91.12    91.12    91.12%\n",
      "Epoch   4534 Step:      0 Loss:   0.2120 Training Acc:    91.08    91.08    91.08%\n",
      "Epoch   4535 Step:      0 Loss:   0.2122 Training Acc:    91.11    91.11    91.11%\n",
      "Epoch   4536 Step:      0 Loss:   0.2119 Training Acc:    91.10    91.10    91.10%\n",
      "Epoch   4537 Step:      0 Loss:   0.2116 Training Acc:    91.11    91.11    91.11%\n",
      "Epoch   4538 Step:      0 Loss:   0.2115 Training Acc:    91.16    91.16    91.16%\n",
      "Epoch   4539 Step:      0 Loss:   0.2117 Training Acc:    91.11    91.11    91.11%\n",
      "Epoch   4540 Step:      0 Loss:   0.2116 Training Acc:    91.13    91.13    91.13%\n",
      "Epoch   4541 Step:      0 Loss:   0.2114 Training Acc:    91.14    91.14    91.14%\n",
      "Epoch   4542 Step:      0 Loss:   0.2113 Training Acc:    91.15    91.15    91.15%\n",
      "Epoch   4543 Step:      0 Loss:   0.2113 Training Acc:    91.16    91.16    91.16%\n",
      "Epoch   4544 Step:      0 Loss:   0.2114 Training Acc:    91.11    91.11    91.11%\n",
      "Epoch   4545 Step:      0 Loss:   0.2113 Training Acc:    91.15    91.15    91.15%\n",
      "Epoch   4546 Step:      0 Loss:   0.2110 Training Acc:    91.16    91.16    91.16%\n",
      "Epoch   4547 Step:      0 Loss:   0.2111 Training Acc:    91.15    91.15    91.15%\n",
      "Epoch   4548 Step:      0 Loss:   0.2111 Training Acc:    91.16    91.16    91.16%\n",
      "Epoch   4549 Step:      0 Loss:   0.2110 Training Acc:    91.15    91.15    91.15%\n",
      "Epoch   4550 Step:      0 Loss:   0.2110 Training Acc:    91.18    91.18    91.18%\n",
      "Epoch   4551 Step:      0 Loss:   0.2109 Training Acc:    91.17    91.17    91.17%\n",
      "Epoch   4552 Step:      0 Loss:   0.2109 Training Acc:    91.17    91.17    91.17%\n",
      "Epoch   4553 Step:      0 Loss:   0.2109 Training Acc:    91.19    91.19    91.19%\n",
      "Epoch   4554 Step:      0 Loss:   0.2108 Training Acc:    91.17    91.17    91.17%\n",
      "Epoch   4555 Step:      0 Loss:   0.2107 Training Acc:    91.19    91.19    91.19%\n",
      "Epoch   4556 Step:      0 Loss:   0.2107 Training Acc:    91.19    91.19    91.19%\n",
      "Epoch   4557 Step:      0 Loss:   0.2107 Training Acc:    91.18    91.18    91.18%\n",
      "Epoch   4558 Step:      0 Loss:   0.2106 Training Acc:    91.18    91.18    91.18%\n",
      "Epoch   4559 Step:      0 Loss:   0.2106 Training Acc:    91.19    91.19    91.19%\n",
      "Epoch   4560 Step:      0 Loss:   0.2105 Training Acc:    91.19    91.19    91.19%\n",
      "Epoch   4561 Step:      0 Loss:   0.2105 Training Acc:    91.19    91.19    91.19%\n",
      "Epoch   4562 Step:      0 Loss:   0.2105 Training Acc:    91.19    91.19    91.19%\n",
      "Epoch   4563 Step:      0 Loss:   0.2104 Training Acc:    91.19    91.19    91.19%\n",
      "Epoch   4564 Step:      0 Loss:   0.2104 Training Acc:    91.19    91.19    91.19%\n",
      "Epoch   4565 Step:      0 Loss:   0.2103 Training Acc:    91.21    91.21    91.21%\n",
      "Epoch   4566 Step:      0 Loss:   0.2103 Training Acc:    91.19    91.19    91.19%\n",
      "Epoch   4567 Step:      0 Loss:   0.2103 Training Acc:    91.20    91.20    91.20%\n",
      "Epoch   4568 Step:      0 Loss:   0.2102 Training Acc:    91.21    91.21    91.21%\n",
      "Epoch   4569 Step:      0 Loss:   0.2102 Training Acc:    91.21    91.21    91.21%\n",
      "Epoch   4570 Step:      0 Loss:   0.2101 Training Acc:    91.21    91.21    91.21%\n",
      "Epoch   4571 Step:      0 Loss:   0.2101 Training Acc:    91.22    91.22    91.22%\n",
      "Epoch   4572 Step:      0 Loss:   0.2101 Training Acc:    91.20    91.20    91.20%\n",
      "Epoch   4573 Step:      0 Loss:   0.2100 Training Acc:    91.22    91.22    91.22%\n",
      "Epoch   4574 Step:      0 Loss:   0.2100 Training Acc:    91.22    91.22    91.22%\n",
      "Epoch   4575 Step:      0 Loss:   0.2099 Training Acc:    91.22    91.22    91.22%\n",
      "Epoch   4576 Step:      0 Loss:   0.2099 Training Acc:    91.22    91.22    91.22%\n",
      "Epoch   4577 Step:      0 Loss:   0.2099 Training Acc:    91.22    91.22    91.22%\n",
      "Epoch   4578 Step:      0 Loss:   0.2098 Training Acc:    91.22    91.22    91.22%\n",
      "Epoch   4579 Step:      0 Loss:   0.2098 Training Acc:    91.23    91.23    91.23%\n",
      "Epoch   4580 Step:      0 Loss:   0.2098 Training Acc:    91.23    91.23    91.23%\n",
      "Epoch   4581 Step:      0 Loss:   0.2097 Training Acc:    91.23    91.23    91.23%\n",
      "Epoch   4582 Step:      0 Loss:   0.2097 Training Acc:    91.24    91.24    91.24%\n",
      "Epoch   4583 Step:      0 Loss:   0.2096 Training Acc:    91.24    91.24    91.24%\n",
      "Epoch   4584 Step:      0 Loss:   0.2096 Training Acc:    91.24    91.24    91.24%\n",
      "Epoch   4585 Step:      0 Loss:   0.2096 Training Acc:    91.24    91.24    91.24%\n",
      "Epoch   4586 Step:      0 Loss:   0.2095 Training Acc:    91.24    91.24    91.24%\n",
      "Epoch   4587 Step:      0 Loss:   0.2095 Training Acc:    91.25    91.25    91.25%\n",
      "Epoch   4588 Step:      0 Loss:   0.2095 Training Acc:    91.24    91.24    91.24%\n",
      "Epoch   4589 Step:      0 Loss:   0.2094 Training Acc:    91.25    91.25    91.25%\n",
      "Epoch   4590 Step:      0 Loss:   0.2094 Training Acc:    91.25    91.25    91.25%\n",
      "Epoch   4591 Step:      0 Loss:   0.2094 Training Acc:    91.25    91.25    91.25%\n",
      "Epoch   4592 Step:      0 Loss:   0.2093 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4593 Step:      0 Loss:   0.2093 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4594 Step:      0 Loss:   0.2092 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4595 Step:      0 Loss:   0.2092 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4596 Step:      0 Loss:   0.2092 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4597 Step:      0 Loss:   0.2091 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4598 Step:      0 Loss:   0.2091 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4599 Step:      0 Loss:   0.2091 Training Acc:    91.27    91.27    91.27%\n",
      "Epoch   4600 Step:      0 Loss:   0.2090 Training Acc:    91.27    91.27    91.27%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4601 Step:      0 Loss:   0.2090 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4602 Step:      0 Loss:   0.2089 Training Acc:    91.27    91.27    91.27%\n",
      "Epoch   4603 Step:      0 Loss:   0.2089 Training Acc:    91.27    91.27    91.27%\n",
      "Epoch   4604 Step:      0 Loss:   0.2089 Training Acc:    91.28    91.28    91.28%\n",
      "Epoch   4605 Step:      0 Loss:   0.2088 Training Acc:    91.27    91.27    91.27%\n",
      "Epoch   4606 Step:      0 Loss:   0.2088 Training Acc:    91.28    91.28    91.28%\n",
      "Epoch   4607 Step:      0 Loss:   0.2088 Training Acc:    91.28    91.28    91.28%\n",
      "Epoch   4608 Step:      0 Loss:   0.2087 Training Acc:    91.28    91.28    91.28%\n",
      "Epoch   4609 Step:      0 Loss:   0.2087 Training Acc:    91.28    91.28    91.28%\n",
      "Epoch   4610 Step:      0 Loss:   0.2087 Training Acc:    91.28    91.28    91.28%\n",
      "Epoch   4611 Step:      0 Loss:   0.2086 Training Acc:    91.29    91.29    91.29%\n",
      "Epoch   4612 Step:      0 Loss:   0.2086 Training Acc:    91.29    91.29    91.29%\n",
      "Epoch   4613 Step:      0 Loss:   0.2085 Training Acc:    91.29    91.29    91.29%\n",
      "Epoch   4614 Step:      0 Loss:   0.2085 Training Acc:    91.29    91.29    91.29%\n",
      "Epoch   4615 Step:      0 Loss:   0.2085 Training Acc:    91.29    91.29    91.29%\n",
      "Epoch   4616 Step:      0 Loss:   0.2084 Training Acc:    91.30    91.30    91.30%\n",
      "Epoch   4617 Step:      0 Loss:   0.2084 Training Acc:    91.30    91.30    91.30%\n",
      "Epoch   4618 Step:      0 Loss:   0.2084 Training Acc:    91.30    91.30    91.30%\n",
      "Epoch   4619 Step:      0 Loss:   0.2083 Training Acc:    91.30    91.30    91.30%\n",
      "Epoch   4620 Step:      0 Loss:   0.2083 Training Acc:    91.30    91.30    91.30%\n",
      "Epoch   4621 Step:      0 Loss:   0.2082 Training Acc:    91.31    91.31    91.31%\n",
      "Epoch   4622 Step:      0 Loss:   0.2082 Training Acc:    91.31    91.31    91.31%\n",
      "Epoch   4623 Step:      0 Loss:   0.2082 Training Acc:    91.31    91.31    91.31%\n",
      "Epoch   4624 Step:      0 Loss:   0.2081 Training Acc:    91.31    91.31    91.31%\n",
      "Epoch   4625 Step:      0 Loss:   0.2081 Training Acc:    91.31    91.31    91.31%\n",
      "Epoch   4626 Step:      0 Loss:   0.2081 Training Acc:    91.32    91.32    91.32%\n",
      "Epoch   4627 Step:      0 Loss:   0.2080 Training Acc:    91.32    91.32    91.32%\n",
      "Epoch   4628 Step:      0 Loss:   0.2080 Training Acc:    91.32    91.32    91.32%\n",
      "Epoch   4629 Step:      0 Loss:   0.2079 Training Acc:    91.32    91.32    91.32%\n",
      "Epoch   4630 Step:      0 Loss:   0.2079 Training Acc:    91.32    91.32    91.32%\n",
      "Epoch   4631 Step:      0 Loss:   0.2079 Training Acc:    91.33    91.33    91.33%\n",
      "Epoch   4632 Step:      0 Loss:   0.2078 Training Acc:    91.33    91.33    91.33%\n",
      "Epoch   4633 Step:      0 Loss:   0.2078 Training Acc:    91.33    91.33    91.33%\n",
      "Epoch   4634 Step:      0 Loss:   0.2078 Training Acc:    91.33    91.33    91.33%\n",
      "Epoch   4635 Step:      0 Loss:   0.2077 Training Acc:    91.34    91.34    91.34%\n",
      "Epoch   4636 Step:      0 Loss:   0.2077 Training Acc:    91.34    91.34    91.34%\n",
      "Epoch   4637 Step:      0 Loss:   0.2077 Training Acc:    91.34    91.34    91.34%\n",
      "Epoch   4638 Step:      0 Loss:   0.2076 Training Acc:    91.34    91.34    91.34%\n",
      "Epoch   4639 Step:      0 Loss:   0.2076 Training Acc:    91.34    91.34    91.34%\n",
      "Epoch   4640 Step:      0 Loss:   0.2075 Training Acc:    91.35    91.35    91.35%\n",
      "Epoch   4641 Step:      0 Loss:   0.2075 Training Acc:    91.35    91.35    91.35%\n",
      "Epoch   4642 Step:      0 Loss:   0.2075 Training Acc:    91.35    91.35    91.35%\n",
      "Epoch   4643 Step:      0 Loss:   0.2074 Training Acc:    91.35    91.35    91.35%\n",
      "Epoch   4644 Step:      0 Loss:   0.2074 Training Acc:    91.35    91.35    91.35%\n",
      "Epoch   4645 Step:      0 Loss:   0.2074 Training Acc:    91.35    91.35    91.35%\n",
      "Epoch   4646 Step:      0 Loss:   0.2073 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   4647 Step:      0 Loss:   0.2073 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   4648 Step:      0 Loss:   0.2073 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   4649 Step:      0 Loss:   0.2072 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   4650 Step:      0 Loss:   0.2072 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   4651 Step:      0 Loss:   0.2072 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   4652 Step:      0 Loss:   0.2071 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   4653 Step:      0 Loss:   0.2071 Training Acc:    91.37    91.37    91.37%\n",
      "Epoch   4654 Step:      0 Loss:   0.2071 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   4655 Step:      0 Loss:   0.2071 Training Acc:    91.38    91.38    91.38%\n",
      "Epoch   4656 Step:      0 Loss:   0.2071 Training Acc:    91.35    91.35    91.35%\n",
      "Epoch   4657 Step:      0 Loss:   0.2072 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   4658 Step:      0 Loss:   0.2073 Training Acc:    91.33    91.33    91.33%\n",
      "Epoch   4659 Step:      0 Loss:   0.2076 Training Acc:    91.34    91.34    91.34%\n",
      "Epoch   4660 Step:      0 Loss:   0.2080 Training Acc:    91.28    91.28    91.28%\n",
      "Epoch   4661 Step:      0 Loss:   0.2088 Training Acc:    91.25    91.25    91.25%\n",
      "Epoch   4662 Step:      0 Loss:   0.2101 Training Acc:    91.07    91.07    91.07%\n",
      "Epoch   4663 Step:      0 Loss:   0.2119 Training Acc:    91.06    91.06    91.06%\n",
      "Epoch   4664 Step:      0 Loss:   0.2150 Training Acc:    90.71    90.71    90.71%\n",
      "Epoch   4665 Step:      0 Loss:   0.2181 Training Acc:    90.63    90.63    90.63%\n",
      "Epoch   4666 Step:      0 Loss:   0.2217 Training Acc:    90.26    90.26    90.26%\n",
      "Epoch   4667 Step:      0 Loss:   0.2214 Training Acc:    90.43    90.43    90.43%\n",
      "Epoch   4668 Step:      0 Loss:   0.2176 Training Acc:    90.53    90.53    90.53%\n",
      "Epoch   4669 Step:      0 Loss:   0.2106 Training Acc:    91.14    91.14    91.14%\n",
      "Epoch   4670 Step:      0 Loss:   0.2071 Training Acc:    91.35    91.35    91.35%\n",
      "Epoch   4671 Step:      0 Loss:   0.2091 Training Acc:    91.16    91.16    91.16%\n",
      "Epoch   4672 Step:      0 Loss:   0.2125 Training Acc:    91.01    91.01    91.01%\n",
      "Epoch   4673 Step:      0 Loss:   0.2133 Training Acc:    90.81    90.81    90.81%\n",
      "Epoch   4674 Step:      0 Loss:   0.2101 Training Acc:    91.16    91.16    91.16%\n",
      "Epoch   4675 Step:      0 Loss:   0.2073 Training Acc:    91.33    91.33    91.33%\n",
      "Epoch   4676 Step:      0 Loss:   0.2076 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4677 Step:      0 Loss:   0.2096 Training Acc:    91.20    91.20    91.20%\n",
      "Epoch   4678 Step:      0 Loss:   0.2103 Training Acc:    91.06    91.06    91.06%\n",
      "Epoch   4679 Step:      0 Loss:   0.2084 Training Acc:    91.27    91.27    91.27%\n",
      "Epoch   4680 Step:      0 Loss:   0.2067 Training Acc:    91.37    91.37    91.37%\n",
      "Epoch   4681 Step:      0 Loss:   0.2069 Training Acc:    91.33    91.33    91.33%\n",
      "Epoch   4682 Step:      0 Loss:   0.2082 Training Acc:    91.29    91.29    91.29%\n",
      "Epoch   4683 Step:      0 Loss:   0.2086 Training Acc:    91.17    91.17    91.17%\n",
      "Epoch   4684 Step:      0 Loss:   0.2073 Training Acc:    91.33    91.33    91.33%\n",
      "Epoch   4685 Step:      0 Loss:   0.2061 Training Acc:    91.40    91.40    91.40%\n",
      "Epoch   4686 Step:      0 Loss:   0.2063 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   4687 Step:      0 Loss:   0.2072 Training Acc:    91.34    91.34    91.34%\n",
      "Epoch   4688 Step:      0 Loss:   0.2075 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4689 Step:      0 Loss:   0.2067 Training Acc:    91.38    91.38    91.38%\n",
      "Epoch   4690 Step:      0 Loss:   0.2059 Training Acc:    91.39    91.39    91.39%\n",
      "Epoch   4691 Step:      0 Loss:   0.2059 Training Acc:    91.40    91.40    91.40%\n",
      "Epoch   4692 Step:      0 Loss:   0.2064 Training Acc:    91.40    91.40    91.40%\n",
      "Epoch   4693 Step:      0 Loss:   0.2067 Training Acc:    91.33    91.33    91.33%\n",
      "Epoch   4694 Step:      0 Loss:   0.2062 Training Acc:    91.43    91.43    91.43%\n",
      "Epoch   4695 Step:      0 Loss:   0.2057 Training Acc:    91.40    91.40    91.40%\n",
      "Epoch   4696 Step:      0 Loss:   0.2056 Training Acc:    91.42    91.42    91.42%\n",
      "Epoch   4697 Step:      0 Loss:   0.2059 Training Acc:    91.42    91.42    91.42%\n",
      "Epoch   4698 Step:      0 Loss:   0.2060 Training Acc:    91.37    91.37    91.37%\n",
      "Epoch   4699 Step:      0 Loss:   0.2058 Training Acc:    91.43    91.43    91.43%\n",
      "Epoch   4700 Step:      0 Loss:   0.2055 Training Acc:    91.42    91.42    91.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4701 Step:      0 Loss:   0.2054 Training Acc:    91.43    91.43    91.43%\n",
      "Epoch   4702 Step:      0 Loss:   0.2055 Training Acc:    91.44    91.44    91.44%\n",
      "Epoch   4703 Step:      0 Loss:   0.2056 Training Acc:    91.41    91.41    91.41%\n",
      "Epoch   4704 Step:      0 Loss:   0.2055 Training Acc:    91.44    91.44    91.44%\n",
      "Epoch   4705 Step:      0 Loss:   0.2053 Training Acc:    91.43    91.43    91.43%\n",
      "Epoch   4706 Step:      0 Loss:   0.2052 Training Acc:    91.46    91.46    91.46%\n",
      "Epoch   4707 Step:      0 Loss:   0.2052 Training Acc:    91.46    91.46    91.46%\n",
      "Epoch   4708 Step:      0 Loss:   0.2053 Training Acc:    91.43    91.43    91.43%\n",
      "Epoch   4709 Step:      0 Loss:   0.2053 Training Acc:    91.46    91.46    91.46%\n",
      "Epoch   4710 Step:      0 Loss:   0.2051 Training Acc:    91.44    91.44    91.44%\n",
      "Epoch   4711 Step:      0 Loss:   0.2050 Training Acc:    91.47    91.47    91.47%\n",
      "Epoch   4712 Step:      0 Loss:   0.2050 Training Acc:    91.48    91.48    91.48%\n",
      "Epoch   4713 Step:      0 Loss:   0.2050 Training Acc:    91.46    91.46    91.46%\n",
      "Epoch   4714 Step:      0 Loss:   0.2050 Training Acc:    91.48    91.48    91.48%\n",
      "Epoch   4715 Step:      0 Loss:   0.2050 Training Acc:    91.46    91.46    91.46%\n",
      "Epoch   4716 Step:      0 Loss:   0.2049 Training Acc:    91.48    91.48    91.48%\n",
      "Epoch   4717 Step:      0 Loss:   0.2048 Training Acc:    91.49    91.49    91.49%\n",
      "Epoch   4718 Step:      0 Loss:   0.2048 Training Acc:    91.47    91.47    91.47%\n",
      "Epoch   4719 Step:      0 Loss:   0.2048 Training Acc:    91.49    91.49    91.49%\n",
      "Epoch   4720 Step:      0 Loss:   0.2048 Training Acc:    91.47    91.47    91.47%\n",
      "Epoch   4721 Step:      0 Loss:   0.2047 Training Acc:    91.50    91.50    91.50%\n",
      "Epoch   4722 Step:      0 Loss:   0.2046 Training Acc:    91.49    91.49    91.49%\n",
      "Epoch   4723 Step:      0 Loss:   0.2046 Training Acc:    91.49    91.49    91.49%\n",
      "Epoch   4724 Step:      0 Loss:   0.2046 Training Acc:    91.51    91.51    91.51%\n",
      "Epoch   4725 Step:      0 Loss:   0.2045 Training Acc:    91.48    91.48    91.48%\n",
      "Epoch   4726 Step:      0 Loss:   0.2045 Training Acc:    91.51    91.51    91.51%\n",
      "Epoch   4727 Step:      0 Loss:   0.2045 Training Acc:    91.50    91.50    91.50%\n",
      "Epoch   4728 Step:      0 Loss:   0.2044 Training Acc:    91.51    91.51    91.51%\n",
      "Epoch   4729 Step:      0 Loss:   0.2044 Training Acc:    91.51    91.51    91.51%\n",
      "Epoch   4730 Step:      0 Loss:   0.2043 Training Acc:    91.50    91.50    91.50%\n",
      "Epoch   4731 Step:      0 Loss:   0.2043 Training Acc:    91.52    91.52    91.52%\n",
      "Epoch   4732 Step:      0 Loss:   0.2043 Training Acc:    91.50    91.50    91.50%\n",
      "Epoch   4733 Step:      0 Loss:   0.2042 Training Acc:    91.52    91.52    91.52%\n",
      "Epoch   4734 Step:      0 Loss:   0.2042 Training Acc:    91.51    91.51    91.51%\n",
      "Epoch   4735 Step:      0 Loss:   0.2042 Training Acc:    91.51    91.51    91.51%\n",
      "Epoch   4736 Step:      0 Loss:   0.2041 Training Acc:    91.53    91.53    91.53%\n",
      "Epoch   4737 Step:      0 Loss:   0.2041 Training Acc:    91.52    91.52    91.52%\n",
      "Epoch   4738 Step:      0 Loss:   0.2041 Training Acc:    91.53    91.53    91.53%\n",
      "Epoch   4739 Step:      0 Loss:   0.2040 Training Acc:    91.52    91.52    91.52%\n",
      "Epoch   4740 Step:      0 Loss:   0.2040 Training Acc:    91.53    91.53    91.53%\n",
      "Epoch   4741 Step:      0 Loss:   0.2039 Training Acc:    91.53    91.53    91.53%\n",
      "Epoch   4742 Step:      0 Loss:   0.2039 Training Acc:    91.52    91.52    91.52%\n",
      "Epoch   4743 Step:      0 Loss:   0.2039 Training Acc:    91.54    91.54    91.54%\n",
      "Epoch   4744 Step:      0 Loss:   0.2038 Training Acc:    91.53    91.53    91.53%\n",
      "Epoch   4745 Step:      0 Loss:   0.2038 Training Acc:    91.54    91.54    91.54%\n",
      "Epoch   4746 Step:      0 Loss:   0.2038 Training Acc:    91.53    91.53    91.53%\n",
      "Epoch   4747 Step:      0 Loss:   0.2037 Training Acc:    91.54    91.54    91.54%\n",
      "Epoch   4748 Step:      0 Loss:   0.2037 Training Acc:    91.54    91.54    91.54%\n",
      "Epoch   4749 Step:      0 Loss:   0.2037 Training Acc:    91.54    91.54    91.54%\n",
      "Epoch   4750 Step:      0 Loss:   0.2036 Training Acc:    91.55    91.55    91.55%\n",
      "Epoch   4751 Step:      0 Loss:   0.2036 Training Acc:    91.55    91.55    91.55%\n",
      "Epoch   4752 Step:      0 Loss:   0.2035 Training Acc:    91.55    91.55    91.55%\n",
      "Epoch   4753 Step:      0 Loss:   0.2035 Training Acc:    91.55    91.55    91.55%\n",
      "Epoch   4754 Step:      0 Loss:   0.2035 Training Acc:    91.55    91.55    91.55%\n",
      "Epoch   4755 Step:      0 Loss:   0.2034 Training Acc:    91.55    91.55    91.55%\n",
      "Epoch   4756 Step:      0 Loss:   0.2034 Training Acc:    91.55    91.55    91.55%\n",
      "Epoch   4757 Step:      0 Loss:   0.2034 Training Acc:    91.56    91.56    91.56%\n",
      "Epoch   4758 Step:      0 Loss:   0.2033 Training Acc:    91.56    91.56    91.56%\n",
      "Epoch   4759 Step:      0 Loss:   0.2033 Training Acc:    91.56    91.56    91.56%\n",
      "Epoch   4760 Step:      0 Loss:   0.2033 Training Acc:    91.56    91.56    91.56%\n",
      "Epoch   4761 Step:      0 Loss:   0.2032 Training Acc:    91.57    91.57    91.57%\n",
      "Epoch   4762 Step:      0 Loss:   0.2032 Training Acc:    91.57    91.57    91.57%\n",
      "Epoch   4763 Step:      0 Loss:   0.2032 Training Acc:    91.57    91.57    91.57%\n",
      "Epoch   4764 Step:      0 Loss:   0.2031 Training Acc:    91.57    91.57    91.57%\n",
      "Epoch   4765 Step:      0 Loss:   0.2031 Training Acc:    91.58    91.58    91.58%\n",
      "Epoch   4766 Step:      0 Loss:   0.2030 Training Acc:    91.57    91.57    91.57%\n",
      "Epoch   4767 Step:      0 Loss:   0.2030 Training Acc:    91.58    91.58    91.58%\n",
      "Epoch   4768 Step:      0 Loss:   0.2030 Training Acc:    91.58    91.58    91.58%\n",
      "Epoch   4769 Step:      0 Loss:   0.2029 Training Acc:    91.58    91.58    91.58%\n",
      "Epoch   4770 Step:      0 Loss:   0.2029 Training Acc:    91.59    91.59    91.59%\n",
      "Epoch   4771 Step:      0 Loss:   0.2029 Training Acc:    91.58    91.58    91.58%\n",
      "Epoch   4772 Step:      0 Loss:   0.2028 Training Acc:    91.59    91.59    91.59%\n",
      "Epoch   4773 Step:      0 Loss:   0.2028 Training Acc:    91.59    91.59    91.59%\n",
      "Epoch   4774 Step:      0 Loss:   0.2028 Training Acc:    91.59    91.59    91.59%\n",
      "Epoch   4775 Step:      0 Loss:   0.2027 Training Acc:    91.59    91.59    91.59%\n",
      "Epoch   4776 Step:      0 Loss:   0.2027 Training Acc:    91.59    91.59    91.59%\n",
      "Epoch   4777 Step:      0 Loss:   0.2027 Training Acc:    91.60    91.60    91.60%\n",
      "Epoch   4778 Step:      0 Loss:   0.2026 Training Acc:    91.60    91.60    91.60%\n",
      "Epoch   4779 Step:      0 Loss:   0.2026 Training Acc:    91.60    91.60    91.60%\n",
      "Epoch   4780 Step:      0 Loss:   0.2025 Training Acc:    91.60    91.60    91.60%\n",
      "Epoch   4781 Step:      0 Loss:   0.2025 Training Acc:    91.60    91.60    91.60%\n",
      "Epoch   4782 Step:      0 Loss:   0.2025 Training Acc:    91.60    91.60    91.60%\n",
      "Epoch   4783 Step:      0 Loss:   0.2024 Training Acc:    91.61    91.61    91.61%\n",
      "Epoch   4784 Step:      0 Loss:   0.2024 Training Acc:    91.61    91.61    91.61%\n",
      "Epoch   4785 Step:      0 Loss:   0.2024 Training Acc:    91.61    91.61    91.61%\n",
      "Epoch   4786 Step:      0 Loss:   0.2023 Training Acc:    91.61    91.61    91.61%\n",
      "Epoch   4787 Step:      0 Loss:   0.2023 Training Acc:    91.61    91.61    91.61%\n",
      "Epoch   4788 Step:      0 Loss:   0.2023 Training Acc:    91.61    91.61    91.61%\n",
      "Epoch   4789 Step:      0 Loss:   0.2022 Training Acc:    91.61    91.61    91.61%\n",
      "Epoch   4790 Step:      0 Loss:   0.2022 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   4791 Step:      0 Loss:   0.2021 Training Acc:    91.61    91.61    91.61%\n",
      "Epoch   4792 Step:      0 Loss:   0.2021 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   4793 Step:      0 Loss:   0.2021 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   4794 Step:      0 Loss:   0.2020 Training Acc:    91.63    91.63    91.63%\n",
      "Epoch   4795 Step:      0 Loss:   0.2020 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   4796 Step:      0 Loss:   0.2020 Training Acc:    91.63    91.63    91.63%\n",
      "Epoch   4797 Step:      0 Loss:   0.2019 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   4798 Step:      0 Loss:   0.2019 Training Acc:    91.63    91.63    91.63%\n",
      "Epoch   4799 Step:      0 Loss:   0.2019 Training Acc:    91.63    91.63    91.63%\n",
      "Epoch   4800 Step:      0 Loss:   0.2018 Training Acc:    91.63    91.63    91.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4801 Step:      0 Loss:   0.2018 Training Acc:    91.63    91.63    91.63%\n",
      "Epoch   4802 Step:      0 Loss:   0.2018 Training Acc:    91.64    91.64    91.64%\n",
      "Epoch   4803 Step:      0 Loss:   0.2017 Training Acc:    91.63    91.63    91.63%\n",
      "Epoch   4804 Step:      0 Loss:   0.2017 Training Acc:    91.63    91.63    91.63%\n",
      "Epoch   4805 Step:      0 Loss:   0.2017 Training Acc:    91.63    91.63    91.63%\n",
      "Epoch   4806 Step:      0 Loss:   0.2017 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   4807 Step:      0 Loss:   0.2017 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   4808 Step:      0 Loss:   0.2017 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   4809 Step:      0 Loss:   0.2018 Training Acc:    91.60    91.60    91.60%\n",
      "Epoch   4810 Step:      0 Loss:   0.2020 Training Acc:    91.59    91.59    91.59%\n",
      "Epoch   4811 Step:      0 Loss:   0.2023 Training Acc:    91.57    91.57    91.57%\n",
      "Epoch   4812 Step:      0 Loss:   0.2029 Training Acc:    91.53    91.53    91.53%\n",
      "Epoch   4813 Step:      0 Loss:   0.2041 Training Acc:    91.44    91.44    91.44%\n",
      "Epoch   4814 Step:      0 Loss:   0.2059 Training Acc:    91.34    91.34    91.34%\n",
      "Epoch   4815 Step:      0 Loss:   0.2093 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4816 Step:      0 Loss:   0.2138 Training Acc:    90.80    90.80    90.80%\n",
      "Epoch   4817 Step:      0 Loss:   0.2226 Training Acc:    90.15    90.15    90.15%\n",
      "Epoch   4818 Step:      0 Loss:   0.2312 Training Acc:    89.74    89.74    89.74%\n",
      "Epoch   4819 Step:      0 Loss:   0.2503 Training Acc:    88.74    88.74    88.74%\n",
      "Epoch   4820 Step:      0 Loss:   0.2601 Training Acc:    88.53    88.53    88.53%\n",
      "Epoch   4821 Step:      0 Loss:   0.2731 Training Acc:    88.19    88.19    88.19%\n",
      "Epoch   4822 Step:      0 Loss:   0.2161 Training Acc:    90.66    90.66    90.66%\n",
      "Epoch   4823 Step:      0 Loss:   0.2299 Training Acc:    90.22    90.22    90.22%\n",
      "Epoch   4824 Step:      0 Loss:   0.2637 Training Acc:    88.19    88.19    88.19%\n",
      "Epoch   4825 Step:      0 Loss:   0.2385 Training Acc:    89.33    89.33    89.33%\n",
      "Epoch   4826 Step:      0 Loss:   0.2258 Training Acc:    90.13    90.13    90.13%\n",
      "Epoch   4827 Step:      0 Loss:   0.2265 Training Acc:    90.00    90.00    90.00%\n",
      "Epoch   4828 Step:      0 Loss:   0.2346 Training Acc:    89.66    89.66    89.66%\n",
      "Epoch   4829 Step:      0 Loss:   0.2185 Training Acc:    90.49    90.49    90.49%\n",
      "Epoch   4830 Step:      0 Loss:   0.2206 Training Acc:    90.44    90.44    90.44%\n",
      "Epoch   4831 Step:      0 Loss:   0.2214 Training Acc:    90.33    90.33    90.33%\n",
      "Epoch   4832 Step:      0 Loss:   0.2141 Training Acc:    90.69    90.69    90.69%\n",
      "Epoch   4833 Step:      0 Loss:   0.2142 Training Acc:    90.79    90.79    90.79%\n",
      "Epoch   4834 Step:      0 Loss:   0.2161 Training Acc:    90.69    90.69    90.69%\n",
      "Epoch   4835 Step:      0 Loss:   0.2095 Training Acc:    91.06    91.06    91.06%\n",
      "Epoch   4836 Step:      0 Loss:   0.2098 Training Acc:    90.99    90.99    90.99%\n",
      "Epoch   4837 Step:      0 Loss:   0.2125 Training Acc:    90.90    90.90    90.90%\n",
      "Epoch   4838 Step:      0 Loss:   0.2054 Training Acc:    91.37    91.37    91.37%\n",
      "Epoch   4839 Step:      0 Loss:   0.2097 Training Acc:    91.03    91.03    91.03%\n",
      "Epoch   4840 Step:      0 Loss:   0.2066 Training Acc:    91.25    91.25    91.25%\n",
      "Epoch   4841 Step:      0 Loss:   0.2069 Training Acc:    91.24    91.24    91.24%\n",
      "Epoch   4842 Step:      0 Loss:   0.2056 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   4843 Step:      0 Loss:   0.2068 Training Acc:    91.27    91.27    91.27%\n",
      "Epoch   4844 Step:      0 Loss:   0.2036 Training Acc:    91.49    91.49    91.49%\n",
      "Epoch   4845 Step:      0 Loss:   0.2052 Training Acc:    91.30    91.30    91.30%\n",
      "Epoch   4846 Step:      0 Loss:   0.2043 Training Acc:    91.37    91.37    91.37%\n",
      "Epoch   4847 Step:      0 Loss:   0.2030 Training Acc:    91.51    91.51    91.51%\n",
      "Epoch   4848 Step:      0 Loss:   0.2037 Training Acc:    91.45    91.45    91.45%\n",
      "Epoch   4849 Step:      0 Loss:   0.2030 Training Acc:    91.50    91.50    91.50%\n",
      "Epoch   4850 Step:      0 Loss:   0.2025 Training Acc:    91.50    91.50    91.50%\n",
      "Epoch   4851 Step:      0 Loss:   0.2024 Training Acc:    91.56    91.56    91.56%\n",
      "Epoch   4852 Step:      0 Loss:   0.2024 Training Acc:    91.55    91.55    91.55%\n",
      "Epoch   4853 Step:      0 Loss:   0.2016 Training Acc:    91.59    91.59    91.59%\n",
      "Epoch   4854 Step:      0 Loss:   0.2020 Training Acc:    91.57    91.57    91.57%\n",
      "Epoch   4855 Step:      0 Loss:   0.2017 Training Acc:    91.57    91.57    91.57%\n",
      "Epoch   4856 Step:      0 Loss:   0.2011 Training Acc:    91.64    91.64    91.64%\n",
      "Epoch   4857 Step:      0 Loss:   0.2016 Training Acc:    91.60    91.60    91.60%\n",
      "Epoch   4858 Step:      0 Loss:   0.2010 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   4859 Step:      0 Loss:   0.2011 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   4860 Step:      0 Loss:   0.2008 Training Acc:    91.66    91.66    91.66%\n",
      "Epoch   4861 Step:      0 Loss:   0.2010 Training Acc:    91.66    91.66    91.66%\n",
      "Epoch   4862 Step:      0 Loss:   0.2004 Training Acc:    91.68    91.68    91.68%\n",
      "Epoch   4863 Step:      0 Loss:   0.2008 Training Acc:    91.64    91.64    91.64%\n",
      "Epoch   4864 Step:      0 Loss:   0.2004 Training Acc:    91.64    91.64    91.64%\n",
      "Epoch   4865 Step:      0 Loss:   0.2004 Training Acc:    91.66    91.66    91.66%\n",
      "Epoch   4866 Step:      0 Loss:   0.2003 Training Acc:    91.67    91.67    91.67%\n",
      "Epoch   4867 Step:      0 Loss:   0.2004 Training Acc:    91.66    91.66    91.66%\n",
      "Epoch   4868 Step:      0 Loss:   0.2000 Training Acc:    91.68    91.68    91.68%\n",
      "Epoch   4869 Step:      0 Loss:   0.2003 Training Acc:    91.67    91.67    91.67%\n",
      "Epoch   4870 Step:      0 Loss:   0.2000 Training Acc:    91.69    91.69    91.69%\n",
      "Epoch   4871 Step:      0 Loss:   0.2000 Training Acc:    91.68    91.68    91.68%\n",
      "Epoch   4872 Step:      0 Loss:   0.1999 Training Acc:    91.70    91.70    91.70%\n",
      "Epoch   4873 Step:      0 Loss:   0.1999 Training Acc:    91.68    91.68    91.68%\n",
      "Epoch   4874 Step:      0 Loss:   0.1998 Training Acc:    91.71    91.71    91.71%\n",
      "Epoch   4875 Step:      0 Loss:   0.1998 Training Acc:    91.71    91.71    91.71%\n",
      "Epoch   4876 Step:      0 Loss:   0.1997 Training Acc:    91.71    91.71    91.71%\n",
      "Epoch   4877 Step:      0 Loss:   0.1997 Training Acc:    91.72    91.72    91.72%\n",
      "Epoch   4878 Step:      0 Loss:   0.1997 Training Acc:    91.70    91.70    91.70%\n",
      "Epoch   4879 Step:      0 Loss:   0.1996 Training Acc:    91.71    91.71    91.71%\n",
      "Epoch   4880 Step:      0 Loss:   0.1995 Training Acc:    91.72    91.72    91.72%\n",
      "Epoch   4881 Step:      0 Loss:   0.1995 Training Acc:    91.72    91.72    91.72%\n",
      "Epoch   4882 Step:      0 Loss:   0.1995 Training Acc:    91.71    91.71    91.71%\n",
      "Epoch   4883 Step:      0 Loss:   0.1994 Training Acc:    91.73    91.73    91.73%\n",
      "Epoch   4884 Step:      0 Loss:   0.1994 Training Acc:    91.73    91.73    91.73%\n",
      "Epoch   4885 Step:      0 Loss:   0.1994 Training Acc:    91.73    91.73    91.73%\n",
      "Epoch   4886 Step:      0 Loss:   0.1993 Training Acc:    91.73    91.73    91.73%\n",
      "Epoch   4887 Step:      0 Loss:   0.1993 Training Acc:    91.73    91.73    91.73%\n",
      "Epoch   4888 Step:      0 Loss:   0.1993 Training Acc:    91.73    91.73    91.73%\n",
      "Epoch   4889 Step:      0 Loss:   0.1992 Training Acc:    91.74    91.74    91.74%\n",
      "Epoch   4890 Step:      0 Loss:   0.1992 Training Acc:    91.74    91.74    91.74%\n",
      "Epoch   4891 Step:      0 Loss:   0.1992 Training Acc:    91.74    91.74    91.74%\n",
      "Epoch   4892 Step:      0 Loss:   0.1991 Training Acc:    91.74    91.74    91.74%\n",
      "Epoch   4893 Step:      0 Loss:   0.1991 Training Acc:    91.75    91.75    91.75%\n",
      "Epoch   4894 Step:      0 Loss:   0.1990 Training Acc:    91.74    91.74    91.74%\n",
      "Epoch   4895 Step:      0 Loss:   0.1990 Training Acc:    91.74    91.74    91.74%\n",
      "Epoch   4896 Step:      0 Loss:   0.1990 Training Acc:    91.74    91.74    91.74%\n",
      "Epoch   4897 Step:      0 Loss:   0.1990 Training Acc:    91.76    91.76    91.76%\n",
      "Epoch   4898 Step:      0 Loss:   0.1989 Training Acc:    91.75    91.75    91.75%\n",
      "Epoch   4899 Step:      0 Loss:   0.1989 Training Acc:    91.74    91.74    91.74%\n",
      "Epoch   4900 Step:      0 Loss:   0.1989 Training Acc:    91.74    91.74    91.74%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4901 Step:      0 Loss:   0.1988 Training Acc:    91.76    91.76    91.76%\n",
      "Epoch   4902 Step:      0 Loss:   0.1988 Training Acc:    91.76    91.76    91.76%\n",
      "Epoch   4903 Step:      0 Loss:   0.1988 Training Acc:    91.75    91.75    91.75%\n",
      "Epoch   4904 Step:      0 Loss:   0.1987 Training Acc:    91.76    91.76    91.76%\n",
      "Epoch   4905 Step:      0 Loss:   0.1987 Training Acc:    91.77    91.77    91.77%\n",
      "Epoch   4906 Step:      0 Loss:   0.1987 Training Acc:    91.76    91.76    91.76%\n",
      "Epoch   4907 Step:      0 Loss:   0.1986 Training Acc:    91.76    91.76    91.76%\n",
      "Epoch   4908 Step:      0 Loss:   0.1986 Training Acc:    91.77    91.77    91.77%\n",
      "Epoch   4909 Step:      0 Loss:   0.1986 Training Acc:    91.77    91.77    91.77%\n",
      "Epoch   4910 Step:      0 Loss:   0.1985 Training Acc:    91.76    91.76    91.76%\n",
      "Epoch   4911 Step:      0 Loss:   0.1985 Training Acc:    91.77    91.77    91.77%\n",
      "Epoch   4912 Step:      0 Loss:   0.1985 Training Acc:    91.77    91.77    91.77%\n",
      "Epoch   4913 Step:      0 Loss:   0.1984 Training Acc:    91.77    91.77    91.77%\n",
      "Epoch   4914 Step:      0 Loss:   0.1984 Training Acc:    91.78    91.78    91.78%\n",
      "Epoch   4915 Step:      0 Loss:   0.1984 Training Acc:    91.78    91.78    91.78%\n",
      "Epoch   4916 Step:      0 Loss:   0.1984 Training Acc:    91.78    91.78    91.78%\n",
      "Epoch   4917 Step:      0 Loss:   0.1983 Training Acc:    91.78    91.78    91.78%\n",
      "Epoch   4918 Step:      0 Loss:   0.1983 Training Acc:    91.78    91.78    91.78%\n",
      "Epoch   4919 Step:      0 Loss:   0.1983 Training Acc:    91.79    91.79    91.79%\n",
      "Epoch   4920 Step:      0 Loss:   0.1982 Training Acc:    91.79    91.79    91.79%\n",
      "Epoch   4921 Step:      0 Loss:   0.1982 Training Acc:    91.79    91.79    91.79%\n",
      "Epoch   4922 Step:      0 Loss:   0.1982 Training Acc:    91.79    91.79    91.79%\n",
      "Epoch   4923 Step:      0 Loss:   0.1981 Training Acc:    91.79    91.79    91.79%\n",
      "Epoch   4924 Step:      0 Loss:   0.1981 Training Acc:    91.79    91.79    91.79%\n",
      "Epoch   4925 Step:      0 Loss:   0.1981 Training Acc:    91.79    91.79    91.79%\n",
      "Epoch   4926 Step:      0 Loss:   0.1980 Training Acc:    91.80    91.80    91.80%\n",
      "Epoch   4927 Step:      0 Loss:   0.1980 Training Acc:    91.79    91.79    91.79%\n",
      "Epoch   4928 Step:      0 Loss:   0.1980 Training Acc:    91.80    91.80    91.80%\n",
      "Epoch   4929 Step:      0 Loss:   0.1980 Training Acc:    91.80    91.80    91.80%\n",
      "Epoch   4930 Step:      0 Loss:   0.1979 Training Acc:    91.80    91.80    91.80%\n",
      "Epoch   4931 Step:      0 Loss:   0.1979 Training Acc:    91.81    91.81    91.81%\n",
      "Epoch   4932 Step:      0 Loss:   0.1979 Training Acc:    91.80    91.80    91.80%\n",
      "Epoch   4933 Step:      0 Loss:   0.1978 Training Acc:    91.81    91.81    91.81%\n",
      "Epoch   4934 Step:      0 Loss:   0.1978 Training Acc:    91.81    91.81    91.81%\n",
      "Epoch   4935 Step:      0 Loss:   0.1978 Training Acc:    91.81    91.81    91.81%\n",
      "Epoch   4936 Step:      0 Loss:   0.1977 Training Acc:    91.81    91.81    91.81%\n",
      "Epoch   4937 Step:      0 Loss:   0.1977 Training Acc:    91.81    91.81    91.81%\n",
      "Epoch   4938 Step:      0 Loss:   0.1977 Training Acc:    91.82    91.82    91.82%\n",
      "Epoch   4939 Step:      0 Loss:   0.1976 Training Acc:    91.82    91.82    91.82%\n",
      "Epoch   4940 Step:      0 Loss:   0.1976 Training Acc:    91.82    91.82    91.82%\n",
      "Epoch   4941 Step:      0 Loss:   0.1976 Training Acc:    91.82    91.82    91.82%\n",
      "Epoch   4942 Step:      0 Loss:   0.1976 Training Acc:    91.82    91.82    91.82%\n",
      "Epoch   4943 Step:      0 Loss:   0.1975 Training Acc:    91.82    91.82    91.82%\n",
      "Epoch   4944 Step:      0 Loss:   0.1975 Training Acc:    91.83    91.83    91.83%\n",
      "Epoch   4945 Step:      0 Loss:   0.1975 Training Acc:    91.83    91.83    91.83%\n",
      "Epoch   4946 Step:      0 Loss:   0.1974 Training Acc:    91.83    91.83    91.83%\n",
      "Epoch   4947 Step:      0 Loss:   0.1974 Training Acc:    91.83    91.83    91.83%\n",
      "Epoch   4948 Step:      0 Loss:   0.1974 Training Acc:    91.83    91.83    91.83%\n",
      "Epoch   4949 Step:      0 Loss:   0.1973 Training Acc:    91.83    91.83    91.83%\n",
      "Epoch   4950 Step:      0 Loss:   0.1973 Training Acc:    91.83    91.83    91.83%\n",
      "Epoch   4951 Step:      0 Loss:   0.1973 Training Acc:    91.84    91.84    91.84%\n",
      "Epoch   4952 Step:      0 Loss:   0.1972 Training Acc:    91.84    91.84    91.84%\n",
      "Epoch   4953 Step:      0 Loss:   0.1972 Training Acc:    91.84    91.84    91.84%\n",
      "Epoch   4954 Step:      0 Loss:   0.1972 Training Acc:    91.84    91.84    91.84%\n",
      "Epoch   4955 Step:      0 Loss:   0.1972 Training Acc:    91.84    91.84    91.84%\n",
      "Epoch   4956 Step:      0 Loss:   0.1971 Training Acc:    91.84    91.84    91.84%\n",
      "Epoch   4957 Step:      0 Loss:   0.1971 Training Acc:    91.84    91.84    91.84%\n",
      "Epoch   4958 Step:      0 Loss:   0.1971 Training Acc:    91.85    91.85    91.85%\n",
      "Epoch   4959 Step:      0 Loss:   0.1970 Training Acc:    91.85    91.85    91.85%\n",
      "Epoch   4960 Step:      0 Loss:   0.1970 Training Acc:    91.85    91.85    91.85%\n",
      "Epoch   4961 Step:      0 Loss:   0.1970 Training Acc:    91.85    91.85    91.85%\n",
      "Epoch   4962 Step:      0 Loss:   0.1969 Training Acc:    91.85    91.85    91.85%\n",
      "Epoch   4963 Step:      0 Loss:   0.1969 Training Acc:    91.85    91.85    91.85%\n",
      "Epoch   4964 Step:      0 Loss:   0.1969 Training Acc:    91.85    91.85    91.85%\n",
      "Epoch   4965 Step:      0 Loss:   0.1969 Training Acc:    91.85    91.85    91.85%\n",
      "Epoch   4966 Step:      0 Loss:   0.1968 Training Acc:    91.85    91.85    91.85%\n",
      "Epoch   4967 Step:      0 Loss:   0.1968 Training Acc:    91.86    91.86    91.86%\n",
      "Epoch   4968 Step:      0 Loss:   0.1968 Training Acc:    91.86    91.86    91.86%\n",
      "Epoch   4969 Step:      0 Loss:   0.1967 Training Acc:    91.86    91.86    91.86%\n",
      "Epoch   4970 Step:      0 Loss:   0.1967 Training Acc:    91.86    91.86    91.86%\n",
      "Epoch   4971 Step:      0 Loss:   0.1967 Training Acc:    91.86    91.86    91.86%\n",
      "Epoch   4972 Step:      0 Loss:   0.1966 Training Acc:    91.86    91.86    91.86%\n",
      "Epoch   4973 Step:      0 Loss:   0.1966 Training Acc:    91.86    91.86    91.86%\n",
      "Epoch   4974 Step:      0 Loss:   0.1966 Training Acc:    91.86    91.86    91.86%\n",
      "Epoch   4975 Step:      0 Loss:   0.1965 Training Acc:    91.87    91.87    91.87%\n",
      "Epoch   4976 Step:      0 Loss:   0.1965 Training Acc:    91.87    91.87    91.87%\n",
      "Epoch   4977 Step:      0 Loss:   0.1965 Training Acc:    91.87    91.87    91.87%\n",
      "Epoch   4978 Step:      0 Loss:   0.1965 Training Acc:    91.87    91.87    91.87%\n",
      "Epoch   4979 Step:      0 Loss:   0.1964 Training Acc:    91.87    91.87    91.87%\n",
      "Epoch   4980 Step:      0 Loss:   0.1964 Training Acc:    91.87    91.87    91.87%\n",
      "Epoch   4981 Step:      0 Loss:   0.1964 Training Acc:    91.88    91.88    91.88%\n",
      "Epoch   4982 Step:      0 Loss:   0.1963 Training Acc:    91.88    91.88    91.88%\n",
      "Epoch   4983 Step:      0 Loss:   0.1963 Training Acc:    91.88    91.88    91.88%\n",
      "Epoch   4984 Step:      0 Loss:   0.1963 Training Acc:    91.88    91.88    91.88%\n",
      "Epoch   4985 Step:      0 Loss:   0.1962 Training Acc:    91.88    91.88    91.88%\n",
      "Epoch   4986 Step:      0 Loss:   0.1962 Training Acc:    91.88    91.88    91.88%\n",
      "Epoch   4987 Step:      0 Loss:   0.1962 Training Acc:    91.88    91.88    91.88%\n",
      "Epoch   4988 Step:      0 Loss:   0.1961 Training Acc:    91.89    91.89    91.89%\n",
      "Epoch   4989 Step:      0 Loss:   0.1961 Training Acc:    91.89    91.89    91.89%\n",
      "Epoch   4990 Step:      0 Loss:   0.1961 Training Acc:    91.89    91.89    91.89%\n",
      "Epoch   4991 Step:      0 Loss:   0.1961 Training Acc:    91.89    91.89    91.89%\n",
      "Epoch   4992 Step:      0 Loss:   0.1960 Training Acc:    91.89    91.89    91.89%\n",
      "Epoch   4993 Step:      0 Loss:   0.1960 Training Acc:    91.89    91.89    91.89%\n",
      "Epoch   4994 Step:      0 Loss:   0.1960 Training Acc:    91.89    91.89    91.89%\n",
      "Epoch   4995 Step:      0 Loss:   0.1959 Training Acc:    91.90    91.90    91.90%\n",
      "Epoch   4996 Step:      0 Loss:   0.1959 Training Acc:    91.90    91.90    91.90%\n",
      "Epoch   4997 Step:      0 Loss:   0.1959 Training Acc:    91.90    91.90    91.90%\n",
      "Epoch   4998 Step:      0 Loss:   0.1958 Training Acc:    91.90    91.90    91.90%\n",
      "Epoch   4999 Step:      0 Loss:   0.1958 Training Acc:    91.90    91.90    91.90%\n",
      "Epoch   5000 Step:      0 Loss:   0.1958 Training Acc:    91.90    91.90    91.90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5001 Step:      0 Loss:   0.1957 Training Acc:    91.91    91.91    91.91%\n",
      "Epoch   5002 Step:      0 Loss:   0.1957 Training Acc:    91.91    91.91    91.91%\n",
      "Epoch   5003 Step:      0 Loss:   0.1957 Training Acc:    91.91    91.91    91.91%\n",
      "Epoch   5004 Step:      0 Loss:   0.1957 Training Acc:    91.91    91.91    91.91%\n",
      "Epoch   5005 Step:      0 Loss:   0.1956 Training Acc:    91.91    91.91    91.91%\n",
      "Epoch   5006 Step:      0 Loss:   0.1956 Training Acc:    91.91    91.91    91.91%\n",
      "Epoch   5007 Step:      0 Loss:   0.1956 Training Acc:    91.92    91.92    91.92%\n",
      "Epoch   5008 Step:      0 Loss:   0.1955 Training Acc:    91.92    91.92    91.92%\n",
      "Epoch   5009 Step:      0 Loss:   0.1955 Training Acc:    91.92    91.92    91.92%\n",
      "Epoch   5010 Step:      0 Loss:   0.1955 Training Acc:    91.92    91.92    91.92%\n",
      "Epoch   5011 Step:      0 Loss:   0.1954 Training Acc:    91.92    91.92    91.92%\n",
      "Epoch   5012 Step:      0 Loss:   0.1954 Training Acc:    91.92    91.92    91.92%\n",
      "Epoch   5013 Step:      0 Loss:   0.1954 Training Acc:    91.92    91.92    91.92%\n",
      "Epoch   5014 Step:      0 Loss:   0.1953 Training Acc:    91.92    91.92    91.92%\n",
      "Epoch   5015 Step:      0 Loss:   0.1953 Training Acc:    91.92    91.92    91.92%\n",
      "Epoch   5016 Step:      0 Loss:   0.1953 Training Acc:    91.93    91.93    91.93%\n",
      "Epoch   5017 Step:      0 Loss:   0.1953 Training Acc:    91.93    91.93    91.93%\n",
      "Epoch   5018 Step:      0 Loss:   0.1952 Training Acc:    91.93    91.93    91.93%\n",
      "Epoch   5019 Step:      0 Loss:   0.1952 Training Acc:    91.93    91.93    91.93%\n",
      "Epoch   5020 Step:      0 Loss:   0.1952 Training Acc:    91.94    91.94    91.94%\n",
      "Epoch   5021 Step:      0 Loss:   0.1951 Training Acc:    91.94    91.94    91.94%\n",
      "Epoch   5022 Step:      0 Loss:   0.1951 Training Acc:    91.94    91.94    91.94%\n",
      "Epoch   5023 Step:      0 Loss:   0.1951 Training Acc:    91.94    91.94    91.94%\n",
      "Epoch   5024 Step:      0 Loss:   0.1950 Training Acc:    91.94    91.94    91.94%\n",
      "Epoch   5025 Step:      0 Loss:   0.1950 Training Acc:    91.94    91.94    91.94%\n",
      "Epoch   5026 Step:      0 Loss:   0.1950 Training Acc:    91.94    91.94    91.94%\n",
      "Epoch   5027 Step:      0 Loss:   0.1949 Training Acc:    91.94    91.94    91.94%\n",
      "Epoch   5028 Step:      0 Loss:   0.1949 Training Acc:    91.95    91.95    91.95%\n",
      "Epoch   5029 Step:      0 Loss:   0.1949 Training Acc:    91.95    91.95    91.95%\n",
      "Epoch   5030 Step:      0 Loss:   0.1948 Training Acc:    91.95    91.95    91.95%\n",
      "Epoch   5031 Step:      0 Loss:   0.1948 Training Acc:    91.95    91.95    91.95%\n",
      "Epoch   5032 Step:      0 Loss:   0.1948 Training Acc:    91.95    91.95    91.95%\n",
      "Epoch   5033 Step:      0 Loss:   0.1948 Training Acc:    91.96    91.96    91.96%\n",
      "Epoch   5034 Step:      0 Loss:   0.1947 Training Acc:    91.96    91.96    91.96%\n",
      "Epoch   5035 Step:      0 Loss:   0.1947 Training Acc:    91.96    91.96    91.96%\n",
      "Epoch   5036 Step:      0 Loss:   0.1947 Training Acc:    91.96    91.96    91.96%\n",
      "Epoch   5037 Step:      0 Loss:   0.1946 Training Acc:    91.96    91.96    91.96%\n",
      "Epoch   5038 Step:      0 Loss:   0.1946 Training Acc:    91.97    91.97    91.97%\n",
      "Epoch   5039 Step:      0 Loss:   0.1946 Training Acc:    91.97    91.97    91.97%\n",
      "Epoch   5040 Step:      0 Loss:   0.1945 Training Acc:    91.97    91.97    91.97%\n",
      "Epoch   5041 Step:      0 Loss:   0.1945 Training Acc:    91.97    91.97    91.97%\n",
      "Epoch   5042 Step:      0 Loss:   0.1945 Training Acc:    91.98    91.98    91.98%\n",
      "Epoch   5043 Step:      0 Loss:   0.1944 Training Acc:    91.98    91.98    91.98%\n",
      "Epoch   5044 Step:      0 Loss:   0.1944 Training Acc:    91.98    91.98    91.98%\n",
      "Epoch   5045 Step:      0 Loss:   0.1944 Training Acc:    91.99    91.99    91.99%\n",
      "Epoch   5046 Step:      0 Loss:   0.1944 Training Acc:    91.99    91.99    91.99%\n",
      "Epoch   5047 Step:      0 Loss:   0.1943 Training Acc:    91.99    91.99    91.99%\n",
      "Epoch   5048 Step:      0 Loss:   0.1943 Training Acc:    91.99    91.99    91.99%\n",
      "Epoch   5049 Step:      0 Loss:   0.1943 Training Acc:    91.99    91.99    91.99%\n",
      "Epoch   5050 Step:      0 Loss:   0.1942 Training Acc:    91.99    91.99    91.99%\n",
      "Epoch   5051 Step:      0 Loss:   0.1942 Training Acc:    91.99    91.99    91.99%\n",
      "Epoch   5052 Step:      0 Loss:   0.1942 Training Acc:    92.00    92.00    92.00%\n",
      "Epoch   5053 Step:      0 Loss:   0.1941 Training Acc:    92.00    92.00    92.00%\n",
      "Epoch   5054 Step:      0 Loss:   0.1941 Training Acc:    92.00    92.00    92.00%\n",
      "Epoch   5055 Step:      0 Loss:   0.1941 Training Acc:    92.00    92.00    92.00%\n",
      "Epoch   5056 Step:      0 Loss:   0.1940 Training Acc:    92.01    92.01    92.01%\n",
      "Epoch   5057 Step:      0 Loss:   0.1940 Training Acc:    92.01    92.01    92.01%\n",
      "Epoch   5058 Step:      0 Loss:   0.1940 Training Acc:    92.01    92.01    92.01%\n",
      "Epoch   5059 Step:      0 Loss:   0.1939 Training Acc:    92.01    92.01    92.01%\n",
      "Epoch   5060 Step:      0 Loss:   0.1939 Training Acc:    92.01    92.01    92.01%\n",
      "Epoch   5061 Step:      0 Loss:   0.1939 Training Acc:    92.02    92.02    92.02%\n",
      "Epoch   5062 Step:      0 Loss:   0.1939 Training Acc:    92.02    92.02    92.02%\n",
      "Epoch   5063 Step:      0 Loss:   0.1938 Training Acc:    92.02    92.02    92.02%\n",
      "Epoch   5064 Step:      0 Loss:   0.1938 Training Acc:    92.02    92.02    92.02%\n",
      "Epoch   5065 Step:      0 Loss:   0.1938 Training Acc:    92.02    92.02    92.02%\n",
      "Epoch   5066 Step:      0 Loss:   0.1937 Training Acc:    92.03    92.03    92.03%\n",
      "Epoch   5067 Step:      0 Loss:   0.1937 Training Acc:    92.03    92.03    92.03%\n",
      "Epoch   5068 Step:      0 Loss:   0.1937 Training Acc:    92.03    92.03    92.03%\n",
      "Epoch   5069 Step:      0 Loss:   0.1936 Training Acc:    92.03    92.03    92.03%\n",
      "Epoch   5070 Step:      0 Loss:   0.1936 Training Acc:    92.04    92.04    92.04%\n",
      "Epoch   5071 Step:      0 Loss:   0.1936 Training Acc:    92.04    92.04    92.04%\n",
      "Epoch   5072 Step:      0 Loss:   0.1935 Training Acc:    92.04    92.04    92.04%\n",
      "Epoch   5073 Step:      0 Loss:   0.1935 Training Acc:    92.04    92.04    92.04%\n",
      "Epoch   5074 Step:      0 Loss:   0.1935 Training Acc:    92.04    92.04    92.04%\n",
      "Epoch   5075 Step:      0 Loss:   0.1934 Training Acc:    92.04    92.04    92.04%\n",
      "Epoch   5076 Step:      0 Loss:   0.1934 Training Acc:    92.04    92.04    92.04%\n",
      "Epoch   5077 Step:      0 Loss:   0.1934 Training Acc:    92.05    92.05    92.05%\n",
      "Epoch   5078 Step:      0 Loss:   0.1934 Training Acc:    92.05    92.05    92.05%\n",
      "Epoch   5079 Step:      0 Loss:   0.1933 Training Acc:    92.05    92.05    92.05%\n",
      "Epoch   5080 Step:      0 Loss:   0.1933 Training Acc:    92.05    92.05    92.05%\n",
      "Epoch   5081 Step:      0 Loss:   0.1933 Training Acc:    92.05    92.05    92.05%\n",
      "Epoch   5082 Step:      0 Loss:   0.1932 Training Acc:    92.05    92.05    92.05%\n",
      "Epoch   5083 Step:      0 Loss:   0.1932 Training Acc:    92.06    92.06    92.06%\n",
      "Epoch   5084 Step:      0 Loss:   0.1932 Training Acc:    92.06    92.06    92.06%\n",
      "Epoch   5085 Step:      0 Loss:   0.1931 Training Acc:    92.06    92.06    92.06%\n",
      "Epoch   5086 Step:      0 Loss:   0.1931 Training Acc:    92.06    92.06    92.06%\n",
      "Epoch   5087 Step:      0 Loss:   0.1931 Training Acc:    92.06    92.06    92.06%\n",
      "Epoch   5088 Step:      0 Loss:   0.1930 Training Acc:    92.07    92.07    92.07%\n",
      "Epoch   5089 Step:      0 Loss:   0.1930 Training Acc:    92.07    92.07    92.07%\n",
      "Epoch   5090 Step:      0 Loss:   0.1930 Training Acc:    92.07    92.07    92.07%\n",
      "Epoch   5091 Step:      0 Loss:   0.1929 Training Acc:    92.07    92.07    92.07%\n",
      "Epoch   5092 Step:      0 Loss:   0.1929 Training Acc:    92.07    92.07    92.07%\n",
      "Epoch   5093 Step:      0 Loss:   0.1929 Training Acc:    92.07    92.07    92.07%\n",
      "Epoch   5094 Step:      0 Loss:   0.1928 Training Acc:    92.07    92.07    92.07%\n",
      "Epoch   5095 Step:      0 Loss:   0.1928 Training Acc:    92.07    92.07    92.07%\n",
      "Epoch   5096 Step:      0 Loss:   0.1928 Training Acc:    92.07    92.07    92.07%\n",
      "Epoch   5097 Step:      0 Loss:   0.1928 Training Acc:    92.08    92.08    92.08%\n",
      "Epoch   5098 Step:      0 Loss:   0.1927 Training Acc:    92.08    92.08    92.08%\n",
      "Epoch   5099 Step:      0 Loss:   0.1927 Training Acc:    92.08    92.08    92.08%\n",
      "Epoch   5100 Step:      0 Loss:   0.1927 Training Acc:    92.08    92.08    92.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5101 Step:      0 Loss:   0.1926 Training Acc:    92.08    92.08    92.08%\n",
      "Epoch   5102 Step:      0 Loss:   0.1926 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5103 Step:      0 Loss:   0.1926 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5104 Step:      0 Loss:   0.1925 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5105 Step:      0 Loss:   0.1925 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5106 Step:      0 Loss:   0.1925 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5107 Step:      0 Loss:   0.1924 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5108 Step:      0 Loss:   0.1924 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5109 Step:      0 Loss:   0.1924 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5110 Step:      0 Loss:   0.1923 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5111 Step:      0 Loss:   0.1923 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5112 Step:      0 Loss:   0.1923 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5113 Step:      0 Loss:   0.1922 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5114 Step:      0 Loss:   0.1922 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5115 Step:      0 Loss:   0.1922 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5116 Step:      0 Loss:   0.1922 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5117 Step:      0 Loss:   0.1921 Training Acc:    92.11    92.11    92.11%\n",
      "Epoch   5118 Step:      0 Loss:   0.1921 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5119 Step:      0 Loss:   0.1921 Training Acc:    92.11    92.11    92.11%\n",
      "Epoch   5120 Step:      0 Loss:   0.1920 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5121 Step:      0 Loss:   0.1920 Training Acc:    92.12    92.12    92.12%\n",
      "Epoch   5122 Step:      0 Loss:   0.1920 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5123 Step:      0 Loss:   0.1920 Training Acc:    92.12    92.12    92.12%\n",
      "Epoch   5124 Step:      0 Loss:   0.1920 Training Acc:    92.08    92.08    92.08%\n",
      "Epoch   5125 Step:      0 Loss:   0.1920 Training Acc:    92.11    92.11    92.11%\n",
      "Epoch   5126 Step:      0 Loss:   0.1920 Training Acc:    92.07    92.07    92.07%\n",
      "Epoch   5127 Step:      0 Loss:   0.1922 Training Acc:    92.12    92.12    92.12%\n",
      "Epoch   5128 Step:      0 Loss:   0.1924 Training Acc:    92.03    92.03    92.03%\n",
      "Epoch   5129 Step:      0 Loss:   0.1930 Training Acc:    92.08    92.08    92.08%\n",
      "Epoch   5130 Step:      0 Loss:   0.1940 Training Acc:    91.89    91.89    91.89%\n",
      "Epoch   5131 Step:      0 Loss:   0.1958 Training Acc:    91.87    91.87    91.87%\n",
      "Epoch   5132 Step:      0 Loss:   0.1993 Training Acc:    91.49    91.49    91.49%\n",
      "Epoch   5133 Step:      0 Loss:   0.2042 Training Acc:    91.26    91.26    91.26%\n",
      "Epoch   5134 Step:      0 Loss:   0.2122 Training Acc:    90.68    90.68    90.68%\n",
      "Epoch   5135 Step:      0 Loss:   0.2163 Training Acc:    90.51    90.51    90.51%\n",
      "Epoch   5136 Step:      0 Loss:   0.2152 Training Acc:    90.54    90.54    90.54%\n",
      "Epoch   5137 Step:      0 Loss:   0.2017 Training Acc:    91.47    91.47    91.47%\n",
      "Epoch   5138 Step:      0 Loss:   0.1925 Training Acc:    92.04    92.04    92.04%\n",
      "Epoch   5139 Step:      0 Loss:   0.1965 Training Acc:    91.71    91.71    91.71%\n",
      "Epoch   5140 Step:      0 Loss:   0.2038 Training Acc:    91.33    91.33    91.33%\n",
      "Epoch   5141 Step:      0 Loss:   0.2028 Training Acc:    91.27    91.27    91.27%\n",
      "Epoch   5142 Step:      0 Loss:   0.1943 Training Acc:    91.97    91.97    91.97%\n",
      "Epoch   5143 Step:      0 Loss:   0.1930 Training Acc:    92.03    92.03    92.03%\n",
      "Epoch   5144 Step:      0 Loss:   0.1987 Training Acc:    91.53    91.53    91.53%\n",
      "Epoch   5145 Step:      0 Loss:   0.1993 Training Acc:    91.62    91.62    91.62%\n",
      "Epoch   5146 Step:      0 Loss:   0.1942 Training Acc:    91.88    91.88    91.88%\n",
      "Epoch   5147 Step:      0 Loss:   0.1922 Training Acc:    92.05    92.05    92.05%\n",
      "Epoch   5148 Step:      0 Loss:   0.1955 Training Acc:    91.85    91.85    91.85%\n",
      "Epoch   5149 Step:      0 Loss:   0.1969 Training Acc:    91.66    91.66    91.66%\n",
      "Epoch   5150 Step:      0 Loss:   0.1934 Training Acc:    92.04    92.04    92.04%\n",
      "Epoch   5151 Step:      0 Loss:   0.1919 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5152 Step:      0 Loss:   0.1943 Training Acc:    91.84    91.84    91.84%\n",
      "Epoch   5153 Step:      0 Loss:   0.1947 Training Acc:    91.92    91.92    91.92%\n",
      "Epoch   5154 Step:      0 Loss:   0.1925 Training Acc:    92.00    92.00    92.00%\n",
      "Epoch   5155 Step:      0 Loss:   0.1915 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5156 Step:      0 Loss:   0.1931 Training Acc:    92.01    92.01    92.01%\n",
      "Epoch   5157 Step:      0 Loss:   0.1935 Training Acc:    91.91    91.91    91.91%\n",
      "Epoch   5158 Step:      0 Loss:   0.1918 Training Acc:    92.13    92.13    92.13%\n",
      "Epoch   5159 Step:      0 Loss:   0.1912 Training Acc:    92.12    92.12    92.12%\n",
      "Epoch   5160 Step:      0 Loss:   0.1925 Training Acc:    91.99    91.99    91.99%\n",
      "Epoch   5161 Step:      0 Loss:   0.1925 Training Acc:    92.06    92.06    92.06%\n",
      "Epoch   5162 Step:      0 Loss:   0.1914 Training Acc:    92.08    92.08    92.08%\n",
      "Epoch   5163 Step:      0 Loss:   0.1909 Training Acc:    92.14    92.14    92.14%\n",
      "Epoch   5164 Step:      0 Loss:   0.1918 Training Acc:    92.12    92.12    92.12%\n",
      "Epoch   5165 Step:      0 Loss:   0.1920 Training Acc:    92.03    92.03    92.03%\n",
      "Epoch   5166 Step:      0 Loss:   0.1911 Training Acc:    92.15    92.15    92.15%\n",
      "Epoch   5167 Step:      0 Loss:   0.1907 Training Acc:    92.16    92.16    92.16%\n",
      "Epoch   5168 Step:      0 Loss:   0.1913 Training Acc:    92.09    92.09    92.09%\n",
      "Epoch   5169 Step:      0 Loss:   0.1914 Training Acc:    92.15    92.15    92.15%\n",
      "Epoch   5170 Step:      0 Loss:   0.1909 Training Acc:    92.13    92.13    92.13%\n",
      "Epoch   5171 Step:      0 Loss:   0.1906 Training Acc:    92.17    92.17    92.17%\n",
      "Epoch   5172 Step:      0 Loss:   0.1909 Training Acc:    92.17    92.17    92.17%\n",
      "Epoch   5173 Step:      0 Loss:   0.1910 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5174 Step:      0 Loss:   0.1907 Training Acc:    92.18    92.18    92.18%\n",
      "Epoch   5175 Step:      0 Loss:   0.1904 Training Acc:    92.17    92.17    92.17%\n",
      "Epoch   5176 Step:      0 Loss:   0.1906 Training Acc:    92.14    92.14    92.14%\n",
      "Epoch   5177 Step:      0 Loss:   0.1907 Training Acc:    92.17    92.17    92.17%\n",
      "Epoch   5178 Step:      0 Loss:   0.1905 Training Acc:    92.14    92.14    92.14%\n",
      "Epoch   5179 Step:      0 Loss:   0.1903 Training Acc:    92.17    92.17    92.17%\n",
      "Epoch   5180 Step:      0 Loss:   0.1904 Training Acc:    92.18    92.18    92.18%\n",
      "Epoch   5181 Step:      0 Loss:   0.1905 Training Acc:    92.14    92.14    92.14%\n",
      "Epoch   5182 Step:      0 Loss:   0.1903 Training Acc:    92.19    92.19    92.19%\n",
      "Epoch   5183 Step:      0 Loss:   0.1902 Training Acc:    92.18    92.18    92.18%\n",
      "Epoch   5184 Step:      0 Loss:   0.1902 Training Acc:    92.17    92.17    92.17%\n",
      "Epoch   5185 Step:      0 Loss:   0.1903 Training Acc:    92.19    92.19    92.19%\n",
      "Epoch   5186 Step:      0 Loss:   0.1902 Training Acc:    92.18    92.18    92.18%\n",
      "Epoch   5187 Step:      0 Loss:   0.1901 Training Acc:    92.19    92.19    92.19%\n",
      "Epoch   5188 Step:      0 Loss:   0.1900 Training Acc:    92.19    92.19    92.19%\n",
      "Epoch   5189 Step:      0 Loss:   0.1901 Training Acc:    92.18    92.18    92.18%\n",
      "Epoch   5190 Step:      0 Loss:   0.1900 Training Acc:    92.20    92.20    92.20%\n",
      "Epoch   5191 Step:      0 Loss:   0.1899 Training Acc:    92.19    92.19    92.19%\n",
      "Epoch   5192 Step:      0 Loss:   0.1899 Training Acc:    92.20    92.20    92.20%\n",
      "Epoch   5193 Step:      0 Loss:   0.1899 Training Acc:    92.20    92.20    92.20%\n",
      "Epoch   5194 Step:      0 Loss:   0.1899 Training Acc:    92.20    92.20    92.20%\n",
      "Epoch   5195 Step:      0 Loss:   0.1898 Training Acc:    92.20    92.20    92.20%\n",
      "Epoch   5196 Step:      0 Loss:   0.1898 Training Acc:    92.20    92.20    92.20%\n",
      "Epoch   5197 Step:      0 Loss:   0.1898 Training Acc:    92.21    92.21    92.21%\n",
      "Epoch   5198 Step:      0 Loss:   0.1897 Training Acc:    92.20    92.20    92.20%\n",
      "Epoch   5199 Step:      0 Loss:   0.1897 Training Acc:    92.21    92.21    92.21%\n",
      "Epoch   5200 Step:      0 Loss:   0.1897 Training Acc:    92.21    92.21    92.21%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5201 Step:      0 Loss:   0.1896 Training Acc:    92.22    92.22    92.22%\n",
      "Epoch   5202 Step:      0 Loss:   0.1896 Training Acc:    92.21    92.21    92.21%\n",
      "Epoch   5203 Step:      0 Loss:   0.1896 Training Acc:    92.21    92.21    92.21%\n",
      "Epoch   5204 Step:      0 Loss:   0.1895 Training Acc:    92.22    92.22    92.22%\n",
      "Epoch   5205 Step:      0 Loss:   0.1895 Training Acc:    92.22    92.22    92.22%\n",
      "Epoch   5206 Step:      0 Loss:   0.1895 Training Acc:    92.22    92.22    92.22%\n",
      "Epoch   5207 Step:      0 Loss:   0.1895 Training Acc:    92.23    92.23    92.23%\n",
      "Epoch   5208 Step:      0 Loss:   0.1894 Training Acc:    92.22    92.22    92.22%\n",
      "Epoch   5209 Step:      0 Loss:   0.1894 Training Acc:    92.22    92.22    92.22%\n",
      "Epoch   5210 Step:      0 Loss:   0.1893 Training Acc:    92.23    92.23    92.23%\n",
      "Epoch   5211 Step:      0 Loss:   0.1893 Training Acc:    92.23    92.23    92.23%\n",
      "Epoch   5212 Step:      0 Loss:   0.1893 Training Acc:    92.23    92.23    92.23%\n",
      "Epoch   5213 Step:      0 Loss:   0.1893 Training Acc:    92.23    92.23    92.23%\n",
      "Epoch   5214 Step:      0 Loss:   0.1892 Training Acc:    92.23    92.23    92.23%\n",
      "Epoch   5215 Step:      0 Loss:   0.1892 Training Acc:    92.23    92.23    92.23%\n",
      "Epoch   5216 Step:      0 Loss:   0.1892 Training Acc:    92.24    92.24    92.24%\n",
      "Epoch   5217 Step:      0 Loss:   0.1891 Training Acc:    92.24    92.24    92.24%\n",
      "Epoch   5218 Step:      0 Loss:   0.1891 Training Acc:    92.24    92.24    92.24%\n",
      "Epoch   5219 Step:      0 Loss:   0.1891 Training Acc:    92.24    92.24    92.24%\n",
      "Epoch   5220 Step:      0 Loss:   0.1891 Training Acc:    92.24    92.24    92.24%\n",
      "Epoch   5221 Step:      0 Loss:   0.1890 Training Acc:    92.25    92.25    92.25%\n",
      "Epoch   5222 Step:      0 Loss:   0.1890 Training Acc:    92.24    92.24    92.24%\n",
      "Epoch   5223 Step:      0 Loss:   0.1890 Training Acc:    92.25    92.25    92.25%\n",
      "Epoch   5224 Step:      0 Loss:   0.1889 Training Acc:    92.24    92.24    92.24%\n",
      "Epoch   5225 Step:      0 Loss:   0.1889 Training Acc:    92.25    92.25    92.25%\n",
      "Epoch   5226 Step:      0 Loss:   0.1889 Training Acc:    92.25    92.25    92.25%\n",
      "Epoch   5227 Step:      0 Loss:   0.1888 Training Acc:    92.25    92.25    92.25%\n",
      "Epoch   5228 Step:      0 Loss:   0.1888 Training Acc:    92.25    92.25    92.25%\n",
      "Epoch   5229 Step:      0 Loss:   0.1888 Training Acc:    92.26    92.26    92.26%\n",
      "Epoch   5230 Step:      0 Loss:   0.1887 Training Acc:    92.25    92.25    92.25%\n",
      "Epoch   5231 Step:      0 Loss:   0.1887 Training Acc:    92.26    92.26    92.26%\n",
      "Epoch   5232 Step:      0 Loss:   0.1887 Training Acc:    92.26    92.26    92.26%\n",
      "Epoch   5233 Step:      0 Loss:   0.1887 Training Acc:    92.26    92.26    92.26%\n",
      "Epoch   5234 Step:      0 Loss:   0.1886 Training Acc:    92.26    92.26    92.26%\n",
      "Epoch   5235 Step:      0 Loss:   0.1886 Training Acc:    92.27    92.27    92.27%\n",
      "Epoch   5236 Step:      0 Loss:   0.1886 Training Acc:    92.27    92.27    92.27%\n",
      "Epoch   5237 Step:      0 Loss:   0.1885 Training Acc:    92.27    92.27    92.27%\n",
      "Epoch   5238 Step:      0 Loss:   0.1885 Training Acc:    92.27    92.27    92.27%\n",
      "Epoch   5239 Step:      0 Loss:   0.1885 Training Acc:    92.27    92.27    92.27%\n",
      "Epoch   5240 Step:      0 Loss:   0.1884 Training Acc:    92.27    92.27    92.27%\n",
      "Epoch   5241 Step:      0 Loss:   0.1884 Training Acc:    92.28    92.28    92.28%\n",
      "Epoch   5242 Step:      0 Loss:   0.1884 Training Acc:    92.28    92.28    92.28%\n",
      "Epoch   5243 Step:      0 Loss:   0.1884 Training Acc:    92.28    92.28    92.28%\n",
      "Epoch   5244 Step:      0 Loss:   0.1883 Training Acc:    92.28    92.28    92.28%\n",
      "Epoch   5245 Step:      0 Loss:   0.1883 Training Acc:    92.28    92.28    92.28%\n",
      "Epoch   5246 Step:      0 Loss:   0.1883 Training Acc:    92.29    92.29    92.29%\n",
      "Epoch   5247 Step:      0 Loss:   0.1882 Training Acc:    92.29    92.29    92.29%\n",
      "Epoch   5248 Step:      0 Loss:   0.1882 Training Acc:    92.29    92.29    92.29%\n",
      "Epoch   5249 Step:      0 Loss:   0.1882 Training Acc:    92.29    92.29    92.29%\n",
      "Epoch   5250 Step:      0 Loss:   0.1881 Training Acc:    92.29    92.29    92.29%\n",
      "Epoch   5251 Step:      0 Loss:   0.1881 Training Acc:    92.29    92.29    92.29%\n",
      "Epoch   5252 Step:      0 Loss:   0.1881 Training Acc:    92.30    92.30    92.30%\n",
      "Epoch   5253 Step:      0 Loss:   0.1881 Training Acc:    92.30    92.30    92.30%\n",
      "Epoch   5254 Step:      0 Loss:   0.1880 Training Acc:    92.30    92.30    92.30%\n",
      "Epoch   5255 Step:      0 Loss:   0.1880 Training Acc:    92.30    92.30    92.30%\n",
      "Epoch   5256 Step:      0 Loss:   0.1880 Training Acc:    92.30    92.30    92.30%\n",
      "Epoch   5257 Step:      0 Loss:   0.1879 Training Acc:    92.30    92.30    92.30%\n",
      "Epoch   5258 Step:      0 Loss:   0.1879 Training Acc:    92.31    92.31    92.31%\n",
      "Epoch   5259 Step:      0 Loss:   0.1879 Training Acc:    92.31    92.31    92.31%\n",
      "Epoch   5260 Step:      0 Loss:   0.1878 Training Acc:    92.31    92.31    92.31%\n",
      "Epoch   5261 Step:      0 Loss:   0.1878 Training Acc:    92.31    92.31    92.31%\n",
      "Epoch   5262 Step:      0 Loss:   0.1878 Training Acc:    92.31    92.31    92.31%\n",
      "Epoch   5263 Step:      0 Loss:   0.1878 Training Acc:    92.31    92.31    92.31%\n",
      "Epoch   5264 Step:      0 Loss:   0.1877 Training Acc:    92.31    92.31    92.31%\n",
      "Epoch   5265 Step:      0 Loss:   0.1877 Training Acc:    92.32    92.32    92.32%\n",
      "Epoch   5266 Step:      0 Loss:   0.1877 Training Acc:    92.32    92.32    92.32%\n",
      "Epoch   5267 Step:      0 Loss:   0.1876 Training Acc:    92.32    92.32    92.32%\n",
      "Epoch   5268 Step:      0 Loss:   0.1876 Training Acc:    92.32    92.32    92.32%\n",
      "Epoch   5269 Step:      0 Loss:   0.1876 Training Acc:    92.32    92.32    92.32%\n",
      "Epoch   5270 Step:      0 Loss:   0.1875 Training Acc:    92.32    92.32    92.32%\n",
      "Epoch   5271 Step:      0 Loss:   0.1875 Training Acc:    92.33    92.33    92.33%\n",
      "Epoch   5272 Step:      0 Loss:   0.1875 Training Acc:    92.33    92.33    92.33%\n",
      "Epoch   5273 Step:      0 Loss:   0.1875 Training Acc:    92.33    92.33    92.33%\n",
      "Epoch   5274 Step:      0 Loss:   0.1874 Training Acc:    92.33    92.33    92.33%\n",
      "Epoch   5275 Step:      0 Loss:   0.1874 Training Acc:    92.34    92.34    92.34%\n",
      "Epoch   5276 Step:      0 Loss:   0.1874 Training Acc:    92.34    92.34    92.34%\n",
      "Epoch   5277 Step:      0 Loss:   0.1873 Training Acc:    92.34    92.34    92.34%\n",
      "Epoch   5278 Step:      0 Loss:   0.1873 Training Acc:    92.34    92.34    92.34%\n",
      "Epoch   5279 Step:      0 Loss:   0.1873 Training Acc:    92.34    92.34    92.34%\n",
      "Epoch   5280 Step:      0 Loss:   0.1872 Training Acc:    92.34    92.34    92.34%\n",
      "Epoch   5281 Step:      0 Loss:   0.1872 Training Acc:    92.34    92.34    92.34%\n",
      "Epoch   5282 Step:      0 Loss:   0.1872 Training Acc:    92.34    92.34    92.34%\n",
      "Epoch   5283 Step:      0 Loss:   0.1871 Training Acc:    92.35    92.35    92.35%\n",
      "Epoch   5284 Step:      0 Loss:   0.1871 Training Acc:    92.35    92.35    92.35%\n",
      "Epoch   5285 Step:      0 Loss:   0.1871 Training Acc:    92.35    92.35    92.35%\n",
      "Epoch   5286 Step:      0 Loss:   0.1871 Training Acc:    92.35    92.35    92.35%\n",
      "Epoch   5287 Step:      0 Loss:   0.1870 Training Acc:    92.35    92.35    92.35%\n",
      "Epoch   5288 Step:      0 Loss:   0.1870 Training Acc:    92.35    92.35    92.35%\n",
      "Epoch   5289 Step:      0 Loss:   0.1870 Training Acc:    92.35    92.35    92.35%\n",
      "Epoch   5290 Step:      0 Loss:   0.1869 Training Acc:    92.35    92.35    92.35%\n",
      "Epoch   5291 Step:      0 Loss:   0.1869 Training Acc:    92.36    92.36    92.36%\n",
      "Epoch   5292 Step:      0 Loss:   0.1869 Training Acc:    92.36    92.36    92.36%\n",
      "Epoch   5293 Step:      0 Loss:   0.1868 Training Acc:    92.36    92.36    92.36%\n",
      "Epoch   5294 Step:      0 Loss:   0.1868 Training Acc:    92.36    92.36    92.36%\n",
      "Epoch   5295 Step:      0 Loss:   0.1868 Training Acc:    92.36    92.36    92.36%\n",
      "Epoch   5296 Step:      0 Loss:   0.1868 Training Acc:    92.36    92.36    92.36%\n",
      "Epoch   5297 Step:      0 Loss:   0.1867 Training Acc:    92.36    92.36    92.36%\n",
      "Epoch   5298 Step:      0 Loss:   0.1867 Training Acc:    92.36    92.36    92.36%\n",
      "Epoch   5299 Step:      0 Loss:   0.1867 Training Acc:    92.36    92.36    92.36%\n",
      "Epoch   5300 Step:      0 Loss:   0.1866 Training Acc:    92.36    92.36    92.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5301 Step:      0 Loss:   0.1866 Training Acc:    92.37    92.37    92.37%\n",
      "Epoch   5302 Step:      0 Loss:   0.1866 Training Acc:    92.37    92.37    92.37%\n",
      "Epoch   5303 Step:      0 Loss:   0.1865 Training Acc:    92.37    92.37    92.37%\n",
      "Epoch   5304 Step:      0 Loss:   0.1865 Training Acc:    92.37    92.37    92.37%\n",
      "Epoch   5305 Step:      0 Loss:   0.1865 Training Acc:    92.37    92.37    92.37%\n",
      "Epoch   5306 Step:      0 Loss:   0.1864 Training Acc:    92.37    92.37    92.37%\n",
      "Epoch   5307 Step:      0 Loss:   0.1864 Training Acc:    92.37    92.37    92.37%\n",
      "Epoch   5308 Step:      0 Loss:   0.1864 Training Acc:    92.37    92.37    92.37%\n",
      "Epoch   5309 Step:      0 Loss:   0.1864 Training Acc:    92.37    92.37    92.37%\n",
      "Epoch   5310 Step:      0 Loss:   0.1863 Training Acc:    92.38    92.38    92.38%\n",
      "Epoch   5311 Step:      0 Loss:   0.1863 Training Acc:    92.38    92.38    92.38%\n",
      "Epoch   5312 Step:      0 Loss:   0.1863 Training Acc:    92.38    92.38    92.38%\n",
      "Epoch   5313 Step:      0 Loss:   0.1862 Training Acc:    92.38    92.38    92.38%\n",
      "Epoch   5314 Step:      0 Loss:   0.1862 Training Acc:    92.38    92.38    92.38%\n",
      "Epoch   5315 Step:      0 Loss:   0.1862 Training Acc:    92.38    92.38    92.38%\n",
      "Epoch   5316 Step:      0 Loss:   0.1861 Training Acc:    92.39    92.39    92.39%\n",
      "Epoch   5317 Step:      0 Loss:   0.1861 Training Acc:    92.39    92.39    92.39%\n",
      "Epoch   5318 Step:      0 Loss:   0.1861 Training Acc:    92.39    92.39    92.39%\n",
      "Epoch   5319 Step:      0 Loss:   0.1860 Training Acc:    92.39    92.39    92.39%\n",
      "Epoch   5320 Step:      0 Loss:   0.1860 Training Acc:    92.39    92.39    92.39%\n",
      "Epoch   5321 Step:      0 Loss:   0.1860 Training Acc:    92.39    92.39    92.39%\n",
      "Epoch   5322 Step:      0 Loss:   0.1860 Training Acc:    92.39    92.39    92.39%\n",
      "Epoch   5323 Step:      0 Loss:   0.1859 Training Acc:    92.40    92.40    92.40%\n",
      "Epoch   5324 Step:      0 Loss:   0.1859 Training Acc:    92.40    92.40    92.40%\n",
      "Epoch   5325 Step:      0 Loss:   0.1859 Training Acc:    92.40    92.40    92.40%\n",
      "Epoch   5326 Step:      0 Loss:   0.1858 Training Acc:    92.40    92.40    92.40%\n",
      "Epoch   5327 Step:      0 Loss:   0.1858 Training Acc:    92.40    92.40    92.40%\n",
      "Epoch   5328 Step:      0 Loss:   0.1858 Training Acc:    92.40    92.40    92.40%\n",
      "Epoch   5329 Step:      0 Loss:   0.1857 Training Acc:    92.41    92.41    92.41%\n",
      "Epoch   5330 Step:      0 Loss:   0.1857 Training Acc:    92.41    92.41    92.41%\n",
      "Epoch   5331 Step:      0 Loss:   0.1857 Training Acc:    92.41    92.41    92.41%\n",
      "Epoch   5332 Step:      0 Loss:   0.1856 Training Acc:    92.41    92.41    92.41%\n",
      "Epoch   5333 Step:      0 Loss:   0.1856 Training Acc:    92.41    92.41    92.41%\n",
      "Epoch   5334 Step:      0 Loss:   0.1856 Training Acc:    92.41    92.41    92.41%\n",
      "Epoch   5335 Step:      0 Loss:   0.1856 Training Acc:    92.42    92.42    92.42%\n",
      "Epoch   5336 Step:      0 Loss:   0.1855 Training Acc:    92.42    92.42    92.42%\n",
      "Epoch   5337 Step:      0 Loss:   0.1855 Training Acc:    92.42    92.42    92.42%\n",
      "Epoch   5338 Step:      0 Loss:   0.1855 Training Acc:    92.42    92.42    92.42%\n",
      "Epoch   5339 Step:      0 Loss:   0.1854 Training Acc:    92.42    92.42    92.42%\n",
      "Epoch   5340 Step:      0 Loss:   0.1854 Training Acc:    92.42    92.42    92.42%\n",
      "Epoch   5341 Step:      0 Loss:   0.1854 Training Acc:    92.42    92.42    92.42%\n",
      "Epoch   5342 Step:      0 Loss:   0.1853 Training Acc:    92.42    92.42    92.42%\n",
      "Epoch   5343 Step:      0 Loss:   0.1853 Training Acc:    92.43    92.43    92.43%\n",
      "Epoch   5344 Step:      0 Loss:   0.1853 Training Acc:    92.43    92.43    92.43%\n",
      "Epoch   5345 Step:      0 Loss:   0.1852 Training Acc:    92.43    92.43    92.43%\n",
      "Epoch   5346 Step:      0 Loss:   0.1852 Training Acc:    92.43    92.43    92.43%\n",
      "Epoch   5347 Step:      0 Loss:   0.1852 Training Acc:    92.43    92.43    92.43%\n",
      "Epoch   5348 Step:      0 Loss:   0.1852 Training Acc:    92.43    92.43    92.43%\n",
      "Epoch   5349 Step:      0 Loss:   0.1851 Training Acc:    92.43    92.43    92.43%\n",
      "Epoch   5350 Step:      0 Loss:   0.1851 Training Acc:    92.44    92.44    92.44%\n",
      "Epoch   5351 Step:      0 Loss:   0.1851 Training Acc:    92.44    92.44    92.44%\n",
      "Epoch   5352 Step:      0 Loss:   0.1850 Training Acc:    92.44    92.44    92.44%\n",
      "Epoch   5353 Step:      0 Loss:   0.1850 Training Acc:    92.44    92.44    92.44%\n",
      "Epoch   5354 Step:      0 Loss:   0.1850 Training Acc:    92.45    92.45    92.45%\n",
      "Epoch   5355 Step:      0 Loss:   0.1849 Training Acc:    92.45    92.45    92.45%\n",
      "Epoch   5356 Step:      0 Loss:   0.1849 Training Acc:    92.45    92.45    92.45%\n",
      "Epoch   5357 Step:      0 Loss:   0.1849 Training Acc:    92.45    92.45    92.45%\n",
      "Epoch   5358 Step:      0 Loss:   0.1848 Training Acc:    92.45    92.45    92.45%\n",
      "Epoch   5359 Step:      0 Loss:   0.1848 Training Acc:    92.45    92.45    92.45%\n",
      "Epoch   5360 Step:      0 Loss:   0.1848 Training Acc:    92.46    92.46    92.46%\n",
      "Epoch   5361 Step:      0 Loss:   0.1848 Training Acc:    92.46    92.46    92.46%\n",
      "Epoch   5362 Step:      0 Loss:   0.1847 Training Acc:    92.46    92.46    92.46%\n",
      "Epoch   5363 Step:      0 Loss:   0.1847 Training Acc:    92.46    92.46    92.46%\n",
      "Epoch   5364 Step:      0 Loss:   0.1847 Training Acc:    92.46    92.46    92.46%\n",
      "Epoch   5365 Step:      0 Loss:   0.1846 Training Acc:    92.47    92.47    92.47%\n",
      "Epoch   5366 Step:      0 Loss:   0.1846 Training Acc:    92.47    92.47    92.47%\n",
      "Epoch   5367 Step:      0 Loss:   0.1846 Training Acc:    92.47    92.47    92.47%\n",
      "Epoch   5368 Step:      0 Loss:   0.1845 Training Acc:    92.47    92.47    92.47%\n",
      "Epoch   5369 Step:      0 Loss:   0.1845 Training Acc:    92.47    92.47    92.47%\n",
      "Epoch   5370 Step:      0 Loss:   0.1845 Training Acc:    92.47    92.47    92.47%\n",
      "Epoch   5371 Step:      0 Loss:   0.1844 Training Acc:    92.47    92.47    92.47%\n",
      "Epoch   5372 Step:      0 Loss:   0.1844 Training Acc:    92.48    92.48    92.48%\n",
      "Epoch   5373 Step:      0 Loss:   0.1844 Training Acc:    92.47    92.47    92.47%\n",
      "Epoch   5374 Step:      0 Loss:   0.1844 Training Acc:    92.48    92.48    92.48%\n",
      "Epoch   5375 Step:      0 Loss:   0.1843 Training Acc:    92.48    92.48    92.48%\n",
      "Epoch   5376 Step:      0 Loss:   0.1843 Training Acc:    92.48    92.48    92.48%\n",
      "Epoch   5377 Step:      0 Loss:   0.1843 Training Acc:    92.48    92.48    92.48%\n",
      "Epoch   5378 Step:      0 Loss:   0.1842 Training Acc:    92.49    92.49    92.49%\n",
      "Epoch   5379 Step:      0 Loss:   0.1842 Training Acc:    92.48    92.48    92.48%\n",
      "Epoch   5380 Step:      0 Loss:   0.1842 Training Acc:    92.49    92.49    92.49%\n",
      "Epoch   5381 Step:      0 Loss:   0.1842 Training Acc:    92.48    92.48    92.48%\n",
      "Epoch   5382 Step:      0 Loss:   0.1841 Training Acc:    92.49    92.49    92.49%\n",
      "Epoch   5383 Step:      0 Loss:   0.1841 Training Acc:    92.49    92.49    92.49%\n",
      "Epoch   5384 Step:      0 Loss:   0.1842 Training Acc:    92.48    92.48    92.48%\n",
      "Epoch   5385 Step:      0 Loss:   0.1842 Training Acc:    92.49    92.49    92.49%\n",
      "Epoch   5386 Step:      0 Loss:   0.1844 Training Acc:    92.44    92.44    92.44%\n",
      "Epoch   5387 Step:      0 Loss:   0.1847 Training Acc:    92.44    92.44    92.44%\n",
      "Epoch   5388 Step:      0 Loss:   0.1853 Training Acc:    92.35    92.35    92.35%\n",
      "Epoch   5389 Step:      0 Loss:   0.1863 Training Acc:    92.37    92.37    92.37%\n",
      "Epoch   5390 Step:      0 Loss:   0.1885 Training Acc:    92.10    92.10    92.10%\n",
      "Epoch   5391 Step:      0 Loss:   0.1920 Training Acc:    91.98    91.98    91.98%\n",
      "Epoch   5392 Step:      0 Loss:   0.1995 Training Acc:    91.36    91.36    91.36%\n",
      "Epoch   5393 Step:      0 Loss:   0.2095 Training Acc:    90.82    90.82    90.82%\n",
      "Epoch   5394 Step:      0 Loss:   0.2301 Training Acc:    89.54    89.54    89.54%\n",
      "Epoch   5395 Step:      0 Loss:   0.2425 Training Acc:    89.01    89.01    89.01%\n",
      "Epoch   5396 Step:      0 Loss:   0.2548 Training Acc:    88.58    88.58    88.58%\n",
      "Epoch   5397 Step:      0 Loss:   0.2158 Training Acc:    90.52    90.52    90.52%\n",
      "Epoch   5398 Step:      0 Loss:   0.1865 Training Acc:    92.31    92.31    92.31%\n",
      "Epoch   5399 Step:      0 Loss:   0.2077 Training Acc:    90.86    90.86    90.86%\n",
      "Epoch   5400 Step:      0 Loss:   0.2160 Training Acc:    90.48    90.48    90.48%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5401 Step:      0 Loss:   0.1949 Training Acc:    91.71    91.71    91.71%\n",
      "Epoch   5402 Step:      0 Loss:   0.1923 Training Acc:    91.89    91.89    91.89%\n",
      "Epoch   5403 Step:      0 Loss:   0.2077 Training Acc:    90.96    90.96    90.96%\n",
      "Epoch   5404 Step:      0 Loss:   0.1996 Training Acc:    91.38    91.38    91.38%\n",
      "Epoch   5405 Step:      0 Loss:   0.1916 Training Acc:    92.00    92.00    92.00%\n",
      "Epoch   5406 Step:      0 Loss:   0.2017 Training Acc:    91.29    91.29    91.29%\n",
      "Epoch   5407 Step:      0 Loss:   0.1993 Training Acc:    91.39    91.39    91.39%\n",
      "Epoch   5408 Step:      0 Loss:   0.1931 Training Acc:    91.87    91.87    91.87%\n",
      "Epoch   5409 Step:      0 Loss:   0.1943 Training Acc:    91.76    91.76    91.76%\n",
      "Epoch   5410 Step:      0 Loss:   0.1977 Training Acc:    91.54    91.54    91.54%\n",
      "Epoch   5411 Step:      0 Loss:   0.1919 Training Acc:    91.91    91.91    91.91%\n",
      "Epoch   5412 Step:      0 Loss:   0.1877 Training Acc:    92.23    92.23    92.23%\n",
      "Epoch   5413 Step:      0 Loss:   0.1964 Training Acc:    91.64    91.64    91.64%\n",
      "Epoch   5414 Step:      0 Loss:   0.1878 Training Acc:    92.17    92.17    92.17%\n",
      "Epoch   5415 Step:      0 Loss:   0.1878 Training Acc:    92.20    92.20    92.20%\n",
      "Epoch   5416 Step:      0 Loss:   0.1925 Training Acc:    91.88    91.88    91.88%\n",
      "Epoch   5417 Step:      0 Loss:   0.1858 Training Acc:    92.33    92.33    92.33%\n",
      "Epoch   5418 Step:      0 Loss:   0.1878 Training Acc:    92.21    92.21    92.21%\n",
      "Epoch   5419 Step:      0 Loss:   0.1884 Training Acc:    92.13    92.13    92.13%\n",
      "Epoch   5420 Step:      0 Loss:   0.1861 Training Acc:    92.33    92.33    92.33%\n",
      "Epoch   5421 Step:      0 Loss:   0.1859 Training Acc:    92.30    92.30    92.30%\n",
      "Epoch   5422 Step:      0 Loss:   0.1868 Training Acc:    92.22    92.22    92.22%\n",
      "Epoch   5423 Step:      0 Loss:   0.1859 Training Acc:    92.32    92.32    92.32%\n",
      "Epoch   5424 Step:      0 Loss:   0.1843 Training Acc:    92.40    92.40    92.40%\n",
      "Epoch   5425 Step:      0 Loss:   0.1867 Training Acc:    92.22    92.22    92.22%\n",
      "Epoch   5426 Step:      0 Loss:   0.1845 Training Acc:    92.43    92.43    92.43%\n",
      "Epoch   5427 Step:      0 Loss:   0.1843 Training Acc:    92.41    92.41    92.41%\n",
      "Epoch   5428 Step:      0 Loss:   0.1856 Training Acc:    92.31    92.31    92.31%\n",
      "Epoch   5429 Step:      0 Loss:   0.1840 Training Acc:    92.48    92.48    92.48%\n",
      "Epoch   5430 Step:      0 Loss:   0.1841 Training Acc:    92.47    92.47    92.47%\n",
      "Epoch   5431 Step:      0 Loss:   0.1843 Training Acc:    92.39    92.39    92.39%\n",
      "Epoch   5432 Step:      0 Loss:   0.1841 Training Acc:    92.45    92.45    92.45%\n",
      "Epoch   5433 Step:      0 Loss:   0.1833 Training Acc:    92.52    92.52    92.52%\n",
      "Epoch   5434 Step:      0 Loss:   0.1840 Training Acc:    92.42    92.42    92.42%\n",
      "Epoch   5435 Step:      0 Loss:   0.1836 Training Acc:    92.48    92.48    92.48%\n",
      "Epoch   5436 Step:      0 Loss:   0.1831 Training Acc:    92.51    92.51    92.51%\n",
      "Epoch   5437 Step:      0 Loss:   0.1837 Training Acc:    92.46    92.46    92.46%\n",
      "Epoch   5438 Step:      0 Loss:   0.1832 Training Acc:    92.52    92.52    92.52%\n",
      "Epoch   5439 Step:      0 Loss:   0.1830 Training Acc:    92.53    92.53    92.53%\n",
      "Epoch   5440 Step:      0 Loss:   0.1832 Training Acc:    92.49    92.49    92.49%\n",
      "Epoch   5441 Step:      0 Loss:   0.1831 Training Acc:    92.51    92.51    92.51%\n",
      "Epoch   5442 Step:      0 Loss:   0.1829 Training Acc:    92.54    92.54    92.54%\n",
      "Epoch   5443 Step:      0 Loss:   0.1829 Training Acc:    92.52    92.52    92.52%\n",
      "Epoch   5444 Step:      0 Loss:   0.1829 Training Acc:    92.52    92.52    92.52%\n",
      "Epoch   5445 Step:      0 Loss:   0.1827 Training Acc:    92.55    92.55    92.55%\n",
      "Epoch   5446 Step:      0 Loss:   0.1827 Training Acc:    92.53    92.53    92.53%\n",
      "Epoch   5447 Step:      0 Loss:   0.1828 Training Acc:    92.53    92.53    92.53%\n",
      "Epoch   5448 Step:      0 Loss:   0.1825 Training Acc:    92.56    92.56    92.56%\n",
      "Epoch   5449 Step:      0 Loss:   0.1826 Training Acc:    92.55    92.55    92.55%\n",
      "Epoch   5450 Step:      0 Loss:   0.1825 Training Acc:    92.55    92.55    92.55%\n",
      "Epoch   5451 Step:      0 Loss:   0.1825 Training Acc:    92.56    92.56    92.56%\n",
      "Epoch   5452 Step:      0 Loss:   0.1824 Training Acc:    92.56    92.56    92.56%\n",
      "Epoch   5453 Step:      0 Loss:   0.1824 Training Acc:    92.56    92.56    92.56%\n",
      "Epoch   5454 Step:      0 Loss:   0.1824 Training Acc:    92.56    92.56    92.56%\n",
      "Epoch   5455 Step:      0 Loss:   0.1823 Training Acc:    92.57    92.57    92.57%\n",
      "Epoch   5456 Step:      0 Loss:   0.1823 Training Acc:    92.56    92.56    92.56%\n",
      "Epoch   5457 Step:      0 Loss:   0.1822 Training Acc:    92.56    92.56    92.56%\n",
      "Epoch   5458 Step:      0 Loss:   0.1822 Training Acc:    92.58    92.58    92.58%\n",
      "Epoch   5459 Step:      0 Loss:   0.1822 Training Acc:    92.57    92.57    92.57%\n",
      "Epoch   5460 Step:      0 Loss:   0.1821 Training Acc:    92.57    92.57    92.57%\n",
      "Epoch   5461 Step:      0 Loss:   0.1821 Training Acc:    92.58    92.58    92.58%\n",
      "Epoch   5462 Step:      0 Loss:   0.1820 Training Acc:    92.58    92.58    92.58%\n",
      "Epoch   5463 Step:      0 Loss:   0.1820 Training Acc:    92.59    92.59    92.59%\n",
      "Epoch   5464 Step:      0 Loss:   0.1820 Training Acc:    92.58    92.58    92.58%\n",
      "Epoch   5465 Step:      0 Loss:   0.1819 Training Acc:    92.59    92.59    92.59%\n",
      "Epoch   5466 Step:      0 Loss:   0.1820 Training Acc:    92.59    92.59    92.59%\n",
      "Epoch   5467 Step:      0 Loss:   0.1819 Training Acc:    92.59    92.59    92.59%\n",
      "Epoch   5468 Step:      0 Loss:   0.1819 Training Acc:    92.60    92.60    92.60%\n",
      "Epoch   5469 Step:      0 Loss:   0.1818 Training Acc:    92.59    92.59    92.59%\n",
      "Epoch   5470 Step:      0 Loss:   0.1818 Training Acc:    92.59    92.59    92.59%\n",
      "Epoch   5471 Step:      0 Loss:   0.1818 Training Acc:    92.59    92.59    92.59%\n",
      "Epoch   5472 Step:      0 Loss:   0.1817 Training Acc:    92.59    92.59    92.59%\n",
      "Epoch   5473 Step:      0 Loss:   0.1817 Training Acc:    92.59    92.59    92.59%\n",
      "Epoch   5474 Step:      0 Loss:   0.1817 Training Acc:    92.60    92.60    92.60%\n",
      "Epoch   5475 Step:      0 Loss:   0.1817 Training Acc:    92.60    92.60    92.60%\n",
      "Epoch   5476 Step:      0 Loss:   0.1816 Training Acc:    92.60    92.60    92.60%\n",
      "Epoch   5477 Step:      0 Loss:   0.1816 Training Acc:    92.60    92.60    92.60%\n",
      "Epoch   5478 Step:      0 Loss:   0.1816 Training Acc:    92.60    92.60    92.60%\n",
      "Epoch   5479 Step:      0 Loss:   0.1816 Training Acc:    92.61    92.61    92.61%\n",
      "Epoch   5480 Step:      0 Loss:   0.1815 Training Acc:    92.60    92.60    92.60%\n",
      "Epoch   5481 Step:      0 Loss:   0.1815 Training Acc:    92.61    92.61    92.61%\n",
      "Epoch   5482 Step:      0 Loss:   0.1815 Training Acc:    92.61    92.61    92.61%\n",
      "Epoch   5483 Step:      0 Loss:   0.1814 Training Acc:    92.61    92.61    92.61%\n",
      "Epoch   5484 Step:      0 Loss:   0.1814 Training Acc:    92.61    92.61    92.61%\n",
      "Epoch   5485 Step:      0 Loss:   0.1814 Training Acc:    92.62    92.62    92.62%\n",
      "Epoch   5486 Step:      0 Loss:   0.1814 Training Acc:    92.62    92.62    92.62%\n",
      "Epoch   5487 Step:      0 Loss:   0.1813 Training Acc:    92.62    92.62    92.62%\n",
      "Epoch   5488 Step:      0 Loss:   0.1813 Training Acc:    92.62    92.62    92.62%\n",
      "Epoch   5489 Step:      0 Loss:   0.1813 Training Acc:    92.62    92.62    92.62%\n",
      "Epoch   5490 Step:      0 Loss:   0.1812 Training Acc:    92.62    92.62    92.62%\n",
      "Epoch   5491 Step:      0 Loss:   0.1812 Training Acc:    92.62    92.62    92.62%\n",
      "Epoch   5492 Step:      0 Loss:   0.1812 Training Acc:    92.62    92.62    92.62%\n",
      "Epoch   5493 Step:      0 Loss:   0.1812 Training Acc:    92.62    92.62    92.62%\n",
      "Epoch   5494 Step:      0 Loss:   0.1811 Training Acc:    92.63    92.63    92.63%\n",
      "Epoch   5495 Step:      0 Loss:   0.1811 Training Acc:    92.63    92.63    92.63%\n",
      "Epoch   5496 Step:      0 Loss:   0.1811 Training Acc:    92.63    92.63    92.63%\n",
      "Epoch   5497 Step:      0 Loss:   0.1811 Training Acc:    92.63    92.63    92.63%\n",
      "Epoch   5498 Step:      0 Loss:   0.1810 Training Acc:    92.63    92.63    92.63%\n",
      "Epoch   5499 Step:      0 Loss:   0.1810 Training Acc:    92.63    92.63    92.63%\n",
      "Epoch   5500 Step:      0 Loss:   0.1810 Training Acc:    92.63    92.63    92.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5501 Step:      0 Loss:   0.1809 Training Acc:    92.63    92.63    92.63%\n",
      "Epoch   5502 Step:      0 Loss:   0.1809 Training Acc:    92.63    92.63    92.63%\n",
      "Epoch   5503 Step:      0 Loss:   0.1809 Training Acc:    92.64    92.64    92.64%\n",
      "Epoch   5504 Step:      0 Loss:   0.1809 Training Acc:    92.64    92.64    92.64%\n",
      "Epoch   5505 Step:      0 Loss:   0.1808 Training Acc:    92.64    92.64    92.64%\n",
      "Epoch   5506 Step:      0 Loss:   0.1808 Training Acc:    92.64    92.64    92.64%\n",
      "Epoch   5507 Step:      0 Loss:   0.1808 Training Acc:    92.64    92.64    92.64%\n",
      "Epoch   5508 Step:      0 Loss:   0.1808 Training Acc:    92.64    92.64    92.64%\n",
      "Epoch   5509 Step:      0 Loss:   0.1807 Training Acc:    92.64    92.64    92.64%\n",
      "Epoch   5510 Step:      0 Loss:   0.1807 Training Acc:    92.64    92.64    92.64%\n",
      "Epoch   5511 Step:      0 Loss:   0.1807 Training Acc:    92.64    92.64    92.64%\n",
      "Epoch   5512 Step:      0 Loss:   0.1806 Training Acc:    92.65    92.65    92.65%\n",
      "Epoch   5513 Step:      0 Loss:   0.1806 Training Acc:    92.65    92.65    92.65%\n",
      "Epoch   5514 Step:      0 Loss:   0.1806 Training Acc:    92.65    92.65    92.65%\n",
      "Epoch   5515 Step:      0 Loss:   0.1806 Training Acc:    92.65    92.65    92.65%\n",
      "Epoch   5516 Step:      0 Loss:   0.1805 Training Acc:    92.65    92.65    92.65%\n",
      "Epoch   5517 Step:      0 Loss:   0.1805 Training Acc:    92.66    92.66    92.66%\n",
      "Epoch   5518 Step:      0 Loss:   0.1805 Training Acc:    92.66    92.66    92.66%\n",
      "Epoch   5519 Step:      0 Loss:   0.1805 Training Acc:    92.66    92.66    92.66%\n",
      "Epoch   5520 Step:      0 Loss:   0.1804 Training Acc:    92.66    92.66    92.66%\n",
      "Epoch   5521 Step:      0 Loss:   0.1804 Training Acc:    92.66    92.66    92.66%\n",
      "Epoch   5522 Step:      0 Loss:   0.1804 Training Acc:    92.67    92.67    92.67%\n",
      "Epoch   5523 Step:      0 Loss:   0.1803 Training Acc:    92.67    92.67    92.67%\n",
      "Epoch   5524 Step:      0 Loss:   0.1803 Training Acc:    92.67    92.67    92.67%\n",
      "Epoch   5525 Step:      0 Loss:   0.1803 Training Acc:    92.67    92.67    92.67%\n",
      "Epoch   5526 Step:      0 Loss:   0.1803 Training Acc:    92.67    92.67    92.67%\n",
      "Epoch   5527 Step:      0 Loss:   0.1802 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5528 Step:      0 Loss:   0.1802 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5529 Step:      0 Loss:   0.1802 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5530 Step:      0 Loss:   0.1802 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5531 Step:      0 Loss:   0.1801 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5532 Step:      0 Loss:   0.1801 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5533 Step:      0 Loss:   0.1801 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5534 Step:      0 Loss:   0.1801 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5535 Step:      0 Loss:   0.1800 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5536 Step:      0 Loss:   0.1800 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5537 Step:      0 Loss:   0.1800 Training Acc:    92.68    92.68    92.68%\n",
      "Epoch   5538 Step:      0 Loss:   0.1799 Training Acc:    92.69    92.69    92.69%\n",
      "Epoch   5539 Step:      0 Loss:   0.1799 Training Acc:    92.69    92.69    92.69%\n",
      "Epoch   5540 Step:      0 Loss:   0.1799 Training Acc:    92.69    92.69    92.69%\n",
      "Epoch   5541 Step:      0 Loss:   0.1799 Training Acc:    92.69    92.69    92.69%\n",
      "Epoch   5542 Step:      0 Loss:   0.1798 Training Acc:    92.69    92.69    92.69%\n",
      "Epoch   5543 Step:      0 Loss:   0.1798 Training Acc:    92.69    92.69    92.69%\n",
      "Epoch   5544 Step:      0 Loss:   0.1798 Training Acc:    92.69    92.69    92.69%\n",
      "Epoch   5545 Step:      0 Loss:   0.1798 Training Acc:    92.69    92.69    92.69%\n",
      "Epoch   5546 Step:      0 Loss:   0.1797 Training Acc:    92.69    92.69    92.69%\n",
      "Epoch   5547 Step:      0 Loss:   0.1797 Training Acc:    92.70    92.70    92.70%\n",
      "Epoch   5548 Step:      0 Loss:   0.1797 Training Acc:    92.70    92.70    92.70%\n",
      "Epoch   5549 Step:      0 Loss:   0.1796 Training Acc:    92.70    92.70    92.70%\n",
      "Epoch   5550 Step:      0 Loss:   0.1796 Training Acc:    92.70    92.70    92.70%\n",
      "Epoch   5551 Step:      0 Loss:   0.1796 Training Acc:    92.70    92.70    92.70%\n",
      "Epoch   5552 Step:      0 Loss:   0.1796 Training Acc:    92.70    92.70    92.70%\n",
      "Epoch   5553 Step:      0 Loss:   0.1795 Training Acc:    92.70    92.70    92.70%\n",
      "Epoch   5554 Step:      0 Loss:   0.1795 Training Acc:    92.70    92.70    92.70%\n",
      "Epoch   5555 Step:      0 Loss:   0.1795 Training Acc:    92.70    92.70    92.70%\n",
      "Epoch   5556 Step:      0 Loss:   0.1795 Training Acc:    92.71    92.71    92.71%\n",
      "Epoch   5557 Step:      0 Loss:   0.1794 Training Acc:    92.71    92.71    92.71%\n",
      "Epoch   5558 Step:      0 Loss:   0.1794 Training Acc:    92.71    92.71    92.71%\n",
      "Epoch   5559 Step:      0 Loss:   0.1794 Training Acc:    92.71    92.71    92.71%\n",
      "Epoch   5560 Step:      0 Loss:   0.1793 Training Acc:    92.71    92.71    92.71%\n",
      "Epoch   5561 Step:      0 Loss:   0.1793 Training Acc:    92.71    92.71    92.71%\n",
      "Epoch   5562 Step:      0 Loss:   0.1793 Training Acc:    92.72    92.72    92.72%\n",
      "Epoch   5563 Step:      0 Loss:   0.1793 Training Acc:    92.72    92.72    92.72%\n",
      "Epoch   5564 Step:      0 Loss:   0.1792 Training Acc:    92.72    92.72    92.72%\n",
      "Epoch   5565 Step:      0 Loss:   0.1792 Training Acc:    92.72    92.72    92.72%\n",
      "Epoch   5566 Step:      0 Loss:   0.1792 Training Acc:    92.72    92.72    92.72%\n",
      "Epoch   5567 Step:      0 Loss:   0.1791 Training Acc:    92.72    92.72    92.72%\n",
      "Epoch   5568 Step:      0 Loss:   0.1791 Training Acc:    92.73    92.73    92.73%\n",
      "Epoch   5569 Step:      0 Loss:   0.1791 Training Acc:    92.73    92.73    92.73%\n",
      "Epoch   5570 Step:      0 Loss:   0.1791 Training Acc:    92.73    92.73    92.73%\n",
      "Epoch   5571 Step:      0 Loss:   0.1790 Training Acc:    92.73    92.73    92.73%\n",
      "Epoch   5572 Step:      0 Loss:   0.1790 Training Acc:    92.73    92.73    92.73%\n",
      "Epoch   5573 Step:      0 Loss:   0.1790 Training Acc:    92.73    92.73    92.73%\n",
      "Epoch   5574 Step:      0 Loss:   0.1790 Training Acc:    92.73    92.73    92.73%\n",
      "Epoch   5575 Step:      0 Loss:   0.1789 Training Acc:    92.74    92.74    92.74%\n",
      "Epoch   5576 Step:      0 Loss:   0.1789 Training Acc:    92.74    92.74    92.74%\n",
      "Epoch   5577 Step:      0 Loss:   0.1789 Training Acc:    92.74    92.74    92.74%\n",
      "Epoch   5578 Step:      0 Loss:   0.1788 Training Acc:    92.74    92.74    92.74%\n",
      "Epoch   5579 Step:      0 Loss:   0.1788 Training Acc:    92.74    92.74    92.74%\n",
      "Epoch   5580 Step:      0 Loss:   0.1788 Training Acc:    92.74    92.74    92.74%\n",
      "Epoch   5581 Step:      0 Loss:   0.1788 Training Acc:    92.75    92.75    92.75%\n",
      "Epoch   5582 Step:      0 Loss:   0.1787 Training Acc:    92.75    92.75    92.75%\n",
      "Epoch   5583 Step:      0 Loss:   0.1787 Training Acc:    92.75    92.75    92.75%\n",
      "Epoch   5584 Step:      0 Loss:   0.1787 Training Acc:    92.75    92.75    92.75%\n",
      "Epoch   5585 Step:      0 Loss:   0.1787 Training Acc:    92.75    92.75    92.75%\n",
      "Epoch   5586 Step:      0 Loss:   0.1786 Training Acc:    92.75    92.75    92.75%\n",
      "Epoch   5587 Step:      0 Loss:   0.1786 Training Acc:    92.76    92.76    92.76%\n",
      "Epoch   5588 Step:      0 Loss:   0.1786 Training Acc:    92.76    92.76    92.76%\n",
      "Epoch   5589 Step:      0 Loss:   0.1785 Training Acc:    92.76    92.76    92.76%\n",
      "Epoch   5590 Step:      0 Loss:   0.1785 Training Acc:    92.76    92.76    92.76%\n",
      "Epoch   5591 Step:      0 Loss:   0.1785 Training Acc:    92.77    92.77    92.77%\n",
      "Epoch   5592 Step:      0 Loss:   0.1785 Training Acc:    92.77    92.77    92.77%\n",
      "Epoch   5593 Step:      0 Loss:   0.1784 Training Acc:    92.77    92.77    92.77%\n",
      "Epoch   5594 Step:      0 Loss:   0.1784 Training Acc:    92.77    92.77    92.77%\n",
      "Epoch   5595 Step:      0 Loss:   0.1784 Training Acc:    92.77    92.77    92.77%\n",
      "Epoch   5596 Step:      0 Loss:   0.1784 Training Acc:    92.77    92.77    92.77%\n",
      "Epoch   5597 Step:      0 Loss:   0.1783 Training Acc:    92.77    92.77    92.77%\n",
      "Epoch   5598 Step:      0 Loss:   0.1783 Training Acc:    92.77    92.77    92.77%\n",
      "Epoch   5599 Step:      0 Loss:   0.1783 Training Acc:    92.78    92.78    92.78%\n",
      "Epoch   5600 Step:      0 Loss:   0.1782 Training Acc:    92.78    92.78    92.78%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5601 Step:      0 Loss:   0.1782 Training Acc:    92.78    92.78    92.78%\n",
      "Epoch   5602 Step:      0 Loss:   0.1782 Training Acc:    92.78    92.78    92.78%\n"
     ]
    }
   ],
   "source": [
    "# MINIBATCH GRADIENT DESCENT\n",
    "loss_history = []\n",
    "num_steps = int(iteration_one_epoch*50)\n",
    "\n",
    "acc = 0\n",
    "acc_cum = 0\n",
    "acc_temp = 0\n",
    "epoch_idx = 0\n",
    "\n",
    "with tf.Session() as session:\n",
    "#with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    while acc<100:\n",
    "        epoch_idx += 1\n",
    "        acc_cum = 0\n",
    "        for step in range(num_batches):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset_bin[offset:(offset + batch_size), :]\n",
    "            # batch_labels = train_label_hot[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_label_bin[offset:(offset + batch_size), :]\n",
    "            \n",
    "            # batch_labels = train_label_hot[:upper_limit]\n",
    "            # batch_data = train_dataset_reform\n",
    "            # batch_labels = train_label_hot\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,  keep_prob : 1}\n",
    "            # feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions, added_summary = session.run(\n",
    "                [optimizer, loss, train_prediction, loss_summary], feed_dict=feed_dict)\n",
    "\n",
    "            # writer.add_summary(added_summary)\n",
    "            loss_history.append(l)\n",
    "            acc_temp =  accuracy_multilabel(predictions, batch_labels)\n",
    "            acc_cum += acc_temp\n",
    "            acc = acc_cum/(step+1)\n",
    "            print(\"Epoch {0:6d} Step: {1:6d} Loss: {2:8.4f} Training Acc: {3:8.2f} {4:8.2f} {5:8.2f}%\".format(epoch_idx, step, l, acc_temp, acc_cum, acc))\n",
    "\n",
    "            # print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            # print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 1081,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 1082,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 1083,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161206"
      ]
     },
     "execution_count": 1084,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((np.round(predictions) == np.round(batch_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174240"
      ]
     },
     "execution_count": 1085,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape[0]*predictions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
