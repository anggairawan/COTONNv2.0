{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "import random\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plot param\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (22.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_scots(controller_filename, deterministic = False):\n",
    "    f = open(controller_filename, \"r\")\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        if '#MATRIX:DATA\\n' in line:                \n",
    "            for line in f: # now you are at the lines you want\n",
    "                # skip the #BEGIN \n",
    "                # read the state-actions\n",
    "                lines = f.readlines()\n",
    "    del lines[-1] # delete #END\n",
    "    # take the state as the train dataset\n",
    "    list_train_dataset = []\n",
    "    for x in lines:\n",
    "        list_train_dataset.append(x.split(' ')[0])\n",
    "    # convert to the numpy array with float32 data type\n",
    "    train_dataset = np.asarray(list_train_dataset)\n",
    "    train_dataset = train_dataset.astype(np.float32)\n",
    "    # take action/label pair of the state \n",
    "    # take the action(s) [column 1:-1] / the rest of the integer except the state\n",
    "    list_train_label = []\n",
    "    for x in lines:\n",
    "        if deterministic == False:\n",
    "            list_train_label.append(x.strip().split()[1:])\n",
    "        else:\n",
    "            list_train_label.append(x.strip().split()[1])\n",
    "    # convert to numpy array, note that the result is still not in one hot encoding format\n",
    "    train_label = np.asarray(list_train_label)\n",
    "    # define number of samples\n",
    "    num_samples = train_label.shape[0]\n",
    "    if deterministic == False:\n",
    "        # select to use ND or D case here\n",
    "        # create now array to be filled by the encoded label\n",
    "        train_label_hot = np.zeros([num_samples,2], dtype=np.float32)\n",
    "        # encode label to one hot encoding format\n",
    "        for i in range(num_samples):\n",
    "            if train_label[i] == ['0']:\n",
    "                train_label_hot[i] = [1, 0]\n",
    "            elif train_label[i] == ['1']:\n",
    "                train_label_hot[i] = [0, 1]\n",
    "            elif train_label[i] == ['0','1']:\n",
    "                train_label_hot[i] = [1, 1]\n",
    "    else:\n",
    "        train_label_hot = train_label[:, None].astype(np.uint8)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the controller file\n",
    "f = open('../COTONN/dcdc_small/controller.scs', \"r\")\n",
    "\n",
    "lines = []\n",
    "\n",
    "for line in f:\n",
    "    if '#MATRIX:DATA\\n' in line:                \n",
    "        for line in f: # now you are at the lines you want\n",
    "            # skip the #BEGIN \n",
    "            # read the state-actions\n",
    "            lines = f.readlines()\n",
    "            \n",
    "del lines[-1]\n",
    "\n",
    "# take the state as the train dataset\n",
    "ltrain_dataset = []\n",
    "for x in lines:\n",
    "    ltrain_dataset.append(x.split(' ')[0])\n",
    "# del ltrain_dataset[-1] # delete the string #END at the end of the file \n",
    "\n",
    "# convert to the numpy array with float32 data type\n",
    "train_dataset = np.asarray(ltrain_dataset)\n",
    "train_dataset = train_dataset.astype(np.float32)\n",
    "\n",
    "# take action/label pair of the state \n",
    "# take the action(s) [column 1:-1] / the rest of the integer except the state\n",
    "ltrain_label = []\n",
    "for x in lines:\n",
    "    if det == False:\n",
    "        ltrain_label.append(x.strip().split()[1:])\n",
    "    else:\n",
    "        ltrain_label.append(x.strip().split()[1])\n",
    "# del ltrain_label[-1] # delete the string #END at the end of the file  \n",
    "\n",
    "# convert to numpy array, note that the result is still not in one hot encoding format\n",
    "train_label = np.asarray(ltrain_label)\n",
    "\n",
    "# define number of samples\n",
    "num_samples = train_dataset.shape[0]\n",
    "\n",
    "if det == False:\n",
    "    # select to use ND or D case here\n",
    "    # create now array to be filled by the encoded label\n",
    "    train_label_hot = np.zeros([num_samples,2], dtype=np.float32)\n",
    "\n",
    "    # encode label to one hot encoding format\n",
    "    for i in range(num_samples):\n",
    "        if train_label[i] == ['0']:\n",
    "            train_label_hot[i] = [1, 0]\n",
    "        elif train_label[i] == ['1']:\n",
    "            train_label_hot[i] = [0, 1]\n",
    "        elif train_label[i] == ['0','1']:\n",
    "            train_label_hot[i] = [1, 1]\n",
    "else:\n",
    "    train_label_hot = train_label[:, None].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(ltrain_label,key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8677, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8677"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_limit = train_dataset.shape[0]\n",
    "upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dim = train_dataset[:, None]\n",
    "# slice the samples\n",
    "train_sliced = train_dataset_dim[:upper_limit]\n",
    "# change it to init\n",
    "train_sliced_int = train_sliced.astype(np.uint32)\n",
    "# split to 4 bytes\n",
    "tsi8_unordered = train_sliced_int.view(np.uint8)\n",
    "# little endian format\n",
    "tsi8 = np.flip(tsi8_unordered,1)\n",
    "# unpack\n",
    "toy_vehicle_input_unreduced = np.unpackbits(tsi8).reshape(-1,32)\n",
    "\n",
    "# get index of MSB\n",
    "msb = len(bin(int(np.max(train_dataset_dim))))-2\n",
    "# get total bit\n",
    "total_bit = toy_vehicle_input_unreduced.shape[1]\n",
    "# reduce to the minimal binary representation\n",
    "toy_vehicle_input = toy_vehicle_input_unreduced[:, np.arange(total_bit-msb,total_bit)]\n",
    "\n",
    "num_label = train_label_hot.shape[1]\n",
    "\n",
    "input_size = toy_vehicle_input.shape[1]\n",
    "\n",
    "train_dataset_reform = toy_vehicle_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_softmax(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def accuracy_multilabel(predictions, labels):\n",
    "  return (100.0 * np.mean(predictions == labels))\n",
    "\n",
    "def accuracy_multilabel_softmax(predictions, labels):\n",
    "    predictions_soft = np.zeros_like(predictions)\n",
    "    predictions_soft[np.arange(len(predictions)), predictions.argmax(1)] = 1\n",
    "    return (100.0*np.mean(labels[np.arange(len(labels)), predictions_soft.argmax(1)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 128 # num_samples # (num_samples//20) # 113*6 # num_samples\n",
    "num_batches = np.ceil(num_samples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "68.0\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 8\n",
    "hidden_size2 = 8\n",
    "\n",
    "# with tf.device('/device:GPU:0'):\n",
    "# Input data. For the training data, we use a placeholder that will be fed\n",
    "# at run time with a training minibatch.\n",
    "# tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, input_size))\n",
    "# tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_label))\n",
    "tf_train_dataset = tf.placeholder(tf.float32, shape=(None, input_size))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_label))\n",
    "dropout_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Variables.\n",
    "weights1 = tf.Variable(tf.truncated_normal([input_size, hidden_size]))\n",
    "biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "# weights2 = tf.Variable(tf.truncated_normal([hidden_size, num_label]))\n",
    "# biases2 = tf.Variable(tf.zeros([num_label]))\n",
    "\n",
    "weights2 = tf.Variable(tf.truncated_normal([hidden_size, hidden_size2]))\n",
    "biases2 = tf.Variable(tf.zeros([hidden_size2]))\n",
    "\n",
    "weights3 = tf.Variable(tf.truncated_normal([hidden_size2, num_label]))\n",
    "biases3 = tf.Variable(tf.zeros([num_label]))\n",
    "\n",
    "\"\"\"\n",
    "# weights4 = tf.Variable(tf.truncated_normal([hidden_size3, num_label]))\n",
    "# biases4 = tf.Variable(tf.zeros([num_label]))\n",
    "weights4 = tf.Variable(tf.truncated_normal([hidden_size3, hidden_size4]))\n",
    "biases4 = tf.Variable(tf.zeros([hidden_size4]))\n",
    "\n",
    "weights5 = tf.Variable(tf.truncated_normal([hidden_size4, num_label]))\n",
    "biases5 = tf.Variable(tf.zeros([num_label]))\n",
    "\"\"\"\n",
    "\n",
    "# Training computation.\n",
    "logits1 = tf.matmul(tf_train_dataset, weights1) + biases1  \n",
    "relu_act_func_d = tf.nn.sigmoid(logits1)\n",
    "relu_act_func = tf.nn.dropout(relu_act_func_d, dropout_prob)\n",
    "\n",
    "logits2 = tf.matmul(relu_act_func, weights2) + biases2\n",
    "relu_act_func2_d = tf.nn.sigmoid(logits2)\n",
    "relu_act_func2 = tf.nn.dropout(relu_act_func2_d, dropout_prob)\n",
    "\n",
    "logits3 = tf.matmul(relu_act_func2, weights3) + biases3\n",
    "\n",
    "\"\"\"\n",
    "relu_act_func3 = tf.nn.relu(logits3)\n",
    "logits4 = tf.matmul(relu_act_func3, weights4) + biases4\n",
    "\n",
    "relu_act_func4 = tf.nn.relu(logits4)\n",
    "logits5 = tf.matmul(relu_act_func4, weights5) + biases5\n",
    "\"\"\"\n",
    "\n",
    "logits = logits3\n",
    "# train_prediction = tf.round(tf.sigmoid(logits))\n",
    "train_prediction = (tf.nn.softmax(logits))\n",
    "\n",
    "# loss calculation\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(0.0015).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Epoch      1 batch_limit:   8677 Loss:   0.3238 Training Acc:    96.04  5592.91    82.25%\n",
      "Epoch      2 batch_limit:   8677 Loss:   0.3400 Training Acc:    84.16  5828.69    85.72%\n",
      "Epoch      3 batch_limit:   8677 Loss:   0.3622 Training Acc:    84.16  6501.35    95.61%\n",
      "Epoch      4 batch_limit:   8677 Loss:   0.3800 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch      5 batch_limit:   8677 Loss:   0.3920 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch      6 batch_limit:   8677 Loss:   0.3995 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch      7 batch_limit:   8677 Loss:   0.4039 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch      8 batch_limit:   8677 Loss:   0.4064 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch      9 batch_limit:   8677 Loss:   0.4077 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     10 batch_limit:   8677 Loss:   0.4082 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     11 batch_limit:   8677 Loss:   0.4082 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     12 batch_limit:   8677 Loss:   0.4080 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     13 batch_limit:   8677 Loss:   0.4077 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     14 batch_limit:   8677 Loss:   0.4073 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     15 batch_limit:   8677 Loss:   0.4068 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     16 batch_limit:   8677 Loss:   0.4064 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     17 batch_limit:   8677 Loss:   0.4060 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     18 batch_limit:   8677 Loss:   0.4056 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     19 batch_limit:   8677 Loss:   0.4052 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     20 batch_limit:   8677 Loss:   0.4048 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     21 batch_limit:   8677 Loss:   0.4044 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     22 batch_limit:   8677 Loss:   0.4040 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     23 batch_limit:   8677 Loss:   0.4036 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     24 batch_limit:   8677 Loss:   0.4033 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     25 batch_limit:   8677 Loss:   0.4029 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     26 batch_limit:   8677 Loss:   0.4026 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     27 batch_limit:   8677 Loss:   0.4022 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     28 batch_limit:   8677 Loss:   0.4019 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     29 batch_limit:   8677 Loss:   0.4015 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     30 batch_limit:   8677 Loss:   0.4012 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     31 batch_limit:   8677 Loss:   0.4008 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     32 batch_limit:   8677 Loss:   0.4005 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     33 batch_limit:   8677 Loss:   0.4001 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     34 batch_limit:   8677 Loss:   0.3998 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     35 batch_limit:   8677 Loss:   0.3994 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     36 batch_limit:   8677 Loss:   0.3991 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     37 batch_limit:   8677 Loss:   0.3987 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     38 batch_limit:   8677 Loss:   0.3983 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     39 batch_limit:   8677 Loss:   0.3979 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     40 batch_limit:   8677 Loss:   0.3976 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     41 batch_limit:   8677 Loss:   0.3972 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     42 batch_limit:   8677 Loss:   0.3968 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     43 batch_limit:   8677 Loss:   0.3964 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     44 batch_limit:   8677 Loss:   0.3960 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     45 batch_limit:   8677 Loss:   0.3957 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     46 batch_limit:   8677 Loss:   0.3953 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     47 batch_limit:   8677 Loss:   0.3949 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     48 batch_limit:   8677 Loss:   0.3945 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     49 batch_limit:   8677 Loss:   0.3941 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     50 batch_limit:   8677 Loss:   0.3938 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     51 batch_limit:   8677 Loss:   0.3934 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     52 batch_limit:   8677 Loss:   0.3930 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     53 batch_limit:   8677 Loss:   0.3926 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     54 batch_limit:   8677 Loss:   0.3923 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     55 batch_limit:   8677 Loss:   0.3919 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     56 batch_limit:   8677 Loss:   0.3915 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     57 batch_limit:   8677 Loss:   0.3912 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     58 batch_limit:   8677 Loss:   0.3908 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     59 batch_limit:   8677 Loss:   0.3904 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     60 batch_limit:   8677 Loss:   0.3901 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     61 batch_limit:   8677 Loss:   0.3897 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     62 batch_limit:   8677 Loss:   0.3894 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     63 batch_limit:   8677 Loss:   0.3890 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     64 batch_limit:   8677 Loss:   0.3887 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     65 batch_limit:   8677 Loss:   0.3883 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     66 batch_limit:   8677 Loss:   0.3880 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     67 batch_limit:   8677 Loss:   0.3876 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     68 batch_limit:   8677 Loss:   0.3873 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     69 batch_limit:   8677 Loss:   0.3869 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     70 batch_limit:   8677 Loss:   0.3866 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     71 batch_limit:   8677 Loss:   0.3862 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     72 batch_limit:   8677 Loss:   0.3859 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     73 batch_limit:   8677 Loss:   0.3856 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     74 batch_limit:   8677 Loss:   0.3852 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     75 batch_limit:   8677 Loss:   0.3849 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     76 batch_limit:   8677 Loss:   0.3845 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     77 batch_limit:   8677 Loss:   0.3842 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     78 batch_limit:   8677 Loss:   0.3839 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     79 batch_limit:   8677 Loss:   0.3836 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     80 batch_limit:   8677 Loss:   0.3832 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     81 batch_limit:   8677 Loss:   0.3829 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     82 batch_limit:   8677 Loss:   0.3826 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     83 batch_limit:   8677 Loss:   0.3822 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     84 batch_limit:   8677 Loss:   0.3819 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     85 batch_limit:   8677 Loss:   0.3816 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     86 batch_limit:   8677 Loss:   0.3813 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     87 batch_limit:   8677 Loss:   0.3810 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     88 batch_limit:   8677 Loss:   0.3807 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     89 batch_limit:   8677 Loss:   0.3803 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     90 batch_limit:   8677 Loss:   0.3800 Training Acc:    84.16  6525.56    95.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     91 batch_limit:   8677 Loss:   0.3797 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     92 batch_limit:   8677 Loss:   0.3794 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     93 batch_limit:   8677 Loss:   0.3791 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     94 batch_limit:   8677 Loss:   0.3788 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     95 batch_limit:   8677 Loss:   0.3785 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     96 batch_limit:   8677 Loss:   0.3782 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     97 batch_limit:   8677 Loss:   0.3779 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     98 batch_limit:   8677 Loss:   0.3776 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch     99 batch_limit:   8677 Loss:   0.3773 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    100 batch_limit:   8677 Loss:   0.3770 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    101 batch_limit:   8677 Loss:   0.3767 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    102 batch_limit:   8677 Loss:   0.3764 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    103 batch_limit:   8677 Loss:   0.3762 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    104 batch_limit:   8677 Loss:   0.3759 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    105 batch_limit:   8677 Loss:   0.3756 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    106 batch_limit:   8677 Loss:   0.3753 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    107 batch_limit:   8677 Loss:   0.3750 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    108 batch_limit:   8677 Loss:   0.3748 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    109 batch_limit:   8677 Loss:   0.3745 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    110 batch_limit:   8677 Loss:   0.3742 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    111 batch_limit:   8677 Loss:   0.3739 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    112 batch_limit:   8677 Loss:   0.3737 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    113 batch_limit:   8677 Loss:   0.3734 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    114 batch_limit:   8677 Loss:   0.3731 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    115 batch_limit:   8677 Loss:   0.3729 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    116 batch_limit:   8677 Loss:   0.3726 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    117 batch_limit:   8677 Loss:   0.3723 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    118 batch_limit:   8677 Loss:   0.3721 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    119 batch_limit:   8677 Loss:   0.3718 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    120 batch_limit:   8677 Loss:   0.3715 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    121 batch_limit:   8677 Loss:   0.3713 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    122 batch_limit:   8677 Loss:   0.3710 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    123 batch_limit:   8677 Loss:   0.3708 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    124 batch_limit:   8677 Loss:   0.3705 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    125 batch_limit:   8677 Loss:   0.3703 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    126 batch_limit:   8677 Loss:   0.3700 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    127 batch_limit:   8677 Loss:   0.3698 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    128 batch_limit:   8677 Loss:   0.3695 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    129 batch_limit:   8677 Loss:   0.3693 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    130 batch_limit:   8677 Loss:   0.3690 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    131 batch_limit:   8677 Loss:   0.3688 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    132 batch_limit:   8677 Loss:   0.3685 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    133 batch_limit:   8677 Loss:   0.3683 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    134 batch_limit:   8677 Loss:   0.3680 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    135 batch_limit:   8677 Loss:   0.3678 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    136 batch_limit:   8677 Loss:   0.3675 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    137 batch_limit:   8677 Loss:   0.3673 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    138 batch_limit:   8677 Loss:   0.3670 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    139 batch_limit:   8677 Loss:   0.3668 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    140 batch_limit:   8677 Loss:   0.3665 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    141 batch_limit:   8677 Loss:   0.3663 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    142 batch_limit:   8677 Loss:   0.3660 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    143 batch_limit:   8677 Loss:   0.3658 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    144 batch_limit:   8677 Loss:   0.3655 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    145 batch_limit:   8677 Loss:   0.3653 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    146 batch_limit:   8677 Loss:   0.3650 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    147 batch_limit:   8677 Loss:   0.3647 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    148 batch_limit:   8677 Loss:   0.3645 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    149 batch_limit:   8677 Loss:   0.3642 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    150 batch_limit:   8677 Loss:   0.3640 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    151 batch_limit:   8677 Loss:   0.3637 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    152 batch_limit:   8677 Loss:   0.3635 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    153 batch_limit:   8677 Loss:   0.3632 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    154 batch_limit:   8677 Loss:   0.3629 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    155 batch_limit:   8677 Loss:   0.3627 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    156 batch_limit:   8677 Loss:   0.3624 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    157 batch_limit:   8677 Loss:   0.3621 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    158 batch_limit:   8677 Loss:   0.3618 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    159 batch_limit:   8677 Loss:   0.3616 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    160 batch_limit:   8677 Loss:   0.3613 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    161 batch_limit:   8677 Loss:   0.3610 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    162 batch_limit:   8677 Loss:   0.3607 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    163 batch_limit:   8677 Loss:   0.3604 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    164 batch_limit:   8677 Loss:   0.3601 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    165 batch_limit:   8677 Loss:   0.3598 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    166 batch_limit:   8677 Loss:   0.3595 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    167 batch_limit:   8677 Loss:   0.3592 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    168 batch_limit:   8677 Loss:   0.3589 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    169 batch_limit:   8677 Loss:   0.3585 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    170 batch_limit:   8677 Loss:   0.3582 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    171 batch_limit:   8677 Loss:   0.3579 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    172 batch_limit:   8677 Loss:   0.3575 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    173 batch_limit:   8677 Loss:   0.3572 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    174 batch_limit:   8677 Loss:   0.3568 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    175 batch_limit:   8677 Loss:   0.3565 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    176 batch_limit:   8677 Loss:   0.3561 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    177 batch_limit:   8677 Loss:   0.3557 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    178 batch_limit:   8677 Loss:   0.3554 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    179 batch_limit:   8677 Loss:   0.3550 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    180 batch_limit:   8677 Loss:   0.3546 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    181 batch_limit:   8677 Loss:   0.3542 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    182 batch_limit:   8677 Loss:   0.3538 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    183 batch_limit:   8677 Loss:   0.3534 Training Acc:    84.16  6525.56    95.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    184 batch_limit:   8677 Loss:   0.3530 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    185 batch_limit:   8677 Loss:   0.3526 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    186 batch_limit:   8677 Loss:   0.3522 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    187 batch_limit:   8677 Loss:   0.3518 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    188 batch_limit:   8677 Loss:   0.3514 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    189 batch_limit:   8677 Loss:   0.3510 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    190 batch_limit:   8677 Loss:   0.3505 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    191 batch_limit:   8677 Loss:   0.3501 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    192 batch_limit:   8677 Loss:   0.3497 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    193 batch_limit:   8677 Loss:   0.3492 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    194 batch_limit:   8677 Loss:   0.3488 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    195 batch_limit:   8677 Loss:   0.3483 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    196 batch_limit:   8677 Loss:   0.3478 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    197 batch_limit:   8677 Loss:   0.3474 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    198 batch_limit:   8677 Loss:   0.3469 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    199 batch_limit:   8677 Loss:   0.3464 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    200 batch_limit:   8677 Loss:   0.3459 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    201 batch_limit:   8677 Loss:   0.3454 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    202 batch_limit:   8677 Loss:   0.3449 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    203 batch_limit:   8677 Loss:   0.3444 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    204 batch_limit:   8677 Loss:   0.3438 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    205 batch_limit:   8677 Loss:   0.3433 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    206 batch_limit:   8677 Loss:   0.3427 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    207 batch_limit:   8677 Loss:   0.3422 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    208 batch_limit:   8677 Loss:   0.3416 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    209 batch_limit:   8677 Loss:   0.3410 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    210 batch_limit:   8677 Loss:   0.3404 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    211 batch_limit:   8677 Loss:   0.3398 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    212 batch_limit:   8677 Loss:   0.3392 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    213 batch_limit:   8677 Loss:   0.3386 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    214 batch_limit:   8677 Loss:   0.3379 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    215 batch_limit:   8677 Loss:   0.3373 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    216 batch_limit:   8677 Loss:   0.3367 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    217 batch_limit:   8677 Loss:   0.3360 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    218 batch_limit:   8677 Loss:   0.3353 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    219 batch_limit:   8677 Loss:   0.3346 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    220 batch_limit:   8677 Loss:   0.3339 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    221 batch_limit:   8677 Loss:   0.3332 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    222 batch_limit:   8677 Loss:   0.3325 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    223 batch_limit:   8677 Loss:   0.3318 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    224 batch_limit:   8677 Loss:   0.3311 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    225 batch_limit:   8677 Loss:   0.3303 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    226 batch_limit:   8677 Loss:   0.3296 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    227 batch_limit:   8677 Loss:   0.3288 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    228 batch_limit:   8677 Loss:   0.3281 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    229 batch_limit:   8677 Loss:   0.3273 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    230 batch_limit:   8677 Loss:   0.3265 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    231 batch_limit:   8677 Loss:   0.3258 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    232 batch_limit:   8677 Loss:   0.3250 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    233 batch_limit:   8677 Loss:   0.3242 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    234 batch_limit:   8677 Loss:   0.3234 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    235 batch_limit:   8677 Loss:   0.3226 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    236 batch_limit:   8677 Loss:   0.3218 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    237 batch_limit:   8677 Loss:   0.3210 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    238 batch_limit:   8677 Loss:   0.3202 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    239 batch_limit:   8677 Loss:   0.3194 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    240 batch_limit:   8677 Loss:   0.3186 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    241 batch_limit:   8677 Loss:   0.3178 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    242 batch_limit:   8677 Loss:   0.3170 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    243 batch_limit:   8677 Loss:   0.3162 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    244 batch_limit:   8677 Loss:   0.3154 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    245 batch_limit:   8677 Loss:   0.3146 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    246 batch_limit:   8677 Loss:   0.3138 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    247 batch_limit:   8677 Loss:   0.3130 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    248 batch_limit:   8677 Loss:   0.3123 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    249 batch_limit:   8677 Loss:   0.3115 Training Acc:    84.16  6526.35    95.98%\n",
      "Epoch    250 batch_limit:   8677 Loss:   0.3107 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    251 batch_limit:   8677 Loss:   0.3099 Training Acc:    84.16  6526.35    95.98%\n",
      "Epoch    252 batch_limit:   8677 Loss:   0.3092 Training Acc:    84.16  6526.35    95.98%\n",
      "Epoch    253 batch_limit:   8677 Loss:   0.3084 Training Acc:    84.16  6524.78    95.95%\n",
      "Epoch    254 batch_limit:   8677 Loss:   0.3077 Training Acc:    84.16  6526.35    95.98%\n",
      "Epoch    255 batch_limit:   8677 Loss:   0.3070 Training Acc:    84.16  6527.13    95.99%\n",
      "Epoch    256 batch_limit:   8677 Loss:   0.3062 Training Acc:    84.16  6526.35    95.98%\n",
      "Epoch    257 batch_limit:   8677 Loss:   0.3055 Training Acc:    84.16  6526.35    95.98%\n",
      "Epoch    258 batch_limit:   8677 Loss:   0.3048 Training Acc:    84.16  6526.35    95.98%\n",
      "Epoch    259 batch_limit:   8677 Loss:   0.3041 Training Acc:    84.16  6526.35    95.98%\n",
      "Epoch    260 batch_limit:   8677 Loss:   0.3034 Training Acc:    84.16  6526.35    95.98%\n",
      "Epoch    261 batch_limit:   8677 Loss:   0.3028 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    262 batch_limit:   8677 Loss:   0.3021 Training Acc:    84.16  6524.00    95.94%\n",
      "Epoch    263 batch_limit:   8677 Loss:   0.3014 Training Acc:    84.16  6526.35    95.98%\n",
      "Epoch    264 batch_limit:   8677 Loss:   0.3008 Training Acc:    84.16  6527.13    95.99%\n",
      "Epoch    265 batch_limit:   8677 Loss:   0.3002 Training Acc:    84.16  6522.44    95.92%\n",
      "Epoch    266 batch_limit:   8677 Loss:   0.2995 Training Acc:    84.16  6520.88    95.90%\n",
      "Epoch    267 batch_limit:   8677 Loss:   0.2989 Training Acc:    84.16  6519.31    95.87%\n",
      "Epoch    268 batch_limit:   8677 Loss:   0.2983 Training Acc:    84.16  6518.53    95.86%\n",
      "Epoch    269 batch_limit:   8677 Loss:   0.2977 Training Acc:    84.16  6515.41    95.81%\n",
      "Epoch    270 batch_limit:   8677 Loss:   0.2972 Training Acc:    84.16  6516.97    95.84%\n",
      "Epoch    271 batch_limit:   8677 Loss:   0.2966 Training Acc:    84.16  6516.97    95.84%\n",
      "Epoch    272 batch_limit:   8677 Loss:   0.2960 Training Acc:    84.16  6517.75    95.85%\n",
      "Epoch    273 batch_limit:   8677 Loss:   0.2955 Training Acc:    84.16  6517.75    95.85%\n",
      "Epoch    274 batch_limit:   8677 Loss:   0.2950 Training Acc:    84.16  6518.53    95.86%\n",
      "Epoch    275 batch_limit:   8677 Loss:   0.2944 Training Acc:    84.16  6518.53    95.86%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    276 batch_limit:   8677 Loss:   0.2939 Training Acc:    84.16  6517.75    95.85%\n",
      "Epoch    277 batch_limit:   8677 Loss:   0.2934 Training Acc:    84.16  6518.53    95.86%\n",
      "Epoch    278 batch_limit:   8677 Loss:   0.2929 Training Acc:    84.16  6521.66    95.91%\n",
      "Epoch    279 batch_limit:   8677 Loss:   0.2924 Training Acc:    84.16  6520.88    95.90%\n",
      "Epoch    280 batch_limit:   8677 Loss:   0.2920 Training Acc:    84.16  6520.88    95.90%\n",
      "Epoch    281 batch_limit:   8677 Loss:   0.2915 Training Acc:    84.16  6522.44    95.92%\n",
      "Epoch    282 batch_limit:   8677 Loss:   0.2910 Training Acc:    84.16  6522.44    95.92%\n",
      "Epoch    283 batch_limit:   8677 Loss:   0.2906 Training Acc:    84.16  6522.44    95.92%\n",
      "Epoch    284 batch_limit:   8677 Loss:   0.2901 Training Acc:    84.16  6524.00    95.94%\n",
      "Epoch    285 batch_limit:   8677 Loss:   0.2897 Training Acc:    84.16  6524.78    95.95%\n",
      "Epoch    286 batch_limit:   8677 Loss:   0.2893 Training Acc:    84.16  6525.56    95.96%\n",
      "Epoch    287 batch_limit:   8677 Loss:   0.2888 Training Acc:    84.16  6527.13    95.99%\n",
      "Epoch    288 batch_limit:   8677 Loss:   0.2884 Training Acc:    84.16  6527.13    95.99%\n",
      "Epoch    289 batch_limit:   8677 Loss:   0.2880 Training Acc:    84.16  6529.47    96.02%\n",
      "Epoch    290 batch_limit:   8677 Loss:   0.2876 Training Acc:    84.16  6531.81    96.06%\n",
      "Epoch    291 batch_limit:   8677 Loss:   0.2872 Training Acc:    84.16  6531.81    96.06%\n",
      "Epoch    292 batch_limit:   8677 Loss:   0.2868 Training Acc:    84.16  6532.60    96.07%\n",
      "Epoch    293 batch_limit:   8677 Loss:   0.2865 Training Acc:    84.16  6533.38    96.08%\n",
      "Epoch    294 batch_limit:   8677 Loss:   0.2861 Training Acc:    84.16  6533.38    96.08%\n",
      "Epoch    295 batch_limit:   8677 Loss:   0.2857 Training Acc:    84.16  6534.16    96.09%\n",
      "Epoch    296 batch_limit:   8677 Loss:   0.2854 Training Acc:    84.16  6534.94    96.10%\n",
      "Epoch    297 batch_limit:   8677 Loss:   0.2850 Training Acc:    84.16  6534.94    96.10%\n",
      "Epoch    298 batch_limit:   8677 Loss:   0.2847 Training Acc:    84.16  6536.50    96.13%\n",
      "Epoch    299 batch_limit:   8677 Loss:   0.2843 Training Acc:    84.16  6537.28    96.14%\n",
      "Epoch    300 batch_limit:   8677 Loss:   0.2840 Training Acc:    84.16  6537.28    96.14%\n",
      "Epoch    301 batch_limit:   8677 Loss:   0.2836 Training Acc:    84.16  6538.06    96.15%\n",
      "Epoch    302 batch_limit:   8677 Loss:   0.2833 Training Acc:    84.16  6538.85    96.16%\n",
      "Epoch    303 batch_limit:   8677 Loss:   0.2830 Training Acc:    84.16  6540.41    96.18%\n",
      "Epoch    304 batch_limit:   8677 Loss:   0.2827 Training Acc:    84.16  6541.19    96.19%\n",
      "Epoch    305 batch_limit:   8677 Loss:   0.2824 Training Acc:    84.16  6541.97    96.21%\n",
      "Epoch    306 batch_limit:   8677 Loss:   0.2821 Training Acc:    84.16  6542.75    96.22%\n",
      "Epoch    307 batch_limit:   8677 Loss:   0.2818 Training Acc:    84.16  6545.88    96.26%\n",
      "Epoch    308 batch_limit:   8677 Loss:   0.2815 Training Acc:    84.16  6545.88    96.26%\n",
      "Epoch    309 batch_limit:   8677 Loss:   0.2813 Training Acc:    84.16  6545.88    96.26%\n",
      "Epoch    310 batch_limit:   8677 Loss:   0.2810 Training Acc:    84.16  6545.88    96.26%\n",
      "Epoch    311 batch_limit:   8677 Loss:   0.2807 Training Acc:    84.16  6545.88    96.26%\n",
      "Epoch    312 batch_limit:   8677 Loss:   0.2805 Training Acc:    84.16  6545.88    96.26%\n",
      "Epoch    313 batch_limit:   8677 Loss:   0.2803 Training Acc:    84.16  6546.66    96.27%\n",
      "Epoch    314 batch_limit:   8677 Loss:   0.2800 Training Acc:    84.16  6547.44    96.29%\n",
      "Epoch    315 batch_limit:   8677 Loss:   0.2798 Training Acc:    84.16  6547.44    96.29%\n",
      "Epoch    316 batch_limit:   8677 Loss:   0.2796 Training Acc:    84.16  6548.22    96.30%\n",
      "Epoch    317 batch_limit:   8677 Loss:   0.2793 Training Acc:    84.16  6548.22    96.30%\n",
      "Epoch    318 batch_limit:   8677 Loss:   0.2791 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    319 batch_limit:   8677 Loss:   0.2789 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    320 batch_limit:   8677 Loss:   0.2788 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    321 batch_limit:   8677 Loss:   0.2786 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    322 batch_limit:   8677 Loss:   0.2784 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    323 batch_limit:   8677 Loss:   0.2782 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    324 batch_limit:   8677 Loss:   0.2781 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    325 batch_limit:   8677 Loss:   0.2779 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    326 batch_limit:   8677 Loss:   0.2778 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    327 batch_limit:   8677 Loss:   0.2776 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    328 batch_limit:   8677 Loss:   0.2775 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    329 batch_limit:   8677 Loss:   0.2774 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    330 batch_limit:   8677 Loss:   0.2773 Training Acc:    84.16  6549.00    96.31%\n",
      "Epoch    331 batch_limit:   8677 Loss:   0.2772 Training Acc:    84.16  6549.78    96.32%\n",
      "Epoch    332 batch_limit:   8677 Loss:   0.2771 Training Acc:    84.16  6549.78    96.32%\n",
      "Epoch    333 batch_limit:   8677 Loss:   0.2770 Training Acc:    84.16  6549.78    96.32%\n",
      "Epoch    334 batch_limit:   8677 Loss:   0.2769 Training Acc:    84.16  6549.78    96.32%\n",
      "Epoch    335 batch_limit:   8677 Loss:   0.2769 Training Acc:    84.16  6549.78    96.32%\n",
      "Epoch    336 batch_limit:   8677 Loss:   0.2768 Training Acc:    84.16  6549.78    96.32%\n",
      "Epoch    337 batch_limit:   8677 Loss:   0.2768 Training Acc:    84.16  6549.78    96.32%\n",
      "Epoch    338 batch_limit:   8677 Loss:   0.2767 Training Acc:    84.16  6549.78    96.32%\n",
      "Epoch    339 batch_limit:   8677 Loss:   0.2767 Training Acc:    84.16  6550.56    96.33%\n",
      "Epoch    340 batch_limit:   8677 Loss:   0.2767 Training Acc:    84.16  6550.56    96.33%\n",
      "Epoch    341 batch_limit:   8677 Loss:   0.2766 Training Acc:    84.16  6551.35    96.34%\n",
      "Epoch    342 batch_limit:   8677 Loss:   0.2766 Training Acc:    84.16  6551.35    96.34%\n",
      "Epoch    343 batch_limit:   8677 Loss:   0.2766 Training Acc:    84.16  6551.35    96.34%\n",
      "Epoch    344 batch_limit:   8677 Loss:   0.2766 Training Acc:    84.16  6551.35    96.34%\n",
      "Epoch    345 batch_limit:   8677 Loss:   0.2766 Training Acc:    84.16  6552.13    96.35%\n",
      "Epoch    346 batch_limit:   8677 Loss:   0.2767 Training Acc:    84.16  6552.13    96.35%\n",
      "Epoch    347 batch_limit:   8677 Loss:   0.2767 Training Acc:    84.16  6552.13    96.35%\n",
      "Epoch    348 batch_limit:   8677 Loss:   0.2767 Training Acc:    84.16  6553.69    96.38%\n",
      "Epoch    349 batch_limit:   8677 Loss:   0.2768 Training Acc:    84.16  6553.69    96.38%\n",
      "Epoch    350 batch_limit:   8677 Loss:   0.2768 Training Acc:    84.16  6554.47    96.39%\n",
      "Epoch    351 batch_limit:   8677 Loss:   0.2769 Training Acc:    84.16  6554.47    96.39%\n",
      "Epoch    352 batch_limit:   8677 Loss:   0.2770 Training Acc:    84.16  6554.47    96.39%\n",
      "Epoch    353 batch_limit:   8677 Loss:   0.2771 Training Acc:    84.16  6554.47    96.39%\n",
      "Epoch    354 batch_limit:   8677 Loss:   0.2771 Training Acc:    84.16  6554.47    96.39%\n",
      "Epoch    355 batch_limit:   8677 Loss:   0.2772 Training Acc:    84.16  6555.25    96.40%\n",
      "Epoch    356 batch_limit:   8677 Loss:   0.2773 Training Acc:    84.16  6556.03    96.41%\n",
      "Epoch    357 batch_limit:   8677 Loss:   0.2774 Training Acc:    84.16  6556.03    96.41%\n",
      "Epoch    358 batch_limit:   8677 Loss:   0.2776 Training Acc:    84.16  6556.03    96.41%\n",
      "Epoch    359 batch_limit:   8677 Loss:   0.2777 Training Acc:    84.16  6556.03    96.41%\n",
      "Epoch    360 batch_limit:   8677 Loss:   0.2778 Training Acc:    84.16  6556.03    96.41%\n",
      "Epoch    361 batch_limit:   8677 Loss:   0.2780 Training Acc:    84.16  6556.03    96.41%\n",
      "Epoch    362 batch_limit:   8677 Loss:   0.2781 Training Acc:    84.16  6557.60    96.44%\n",
      "Epoch    363 batch_limit:   8677 Loss:   0.2783 Training Acc:    84.16  6557.60    96.44%\n",
      "Epoch    364 batch_limit:   8677 Loss:   0.2784 Training Acc:    84.16  6557.60    96.44%\n",
      "Epoch    365 batch_limit:   8677 Loss:   0.2786 Training Acc:    84.16  6557.60    96.44%\n",
      "Epoch    366 batch_limit:   8677 Loss:   0.2788 Training Acc:    84.16  6558.38    96.45%\n",
      "Epoch    367 batch_limit:   8677 Loss:   0.2790 Training Acc:    84.16  6558.38    96.45%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    368 batch_limit:   8677 Loss:   0.2792 Training Acc:    84.16  6558.38    96.45%\n",
      "Epoch    369 batch_limit:   8677 Loss:   0.2794 Training Acc:    84.16  6558.38    96.45%\n",
      "Epoch    370 batch_limit:   8677 Loss:   0.2796 Training Acc:    84.16  6558.38    96.45%\n",
      "Epoch    371 batch_limit:   8677 Loss:   0.2798 Training Acc:    84.16  6558.38    96.45%\n",
      "Epoch    372 batch_limit:   8677 Loss:   0.2800 Training Acc:    84.16  6558.38    96.45%\n",
      "Epoch    373 batch_limit:   8677 Loss:   0.2803 Training Acc:    84.16  6558.38    96.45%\n",
      "Epoch    374 batch_limit:   8677 Loss:   0.2805 Training Acc:    84.16  6559.16    96.46%\n",
      "Epoch    375 batch_limit:   8677 Loss:   0.2808 Training Acc:    84.16  6559.94    96.47%\n",
      "Epoch    376 batch_limit:   8677 Loss:   0.2810 Training Acc:    84.16  6559.94    96.47%\n",
      "Epoch    377 batch_limit:   8677 Loss:   0.2813 Training Acc:    84.16  6560.72    96.48%\n",
      "Epoch    378 batch_limit:   8677 Loss:   0.2816 Training Acc:    84.16  6560.72    96.48%\n",
      "Epoch    379 batch_limit:   8677 Loss:   0.2819 Training Acc:    84.16  6560.72    96.48%\n",
      "Epoch    380 batch_limit:   8677 Loss:   0.2822 Training Acc:    84.16  6560.72    96.48%\n",
      "Epoch    381 batch_limit:   8677 Loss:   0.2825 Training Acc:    84.16  6560.72    96.48%\n",
      "Epoch    382 batch_limit:   8677 Loss:   0.2828 Training Acc:    84.16  6560.72    96.48%\n",
      "Epoch    383 batch_limit:   8677 Loss:   0.2831 Training Acc:    84.16  6560.72    96.48%\n",
      "Epoch    384 batch_limit:   8677 Loss:   0.2835 Training Acc:    84.16  6560.72    96.48%\n",
      "Epoch    385 batch_limit:   8677 Loss:   0.2838 Training Acc:    84.16  6561.50    96.49%\n",
      "Epoch    386 batch_limit:   8677 Loss:   0.2842 Training Acc:    84.16  6562.28    96.50%\n",
      "Epoch    387 batch_limit:   8677 Loss:   0.2845 Training Acc:    84.16  6562.28    96.50%\n",
      "Epoch    388 batch_limit:   8677 Loss:   0.2849 Training Acc:    84.16  6563.06    96.52%\n",
      "Epoch    389 batch_limit:   8677 Loss:   0.2853 Training Acc:    84.16  6563.06    96.52%\n",
      "Epoch    390 batch_limit:   8677 Loss:   0.2857 Training Acc:    84.16  6563.06    96.52%\n",
      "Epoch    391 batch_limit:   8677 Loss:   0.2861 Training Acc:    85.15  6564.05    96.53%\n",
      "Epoch    392 batch_limit:   8677 Loss:   0.2865 Training Acc:    85.15  6564.05    96.53%\n",
      "Epoch    393 batch_limit:   8677 Loss:   0.2869 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    394 batch_limit:   8677 Loss:   0.2873 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    395 batch_limit:   8677 Loss:   0.2878 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    396 batch_limit:   8677 Loss:   0.2882 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    397 batch_limit:   8677 Loss:   0.2887 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    398 batch_limit:   8677 Loss:   0.2892 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    399 batch_limit:   8677 Loss:   0.2897 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    400 batch_limit:   8677 Loss:   0.2902 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    401 batch_limit:   8677 Loss:   0.2907 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    402 batch_limit:   8677 Loss:   0.2912 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    403 batch_limit:   8677 Loss:   0.2917 Training Acc:    85.15  6564.84    96.54%\n",
      "Epoch    404 batch_limit:   8677 Loss:   0.2922 Training Acc:    85.15  6565.62    96.55%\n",
      "Epoch    405 batch_limit:   8677 Loss:   0.2928 Training Acc:    85.15  6566.40    96.56%\n",
      "Epoch    406 batch_limit:   8677 Loss:   0.2933 Training Acc:    85.15  6566.40    96.56%\n",
      "Epoch    407 batch_limit:   8677 Loss:   0.2939 Training Acc:    85.15  6566.40    96.56%\n",
      "Epoch    408 batch_limit:   8677 Loss:   0.2944 Training Acc:    85.15  6566.40    96.56%\n",
      "Epoch    409 batch_limit:   8677 Loss:   0.2950 Training Acc:    85.15  6566.40    96.56%\n",
      "Epoch    410 batch_limit:   8677 Loss:   0.2956 Training Acc:    85.15  6567.18    96.58%\n",
      "Epoch    411 batch_limit:   8677 Loss:   0.2962 Training Acc:    85.15  6567.18    96.58%\n",
      "Epoch    412 batch_limit:   8677 Loss:   0.2968 Training Acc:    85.15  6567.18    96.58%\n",
      "Epoch    413 batch_limit:   8677 Loss:   0.2974 Training Acc:    85.15  6567.96    96.59%\n",
      "Epoch    414 batch_limit:   8677 Loss:   0.2981 Training Acc:    85.15  6567.96    96.59%\n",
      "Epoch    415 batch_limit:   8677 Loss:   0.2987 Training Acc:    85.15  6567.96    96.59%\n",
      "Epoch    416 batch_limit:   8677 Loss:   0.2993 Training Acc:    86.14  6568.95    96.60%\n",
      "Epoch    417 batch_limit:   8677 Loss:   0.3000 Training Acc:    86.14  6570.51    96.63%\n",
      "Epoch    418 batch_limit:   8677 Loss:   0.3006 Training Acc:    86.14  6572.08    96.65%\n",
      "Epoch    419 batch_limit:   8677 Loss:   0.3013 Training Acc:    86.14  6572.08    96.65%\n",
      "Epoch    420 batch_limit:   8677 Loss:   0.3020 Training Acc:    86.14  6572.08    96.65%\n",
      "Epoch    421 batch_limit:   8677 Loss:   0.3027 Training Acc:    86.14  6573.64    96.67%\n",
      "Epoch    422 batch_limit:   8677 Loss:   0.3033 Training Acc:    86.14  6573.64    96.67%\n",
      "Epoch    423 batch_limit:   8677 Loss:   0.3040 Training Acc:    86.14  6573.64    96.67%\n",
      "Epoch    424 batch_limit:   8677 Loss:   0.3047 Training Acc:    86.14  6574.42    96.68%\n",
      "Epoch    425 batch_limit:   8677 Loss:   0.3054 Training Acc:    86.14  6574.42    96.68%\n",
      "Epoch    426 batch_limit:   8677 Loss:   0.3061 Training Acc:    86.14  6574.42    96.68%\n",
      "Epoch    427 batch_limit:   8677 Loss:   0.3069 Training Acc:    86.14  6574.42    96.68%\n",
      "Epoch    428 batch_limit:   8677 Loss:   0.3076 Training Acc:    86.14  6574.42    96.68%\n",
      "Epoch    429 batch_limit:   8677 Loss:   0.3083 Training Acc:    86.14  6575.20    96.69%\n",
      "Epoch    430 batch_limit:   8677 Loss:   0.3090 Training Acc:    86.14  6575.20    96.69%\n",
      "Epoch    431 batch_limit:   8677 Loss:   0.3098 Training Acc:    86.14  6575.98    96.71%\n",
      "Epoch    432 batch_limit:   8677 Loss:   0.3105 Training Acc:    86.14  6575.98    96.71%\n",
      "Epoch    433 batch_limit:   8677 Loss:   0.3113 Training Acc:    86.14  6575.98    96.71%\n",
      "Epoch    434 batch_limit:   8677 Loss:   0.3120 Training Acc:    86.14  6575.98    96.71%\n",
      "Epoch    435 batch_limit:   8677 Loss:   0.3128 Training Acc:    86.14  6575.98    96.71%\n",
      "Epoch    436 batch_limit:   8677 Loss:   0.3135 Training Acc:    86.14  6578.33    96.74%\n",
      "Epoch    437 batch_limit:   8677 Loss:   0.3143 Training Acc:    86.14  6579.11    96.75%\n",
      "Epoch    438 batch_limit:   8677 Loss:   0.3151 Training Acc:    86.14  6579.11    96.75%\n",
      "Epoch    439 batch_limit:   8677 Loss:   0.3158 Training Acc:    86.14  6579.11    96.75%\n",
      "Epoch    440 batch_limit:   8677 Loss:   0.3166 Training Acc:    86.14  6579.11    96.75%\n",
      "Epoch    441 batch_limit:   8677 Loss:   0.3174 Training Acc:    87.13  6580.10    96.77%\n",
      "Epoch    442 batch_limit:   8677 Loss:   0.3182 Training Acc:    87.13  6582.44    96.80%\n",
      "Epoch    443 batch_limit:   8677 Loss:   0.3189 Training Acc:    87.13  6587.13    96.87%\n",
      "Epoch    444 batch_limit:   8677 Loss:   0.3197 Training Acc:    87.13  6587.91    96.88%\n",
      "Epoch    445 batch_limit:   8677 Loss:   0.3205 Training Acc:    87.13  6587.91    96.88%\n",
      "Epoch    446 batch_limit:   8677 Loss:   0.3213 Training Acc:    87.13  6588.69    96.89%\n",
      "Epoch    447 batch_limit:   8677 Loss:   0.3221 Training Acc:    87.13  6588.69    96.89%\n",
      "Epoch    448 batch_limit:   8677 Loss:   0.3228 Training Acc:    87.13  6590.25    96.92%\n",
      "Epoch    449 batch_limit:   8677 Loss:   0.3236 Training Acc:    87.13  6590.25    96.92%\n",
      "Epoch    450 batch_limit:   8677 Loss:   0.3244 Training Acc:    87.13  6591.03    96.93%\n",
      "Epoch    451 batch_limit:   8677 Loss:   0.3252 Training Acc:    87.13  6591.82    96.94%\n",
      "Epoch    452 batch_limit:   8677 Loss:   0.3260 Training Acc:    87.13  6591.82    96.94%\n",
      "Epoch    453 batch_limit:   8677 Loss:   0.3268 Training Acc:    87.13  6591.82    96.94%\n",
      "Epoch    454 batch_limit:   8677 Loss:   0.3276 Training Acc:    87.13  6591.82    96.94%\n",
      "Epoch    455 batch_limit:   8677 Loss:   0.3284 Training Acc:    87.13  6591.82    96.94%\n",
      "Epoch    456 batch_limit:   8677 Loss:   0.3291 Training Acc:    87.13  6591.82    96.94%\n",
      "Epoch    457 batch_limit:   8677 Loss:   0.3299 Training Acc:    87.13  6591.82    96.94%\n",
      "Epoch    458 batch_limit:   8677 Loss:   0.3307 Training Acc:    87.13  6593.38    96.96%\n",
      "Epoch    459 batch_limit:   8677 Loss:   0.3315 Training Acc:    87.13  6593.38    96.96%\n",
      "Epoch    460 batch_limit:   8677 Loss:   0.3323 Training Acc:    87.13  6596.50    97.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    461 batch_limit:   8677 Loss:   0.3331 Training Acc:    87.13  6596.50    97.01%\n",
      "Epoch    462 batch_limit:   8677 Loss:   0.3338 Training Acc:    87.13  6596.50    97.01%\n",
      "Epoch    463 batch_limit:   8677 Loss:   0.3346 Training Acc:    87.13  6596.50    97.01%\n",
      "Epoch    464 batch_limit:   8677 Loss:   0.3354 Training Acc:    87.13  6597.28    97.02%\n",
      "Epoch    465 batch_limit:   8677 Loss:   0.3362 Training Acc:    87.13  6598.07    97.03%\n",
      "Epoch    466 batch_limit:   8677 Loss:   0.3370 Training Acc:    87.13  6598.85    97.04%\n",
      "Epoch    467 batch_limit:   8677 Loss:   0.3377 Training Acc:    87.13  6598.85    97.04%\n",
      "Epoch    468 batch_limit:   8677 Loss:   0.3385 Training Acc:    87.13  6601.19    97.08%\n",
      "Epoch    469 batch_limit:   8677 Loss:   0.3393 Training Acc:    87.13  6604.32    97.12%\n",
      "Epoch    470 batch_limit:   8677 Loss:   0.3401 Training Acc:    87.13  6605.10    97.13%\n",
      "Epoch    471 batch_limit:   8677 Loss:   0.3408 Training Acc:    87.13  6605.88    97.15%\n",
      "Epoch    472 batch_limit:   8677 Loss:   0.3416 Training Acc:    87.13  6608.22    97.18%\n",
      "Epoch    473 batch_limit:   8677 Loss:   0.3424 Training Acc:    87.13  6608.22    97.18%\n",
      "Epoch    474 batch_limit:   8677 Loss:   0.3431 Training Acc:    87.13  6609.00    97.19%\n",
      "Epoch    475 batch_limit:   8677 Loss:   0.3439 Training Acc:    87.13  6609.00    97.19%\n",
      "Epoch    476 batch_limit:   8677 Loss:   0.3446 Training Acc:    87.13  6610.57    97.21%\n",
      "Epoch    477 batch_limit:   8677 Loss:   0.3454 Training Acc:    87.13  6612.13    97.24%\n",
      "Epoch    478 batch_limit:   8677 Loss:   0.3461 Training Acc:    87.13  6612.91    97.25%\n",
      "Epoch    479 batch_limit:   8677 Loss:   0.3469 Training Acc:    87.13  6613.69    97.26%\n",
      "Epoch    480 batch_limit:   8677 Loss:   0.3476 Training Acc:    87.13  6614.47    97.27%\n",
      "Epoch    481 batch_limit:   8677 Loss:   0.3484 Training Acc:    87.13  6614.47    97.27%\n",
      "Epoch    482 batch_limit:   8677 Loss:   0.3491 Training Acc:    87.13  6615.25    97.28%\n",
      "Epoch    483 batch_limit:   8677 Loss:   0.3498 Training Acc:    87.13  6616.03    97.29%\n",
      "Epoch    484 batch_limit:   8677 Loss:   0.3506 Training Acc:    87.13  6616.03    97.29%\n",
      "Epoch    485 batch_limit:   8677 Loss:   0.3513 Training Acc:    87.13  6616.82    97.31%\n",
      "Epoch    486 batch_limit:   8677 Loss:   0.3520 Training Acc:    87.13  6616.82    97.31%\n",
      "Epoch    487 batch_limit:   8677 Loss:   0.3527 Training Acc:    87.13  6617.60    97.32%\n",
      "Epoch    488 batch_limit:   8677 Loss:   0.3535 Training Acc:    87.13  6617.60    97.32%\n",
      "Epoch    489 batch_limit:   8677 Loss:   0.3542 Training Acc:    87.13  6617.60    97.32%\n",
      "Epoch    490 batch_limit:   8677 Loss:   0.3549 Training Acc:    87.13  6619.94    97.35%\n",
      "Epoch    491 batch_limit:   8677 Loss:   0.3556 Training Acc:    87.13  6620.72    97.36%\n",
      "Epoch    492 batch_limit:   8677 Loss:   0.3563 Training Acc:    87.13  6620.72    97.36%\n",
      "Epoch    493 batch_limit:   8677 Loss:   0.3570 Training Acc:    87.13  6620.72    97.36%\n",
      "Epoch    494 batch_limit:   8677 Loss:   0.3577 Training Acc:    87.13  6623.07    97.40%\n",
      "Epoch    495 batch_limit:   8677 Loss:   0.3584 Training Acc:    87.13  6623.07    97.40%\n",
      "Epoch    496 batch_limit:   8677 Loss:   0.3591 Training Acc:    87.13  6623.85    97.41%\n",
      "Epoch    497 batch_limit:   8677 Loss:   0.3597 Training Acc:    87.13  6626.19    97.44%\n",
      "Epoch    498 batch_limit:   8677 Loss:   0.3604 Training Acc:    87.13  6626.19    97.44%\n",
      "Epoch    499 batch_limit:   8677 Loss:   0.3611 Training Acc:    87.13  6626.97    97.46%\n",
      "Epoch    500 batch_limit:   8677 Loss:   0.3618 Training Acc:    87.13  6628.53    97.48%\n",
      "Epoch    501 batch_limit:   8677 Loss:   0.3624 Training Acc:    87.13  6629.32    97.49%\n",
      "Epoch    502 batch_limit:   8677 Loss:   0.3631 Training Acc:    87.13  6630.10    97.50%\n",
      "Epoch    503 batch_limit:   8677 Loss:   0.3637 Training Acc:    87.13  6630.10    97.50%\n",
      "Epoch    504 batch_limit:   8677 Loss:   0.3644 Training Acc:    87.13  6630.88    97.51%\n",
      "Epoch    505 batch_limit:   8677 Loss:   0.3650 Training Acc:    87.13  6631.66    97.52%\n",
      "Epoch    506 batch_limit:   8677 Loss:   0.3657 Training Acc:    87.13  6633.22    97.55%\n",
      "Epoch    507 batch_limit:   8677 Loss:   0.3663 Training Acc:    87.13  6633.22    97.55%\n",
      "Epoch    508 batch_limit:   8677 Loss:   0.3669 Training Acc:    87.13  6634.00    97.56%\n",
      "Epoch    509 batch_limit:   8677 Loss:   0.3676 Training Acc:    87.13  6634.78    97.57%\n",
      "Epoch    510 batch_limit:   8677 Loss:   0.3682 Training Acc:    87.13  6634.78    97.57%\n",
      "Epoch    511 batch_limit:   8677 Loss:   0.3688 Training Acc:    87.13  6635.57    97.58%\n",
      "Epoch    512 batch_limit:   8677 Loss:   0.3694 Training Acc:    87.13  6637.13    97.60%\n",
      "Epoch    513 batch_limit:   8677 Loss:   0.3700 Training Acc:    87.13  6638.69    97.63%\n",
      "Epoch    514 batch_limit:   8677 Loss:   0.3706 Training Acc:    87.13  6638.69    97.63%\n",
      "Epoch    515 batch_limit:   8677 Loss:   0.3712 Training Acc:    87.13  6638.69    97.63%\n",
      "Epoch    516 batch_limit:   8677 Loss:   0.3718 Training Acc:    87.13  6639.47    97.64%\n",
      "Epoch    517 batch_limit:   8677 Loss:   0.3724 Training Acc:    87.13  6640.25    97.65%\n",
      "Epoch    518 batch_limit:   8677 Loss:   0.3730 Training Acc:    87.13  6641.03    97.66%\n",
      "Epoch    519 batch_limit:   8677 Loss:   0.3736 Training Acc:    87.13  6643.38    97.70%\n",
      "Epoch    520 batch_limit:   8677 Loss:   0.3742 Training Acc:    88.12  6645.15    97.72%\n",
      "Epoch    521 batch_limit:   8677 Loss:   0.3747 Training Acc:    88.12  6646.71    97.75%\n",
      "Epoch    522 batch_limit:   8677 Loss:   0.3753 Training Acc:    88.12  6647.49    97.76%\n",
      "Epoch    523 batch_limit:   8677 Loss:   0.3758 Training Acc:    88.12  6649.84    97.79%\n",
      "Epoch    524 batch_limit:   8677 Loss:   0.3764 Training Acc:    88.12  6650.62    97.80%\n",
      "Epoch    525 batch_limit:   8677 Loss:   0.3769 Training Acc:    88.12  6650.62    97.80%\n",
      "Epoch    526 batch_limit:   8677 Loss:   0.3775 Training Acc:    88.12  6650.62    97.80%\n",
      "Epoch    527 batch_limit:   8677 Loss:   0.3780 Training Acc:    88.12  6652.18    97.83%\n",
      "Epoch    528 batch_limit:   8677 Loss:   0.3785 Training Acc:    88.12  6652.96    97.84%\n",
      "Epoch    529 batch_limit:   8677 Loss:   0.3791 Training Acc:    88.12  6653.74    97.85%\n",
      "Epoch    530 batch_limit:   8677 Loss:   0.3796 Training Acc:    88.12  6653.74    97.85%\n",
      "Epoch    531 batch_limit:   8677 Loss:   0.3801 Training Acc:    88.12  6654.53    97.86%\n",
      "Epoch    532 batch_limit:   8677 Loss:   0.3806 Training Acc:    88.12  6656.09    97.88%\n",
      "Epoch    533 batch_limit:   8677 Loss:   0.3811 Training Acc:    88.12  6656.09    97.88%\n",
      "Epoch    534 batch_limit:   8677 Loss:   0.3816 Training Acc:    88.12  6656.09    97.88%\n",
      "Epoch    535 batch_limit:   8677 Loss:   0.3821 Training Acc:    88.12  6656.09    97.88%\n",
      "Epoch    536 batch_limit:   8677 Loss:   0.3826 Training Acc:    88.12  6656.87    97.90%\n",
      "Epoch    537 batch_limit:   8677 Loss:   0.3831 Training Acc:    88.12  6656.87    97.90%\n",
      "Epoch    538 batch_limit:   8677 Loss:   0.3836 Training Acc:    88.12  6659.21    97.93%\n",
      "Epoch    539 batch_limit:   8677 Loss:   0.3840 Training Acc:    88.12  6660.78    97.95%\n",
      "Epoch    540 batch_limit:   8677 Loss:   0.3845 Training Acc:    88.12  6661.56    97.96%\n",
      "Epoch    541 batch_limit:   8677 Loss:   0.3850 Training Acc:    88.12  6663.12    97.99%\n",
      "Epoch    542 batch_limit:   8677 Loss:   0.3854 Training Acc:    88.12  6663.90    98.00%\n",
      "Epoch    543 batch_limit:   8677 Loss:   0.3859 Training Acc:    88.12  6663.90    98.00%\n",
      "Epoch    544 batch_limit:   8677 Loss:   0.3863 Training Acc:    88.12  6663.90    98.00%\n",
      "Epoch    545 batch_limit:   8677 Loss:   0.3867 Training Acc:    88.12  6665.46    98.02%\n",
      "Epoch    546 batch_limit:   8677 Loss:   0.3872 Training Acc:    88.12  6665.46    98.02%\n",
      "Epoch    547 batch_limit:   8677 Loss:   0.3876 Training Acc:    88.12  6666.24    98.03%\n",
      "Epoch    548 batch_limit:   8677 Loss:   0.3880 Training Acc:    88.12  6666.24    98.03%\n",
      "Epoch    549 batch_limit:   8677 Loss:   0.3885 Training Acc:    88.12  6666.24    98.03%\n",
      "Epoch    550 batch_limit:   8677 Loss:   0.3889 Training Acc:    88.12  6666.24    98.03%\n",
      "Epoch    551 batch_limit:   8677 Loss:   0.3893 Training Acc:    88.12  6667.81    98.06%\n",
      "Epoch    552 batch_limit:   8677 Loss:   0.3897 Training Acc:    88.12  6669.37    98.08%\n",
      "Epoch    553 batch_limit:   8677 Loss:   0.3901 Training Acc:    88.12  6670.15    98.09%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    554 batch_limit:   8677 Loss:   0.3905 Training Acc:    88.12  6670.93    98.10%\n",
      "Epoch    555 batch_limit:   8677 Loss:   0.3909 Training Acc:    88.12  6671.71    98.11%\n",
      "Epoch    556 batch_limit:   8677 Loss:   0.3912 Training Acc:    88.12  6673.28    98.14%\n",
      "Epoch    557 batch_limit:   8677 Loss:   0.3916 Training Acc:    88.12  6674.84    98.16%\n",
      "Epoch    558 batch_limit:   8677 Loss:   0.3920 Training Acc:    88.12  6675.62    98.17%\n",
      "Epoch    559 batch_limit:   8677 Loss:   0.3924 Training Acc:    88.12  6675.62    98.17%\n",
      "Epoch    560 batch_limit:   8677 Loss:   0.3927 Training Acc:    88.12  6675.62    98.17%\n",
      "Epoch    561 batch_limit:   8677 Loss:   0.3931 Training Acc:    88.12  6675.62    98.17%\n",
      "Epoch    562 batch_limit:   8677 Loss:   0.3934 Training Acc:    88.12  6676.40    98.18%\n",
      "Epoch    563 batch_limit:   8677 Loss:   0.3938 Training Acc:    88.12  6676.40    98.18%\n",
      "Epoch    564 batch_limit:   8677 Loss:   0.3941 Training Acc:    88.12  6676.40    98.18%\n",
      "Epoch    565 batch_limit:   8677 Loss:   0.3945 Training Acc:    88.12  6676.40    98.18%\n",
      "Epoch    566 batch_limit:   8677 Loss:   0.3948 Training Acc:    88.12  6677.18    98.19%\n",
      "Epoch    567 batch_limit:   8677 Loss:   0.3951 Training Acc:    88.12  6677.96    98.21%\n",
      "Epoch    568 batch_limit:   8677 Loss:   0.3955 Training Acc:    88.12  6678.74    98.22%\n",
      "Epoch    569 batch_limit:   8677 Loss:   0.3958 Training Acc:    88.12  6679.53    98.23%\n",
      "Epoch    570 batch_limit:   8677 Loss:   0.3961 Training Acc:    88.12  6680.31    98.24%\n",
      "Epoch    571 batch_limit:   8677 Loss:   0.3964 Training Acc:    88.12  6680.31    98.24%\n",
      "Epoch    572 batch_limit:   8677 Loss:   0.3967 Training Acc:    88.12  6680.31    98.24%\n",
      "Epoch    573 batch_limit:   8677 Loss:   0.3970 Training Acc:    88.12  6681.87    98.26%\n",
      "Epoch    574 batch_limit:   8677 Loss:   0.3973 Training Acc:    88.12  6681.87    98.26%\n",
      "Epoch    575 batch_limit:   8677 Loss:   0.3976 Training Acc:    88.12  6684.21    98.30%\n",
      "Epoch    576 batch_limit:   8677 Loss:   0.3979 Training Acc:    88.12  6684.99    98.31%\n",
      "Epoch    577 batch_limit:   8677 Loss:   0.3982 Training Acc:    88.12  6684.99    98.31%\n",
      "Epoch    578 batch_limit:   8677 Loss:   0.3985 Training Acc:    88.12  6684.99    98.31%\n",
      "Epoch    579 batch_limit:   8677 Loss:   0.3987 Training Acc:    88.12  6685.78    98.32%\n",
      "Epoch    580 batch_limit:   8677 Loss:   0.3990 Training Acc:    88.12  6685.78    98.32%\n",
      "Epoch    581 batch_limit:   8677 Loss:   0.3993 Training Acc:    88.12  6685.78    98.32%\n",
      "Epoch    582 batch_limit:   8677 Loss:   0.3995 Training Acc:    88.12  6688.12    98.35%\n",
      "Epoch    583 batch_limit:   8677 Loss:   0.3998 Training Acc:    88.12  6688.90    98.37%\n",
      "Epoch    584 batch_limit:   8677 Loss:   0.4000 Training Acc:    88.12  6687.34    98.34%\n",
      "Epoch    585 batch_limit:   8677 Loss:   0.4003 Training Acc:    88.12  6686.56    98.33%\n",
      "Epoch    586 batch_limit:   8677 Loss:   0.4005 Training Acc:    88.12  6686.56    98.33%\n",
      "Epoch    587 batch_limit:   8677 Loss:   0.4008 Training Acc:    88.12  6687.34    98.34%\n",
      "Epoch    588 batch_limit:   8677 Loss:   0.4010 Training Acc:    88.12  6688.12    98.35%\n",
      "Epoch    589 batch_limit:   8677 Loss:   0.4013 Training Acc:    88.12  6688.12    98.35%\n",
      "Epoch    590 batch_limit:   8677 Loss:   0.4015 Training Acc:    88.12  6688.12    98.35%\n",
      "Epoch    591 batch_limit:   8677 Loss:   0.4017 Training Acc:    88.12  6689.68    98.38%\n",
      "Epoch    592 batch_limit:   8677 Loss:   0.4019 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    593 batch_limit:   8677 Loss:   0.4022 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    594 batch_limit:   8677 Loss:   0.4024 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    595 batch_limit:   8677 Loss:   0.4026 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    596 batch_limit:   8677 Loss:   0.4028 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    597 batch_limit:   8677 Loss:   0.4030 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    598 batch_limit:   8677 Loss:   0.4032 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    599 batch_limit:   8677 Loss:   0.4034 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    600 batch_limit:   8677 Loss:   0.4036 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    601 batch_limit:   8677 Loss:   0.4038 Training Acc:    88.12  6691.24    98.40%\n",
      "Epoch    602 batch_limit:   8677 Loss:   0.4040 Training Acc:    88.12  6689.68    98.38%\n",
      "Epoch    603 batch_limit:   8677 Loss:   0.4041 Training Acc:    88.12  6689.68    98.38%\n",
      "Epoch    604 batch_limit:   8677 Loss:   0.4043 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    605 batch_limit:   8677 Loss:   0.4045 Training Acc:    88.12  6691.24    98.40%\n",
      "Epoch    606 batch_limit:   8677 Loss:   0.4047 Training Acc:    88.12  6691.24    98.40%\n",
      "Epoch    607 batch_limit:   8677 Loss:   0.4048 Training Acc:    88.12  6691.24    98.40%\n",
      "Epoch    608 batch_limit:   8677 Loss:   0.4050 Training Acc:    88.12  6691.24    98.40%\n",
      "Epoch    609 batch_limit:   8677 Loss:   0.4052 Training Acc:    88.12  6691.24    98.40%\n",
      "Epoch    610 batch_limit:   8677 Loss:   0.4053 Training Acc:    88.12  6691.24    98.40%\n",
      "Epoch    611 batch_limit:   8677 Loss:   0.4055 Training Acc:    88.12  6691.24    98.40%\n",
      "Epoch    612 batch_limit:   8677 Loss:   0.4057 Training Acc:    88.12  6692.81    98.42%\n",
      "Epoch    613 batch_limit:   8677 Loss:   0.4058 Training Acc:    88.12  6693.59    98.44%\n",
      "Epoch    614 batch_limit:   8677 Loss:   0.4060 Training Acc:    88.12  6694.37    98.45%\n",
      "Epoch    615 batch_limit:   8677 Loss:   0.4061 Training Acc:    88.12  6694.37    98.45%\n",
      "Epoch    616 batch_limit:   8677 Loss:   0.4063 Training Acc:    88.12  6694.37    98.45%\n",
      "Epoch    617 batch_limit:   8677 Loss:   0.4064 Training Acc:    88.12  6695.15    98.46%\n",
      "Epoch    618 batch_limit:   8677 Loss:   0.4065 Training Acc:    88.12  6694.37    98.45%\n",
      "Epoch    619 batch_limit:   8677 Loss:   0.4067 Training Acc:    88.12  6695.93    98.47%\n",
      "Epoch    620 batch_limit:   8677 Loss:   0.4068 Training Acc:    88.12  6695.93    98.47%\n",
      "Epoch    621 batch_limit:   8677 Loss:   0.4069 Training Acc:    88.12  6695.15    98.46%\n",
      "Epoch    622 batch_limit:   8677 Loss:   0.4071 Training Acc:    88.12  6694.37    98.45%\n",
      "Epoch    623 batch_limit:   8677 Loss:   0.4072 Training Acc:    88.12  6692.81    98.42%\n",
      "Epoch    624 batch_limit:   8677 Loss:   0.4073 Training Acc:    88.12  6692.03    98.41%\n",
      "Epoch    625 batch_limit:   8677 Loss:   0.4074 Training Acc:    88.12  6691.24    98.40%\n",
      "Epoch    626 batch_limit:   8677 Loss:   0.4076 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    627 batch_limit:   8677 Loss:   0.4077 Training Acc:    88.12  6689.68    98.38%\n",
      "Epoch    628 batch_limit:   8677 Loss:   0.4078 Training Acc:    88.12  6688.12    98.35%\n",
      "Epoch    629 batch_limit:   8677 Loss:   0.4079 Training Acc:    88.12  6687.34    98.34%\n",
      "Epoch    630 batch_limit:   8677 Loss:   0.4080 Training Acc:    88.12  6686.56    98.33%\n",
      "Epoch    631 batch_limit:   8677 Loss:   0.4081 Training Acc:    88.12  6687.34    98.34%\n",
      "Epoch    632 batch_limit:   8677 Loss:   0.4082 Training Acc:    88.12  6688.12    98.35%\n",
      "Epoch    633 batch_limit:   8677 Loss:   0.4083 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    634 batch_limit:   8677 Loss:   0.4084 Training Acc:    88.12  6690.46    98.39%\n",
      "Epoch    635 batch_limit:   8677 Loss:   0.4085 Training Acc:    88.12  6691.24    98.40%\n",
      "Epoch    636 batch_limit:   8677 Loss:   0.4086 Training Acc:    88.12  6693.59    98.44%\n",
      "Epoch    637 batch_limit:   8677 Loss:   0.4087 Training Acc:    88.12  6694.37    98.45%\n",
      "Epoch    638 batch_limit:   8677 Loss:   0.4088 Training Acc:    88.12  6695.93    98.47%\n",
      "Epoch    639 batch_limit:   8677 Loss:   0.4089 Training Acc:    88.12  6697.49    98.49%\n",
      "Epoch    640 batch_limit:   8677 Loss:   0.4090 Training Acc:    88.12  6697.49    98.49%\n",
      "Epoch    641 batch_limit:   8677 Loss:   0.4091 Training Acc:    88.12  6697.49    98.49%\n",
      "Epoch    642 batch_limit:   8677 Loss:   0.4092 Training Acc:    88.12  6698.28    98.50%\n",
      "Epoch    643 batch_limit:   8677 Loss:   0.4093 Training Acc:    88.12  6698.28    98.50%\n",
      "Epoch    644 batch_limit:   8677 Loss:   0.4093 Training Acc:    88.12  6699.06    98.52%\n",
      "Epoch    645 batch_limit:   8677 Loss:   0.4094 Training Acc:    88.12  6700.62    98.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    646 batch_limit:   8677 Loss:   0.4095 Training Acc:    88.12  6701.40    98.55%\n",
      "Epoch    647 batch_limit:   8677 Loss:   0.4096 Training Acc:    88.12  6702.18    98.56%\n",
      "Epoch    648 batch_limit:   8677 Loss:   0.4096 Training Acc:    88.12  6706.09    98.62%\n",
      "Epoch    649 batch_limit:   8677 Loss:   0.4097 Training Acc:    88.12  6709.21    98.66%\n",
      "Epoch    650 batch_limit:   8677 Loss:   0.4098 Training Acc:    88.12  6712.34    98.71%\n",
      "Epoch    651 batch_limit:   8677 Loss:   0.4099 Training Acc:    88.12  6712.34    98.71%\n",
      "Epoch    652 batch_limit:   8677 Loss:   0.4099 Training Acc:    88.12  6714.68    98.75%\n",
      "Epoch    653 batch_limit:   8677 Loss:   0.4100 Training Acc:    88.12  6715.46    98.76%\n",
      "Epoch    654 batch_limit:   8677 Loss:   0.4101 Training Acc:    88.12  6717.03    98.78%\n",
      "Epoch    655 batch_limit:   8677 Loss:   0.4101 Training Acc:    88.12  6717.81    98.79%\n",
      "Epoch    656 batch_limit:   8677 Loss:   0.4102 Training Acc:    88.12  6718.59    98.80%\n",
      "Epoch    657 batch_limit:   8677 Loss:   0.4102 Training Acc:    88.12  6718.59    98.80%\n",
      "Epoch    658 batch_limit:   8677 Loss:   0.4103 Training Acc:    88.12  6723.28    98.87%\n",
      "Epoch    659 batch_limit:   8677 Loss:   0.4103 Training Acc:    88.12  6724.06    98.88%\n",
      "Epoch    660 batch_limit:   8677 Loss:   0.4104 Training Acc:    88.12  6725.62    98.91%\n",
      "Epoch    661 batch_limit:   8677 Loss:   0.4105 Training Acc:    88.12  6727.18    98.93%\n",
      "Epoch    662 batch_limit:   8677 Loss:   0.4105 Training Acc:    88.12  6727.18    98.93%\n",
      "Epoch    663 batch_limit:   8677 Loss:   0.4106 Training Acc:    88.12  6727.96    98.94%\n",
      "Epoch    664 batch_limit:   8677 Loss:   0.4106 Training Acc:    88.12  6727.96    98.94%\n",
      "Epoch    665 batch_limit:   8677 Loss:   0.4107 Training Acc:    88.12  6727.96    98.94%\n",
      "Epoch    666 batch_limit:   8677 Loss:   0.4107 Training Acc:    88.12  6727.96    98.94%\n",
      "Epoch    667 batch_limit:   8677 Loss:   0.4107 Training Acc:    88.12  6727.96    98.94%\n",
      "Epoch    668 batch_limit:   8677 Loss:   0.4108 Training Acc:    88.12  6727.96    98.94%\n",
      "Epoch    669 batch_limit:   8677 Loss:   0.4108 Training Acc:    88.12  6727.96    98.94%\n",
      "Epoch    670 batch_limit:   8677 Loss:   0.4109 Training Acc:    88.12  6728.74    98.95%\n",
      "Epoch    671 batch_limit:   8677 Loss:   0.4109 Training Acc:    88.12  6728.74    98.95%\n",
      "Epoch    672 batch_limit:   8677 Loss:   0.4110 Training Acc:    88.12  6728.74    98.95%\n",
      "Epoch    673 batch_limit:   8677 Loss:   0.4110 Training Acc:    88.12  6728.74    98.95%\n",
      "Epoch    674 batch_limit:   8677 Loss:   0.4110 Training Acc:    88.12  6728.74    98.95%\n",
      "Epoch    675 batch_limit:   8677 Loss:   0.4111 Training Acc:    88.12  6729.53    98.96%\n",
      "Epoch    676 batch_limit:   8677 Loss:   0.4111 Training Acc:    88.12  6729.53    98.96%\n",
      "Epoch    677 batch_limit:   8677 Loss:   0.4111 Training Acc:    88.12  6729.53    98.96%\n",
      "Epoch    678 batch_limit:   8677 Loss:   0.4112 Training Acc:    88.12  6729.53    98.96%\n",
      "Epoch    679 batch_limit:   8677 Loss:   0.4112 Training Acc:    88.12  6729.53    98.96%\n",
      "Epoch    680 batch_limit:   8677 Loss:   0.4112 Training Acc:    88.12  6729.53    98.96%\n",
      "Epoch    681 batch_limit:   8677 Loss:   0.4113 Training Acc:    88.12  6729.53    98.96%\n",
      "Epoch    682 batch_limit:   8677 Loss:   0.4113 Training Acc:    88.12  6729.53    98.96%\n",
      "Epoch    683 batch_limit:   8677 Loss:   0.4113 Training Acc:    88.12  6730.31    98.98%\n",
      "Epoch    684 batch_limit:   8677 Loss:   0.4113 Training Acc:    88.12  6730.31    98.98%\n",
      "Epoch    685 batch_limit:   8677 Loss:   0.4114 Training Acc:    88.12  6730.31    98.98%\n",
      "Epoch    686 batch_limit:   8677 Loss:   0.4114 Training Acc:    88.12  6730.31    98.98%\n",
      "Epoch    687 batch_limit:   8677 Loss:   0.4114 Training Acc:    88.12  6730.31    98.98%\n",
      "Epoch    688 batch_limit:   8677 Loss:   0.4114 Training Acc:    88.12  6730.31    98.98%\n",
      "Epoch    689 batch_limit:   8677 Loss:   0.4114 Training Acc:    88.12  6730.31    98.98%\n",
      "Epoch    690 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6730.31    98.98%\n",
      "Epoch    691 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6731.09    98.99%\n",
      "Epoch    692 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6731.09    98.99%\n",
      "Epoch    693 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6731.87    99.00%\n",
      "Epoch    694 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6731.87    99.00%\n",
      "Epoch    695 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6731.87    99.00%\n",
      "Epoch    696 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6731.87    99.00%\n",
      "Epoch    697 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6732.65    99.01%\n",
      "Epoch    698 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.21    99.03%\n",
      "Epoch    699 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.21    99.03%\n",
      "Epoch    700 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    701 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    702 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    703 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    704 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    705 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    706 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    707 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    708 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    709 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    710 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    711 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    712 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    713 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    714 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    715 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    716 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    717 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    718 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    719 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    720 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    721 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    722 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    723 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    724 batch_limit:   8677 Loss:   0.4116 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    725 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    726 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    727 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    728 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    729 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    730 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    731 batch_limit:   8677 Loss:   0.4115 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    732 batch_limit:   8677 Loss:   0.4114 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    733 batch_limit:   8677 Loss:   0.4114 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    734 batch_limit:   8677 Loss:   0.4114 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    735 batch_limit:   8677 Loss:   0.4114 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    736 batch_limit:   8677 Loss:   0.4114 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    737 batch_limit:   8677 Loss:   0.4113 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    738 batch_limit:   8677 Loss:   0.4113 Training Acc:    88.12  6734.99    99.04%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    739 batch_limit:   8677 Loss:   0.4113 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    740 batch_limit:   8677 Loss:   0.4113 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    741 batch_limit:   8677 Loss:   0.4112 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    742 batch_limit:   8677 Loss:   0.4112 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    743 batch_limit:   8677 Loss:   0.4112 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    744 batch_limit:   8677 Loss:   0.4111 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    745 batch_limit:   8677 Loss:   0.4111 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    746 batch_limit:   8677 Loss:   0.4111 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    747 batch_limit:   8677 Loss:   0.4110 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    748 batch_limit:   8677 Loss:   0.4110 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    749 batch_limit:   8677 Loss:   0.4110 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    750 batch_limit:   8677 Loss:   0.4109 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    751 batch_limit:   8677 Loss:   0.4109 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    752 batch_limit:   8677 Loss:   0.4109 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    753 batch_limit:   8677 Loss:   0.4108 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    754 batch_limit:   8677 Loss:   0.4108 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    755 batch_limit:   8677 Loss:   0.4107 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    756 batch_limit:   8677 Loss:   0.4107 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    757 batch_limit:   8677 Loss:   0.4106 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    758 batch_limit:   8677 Loss:   0.4106 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    759 batch_limit:   8677 Loss:   0.4105 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    760 batch_limit:   8677 Loss:   0.4105 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    761 batch_limit:   8677 Loss:   0.4104 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    762 batch_limit:   8677 Loss:   0.4104 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    763 batch_limit:   8677 Loss:   0.4103 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    764 batch_limit:   8677 Loss:   0.4103 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    765 batch_limit:   8677 Loss:   0.4102 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    766 batch_limit:   8677 Loss:   0.4102 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    767 batch_limit:   8677 Loss:   0.4101 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    768 batch_limit:   8677 Loss:   0.4100 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    769 batch_limit:   8677 Loss:   0.4100 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    770 batch_limit:   8677 Loss:   0.4099 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    771 batch_limit:   8677 Loss:   0.4099 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    772 batch_limit:   8677 Loss:   0.4098 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    773 batch_limit:   8677 Loss:   0.4097 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    774 batch_limit:   8677 Loss:   0.4096 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    775 batch_limit:   8677 Loss:   0.4096 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    776 batch_limit:   8677 Loss:   0.4095 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    777 batch_limit:   8677 Loss:   0.4094 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    778 batch_limit:   8677 Loss:   0.4093 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    779 batch_limit:   8677 Loss:   0.4093 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    780 batch_limit:   8677 Loss:   0.4092 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    781 batch_limit:   8677 Loss:   0.4091 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    782 batch_limit:   8677 Loss:   0.4090 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    783 batch_limit:   8677 Loss:   0.4089 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    784 batch_limit:   8677 Loss:   0.4089 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    785 batch_limit:   8677 Loss:   0.4088 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    786 batch_limit:   8677 Loss:   0.4087 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    787 batch_limit:   8677 Loss:   0.4086 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    788 batch_limit:   8677 Loss:   0.4085 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    789 batch_limit:   8677 Loss:   0.4084 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    790 batch_limit:   8677 Loss:   0.4083 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    791 batch_limit:   8677 Loss:   0.4082 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    792 batch_limit:   8677 Loss:   0.4081 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    793 batch_limit:   8677 Loss:   0.4080 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    794 batch_limit:   8677 Loss:   0.4079 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    795 batch_limit:   8677 Loss:   0.4078 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    796 batch_limit:   8677 Loss:   0.4077 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    797 batch_limit:   8677 Loss:   0.4076 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    798 batch_limit:   8677 Loss:   0.4075 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    799 batch_limit:   8677 Loss:   0.4074 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    800 batch_limit:   8677 Loss:   0.4073 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    801 batch_limit:   8677 Loss:   0.4072 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    802 batch_limit:   8677 Loss:   0.4071 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    803 batch_limit:   8677 Loss:   0.4069 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    804 batch_limit:   8677 Loss:   0.4068 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    805 batch_limit:   8677 Loss:   0.4067 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    806 batch_limit:   8677 Loss:   0.4066 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    807 batch_limit:   8677 Loss:   0.4065 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    808 batch_limit:   8677 Loss:   0.4064 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    809 batch_limit:   8677 Loss:   0.4062 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    810 batch_limit:   8677 Loss:   0.4061 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    811 batch_limit:   8677 Loss:   0.4060 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    812 batch_limit:   8677 Loss:   0.4059 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    813 batch_limit:   8677 Loss:   0.4057 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    814 batch_limit:   8677 Loss:   0.4056 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    815 batch_limit:   8677 Loss:   0.4055 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    816 batch_limit:   8677 Loss:   0.4054 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    817 batch_limit:   8677 Loss:   0.4052 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    818 batch_limit:   8677 Loss:   0.4051 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    819 batch_limit:   8677 Loss:   0.4050 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    820 batch_limit:   8677 Loss:   0.4048 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    821 batch_limit:   8677 Loss:   0.4047 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    822 batch_limit:   8677 Loss:   0.4046 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    823 batch_limit:   8677 Loss:   0.4044 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    824 batch_limit:   8677 Loss:   0.4043 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    825 batch_limit:   8677 Loss:   0.4042 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    826 batch_limit:   8677 Loss:   0.4040 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    827 batch_limit:   8677 Loss:   0.4039 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    828 batch_limit:   8677 Loss:   0.4037 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    829 batch_limit:   8677 Loss:   0.4036 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    830 batch_limit:   8677 Loss:   0.4035 Training Acc:    88.12  6734.99    99.04%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    831 batch_limit:   8677 Loss:   0.4033 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    832 batch_limit:   8677 Loss:   0.4032 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    833 batch_limit:   8677 Loss:   0.4030 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    834 batch_limit:   8677 Loss:   0.4029 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    835 batch_limit:   8677 Loss:   0.4027 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    836 batch_limit:   8677 Loss:   0.4026 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    837 batch_limit:   8677 Loss:   0.4024 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    838 batch_limit:   8677 Loss:   0.4023 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    839 batch_limit:   8677 Loss:   0.4021 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    840 batch_limit:   8677 Loss:   0.4020 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    841 batch_limit:   8677 Loss:   0.4018 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    842 batch_limit:   8677 Loss:   0.4017 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    843 batch_limit:   8677 Loss:   0.4015 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    844 batch_limit:   8677 Loss:   0.4014 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    845 batch_limit:   8677 Loss:   0.4012 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    846 batch_limit:   8677 Loss:   0.4011 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    847 batch_limit:   8677 Loss:   0.4009 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    848 batch_limit:   8677 Loss:   0.4008 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    849 batch_limit:   8677 Loss:   0.4006 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    850 batch_limit:   8677 Loss:   0.4005 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    851 batch_limit:   8677 Loss:   0.4003 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    852 batch_limit:   8677 Loss:   0.4001 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    853 batch_limit:   8677 Loss:   0.4000 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    854 batch_limit:   8677 Loss:   0.3998 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    855 batch_limit:   8677 Loss:   0.3997 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    856 batch_limit:   8677 Loss:   0.3995 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    857 batch_limit:   8677 Loss:   0.3994 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    858 batch_limit:   8677 Loss:   0.3992 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    859 batch_limit:   8677 Loss:   0.3990 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    860 batch_limit:   8677 Loss:   0.3989 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    861 batch_limit:   8677 Loss:   0.3987 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    862 batch_limit:   8677 Loss:   0.3986 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    863 batch_limit:   8677 Loss:   0.3984 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    864 batch_limit:   8677 Loss:   0.3982 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    865 batch_limit:   8677 Loss:   0.3981 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    866 batch_limit:   8677 Loss:   0.3979 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    867 batch_limit:   8677 Loss:   0.3977 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    868 batch_limit:   8677 Loss:   0.3976 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    869 batch_limit:   8677 Loss:   0.3974 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    870 batch_limit:   8677 Loss:   0.3973 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    871 batch_limit:   8677 Loss:   0.3971 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    872 batch_limit:   8677 Loss:   0.3969 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    873 batch_limit:   8677 Loss:   0.3968 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    874 batch_limit:   8677 Loss:   0.3966 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    875 batch_limit:   8677 Loss:   0.3964 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    876 batch_limit:   8677 Loss:   0.3963 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    877 batch_limit:   8677 Loss:   0.3961 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    878 batch_limit:   8677 Loss:   0.3959 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    879 batch_limit:   8677 Loss:   0.3958 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    880 batch_limit:   8677 Loss:   0.3956 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    881 batch_limit:   8677 Loss:   0.3954 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    882 batch_limit:   8677 Loss:   0.3953 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    883 batch_limit:   8677 Loss:   0.3951 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    884 batch_limit:   8677 Loss:   0.3949 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    885 batch_limit:   8677 Loss:   0.3948 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    886 batch_limit:   8677 Loss:   0.3946 Training Acc:    88.12  6734.99    99.04%\n",
      "Epoch    887 batch_limit:   8677 Loss:   0.3944 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    888 batch_limit:   8677 Loss:   0.3943 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    889 batch_limit:   8677 Loss:   0.3941 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    890 batch_limit:   8677 Loss:   0.3939 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    891 batch_limit:   8677 Loss:   0.3938 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    892 batch_limit:   8677 Loss:   0.3936 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    893 batch_limit:   8677 Loss:   0.3934 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    894 batch_limit:   8677 Loss:   0.3933 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    895 batch_limit:   8677 Loss:   0.3931 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    896 batch_limit:   8677 Loss:   0.3929 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    897 batch_limit:   8677 Loss:   0.3928 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    898 batch_limit:   8677 Loss:   0.3926 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    899 batch_limit:   8677 Loss:   0.3924 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    900 batch_limit:   8677 Loss:   0.3923 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    901 batch_limit:   8677 Loss:   0.3921 Training Acc:    89.11  6735.98    99.06%\n",
      "Epoch    902 batch_limit:   8677 Loss:   0.3919 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    903 batch_limit:   8677 Loss:   0.3918 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    904 batch_limit:   8677 Loss:   0.3916 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    905 batch_limit:   8677 Loss:   0.3914 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    906 batch_limit:   8677 Loss:   0.3913 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    907 batch_limit:   8677 Loss:   0.3911 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    908 batch_limit:   8677 Loss:   0.3909 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    909 batch_limit:   8677 Loss:   0.3907 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    910 batch_limit:   8677 Loss:   0.3906 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    911 batch_limit:   8677 Loss:   0.3904 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    912 batch_limit:   8677 Loss:   0.3902 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    913 batch_limit:   8677 Loss:   0.3901 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    914 batch_limit:   8677 Loss:   0.3899 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    915 batch_limit:   8677 Loss:   0.3897 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    916 batch_limit:   8677 Loss:   0.3896 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    917 batch_limit:   8677 Loss:   0.3894 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    918 batch_limit:   8677 Loss:   0.3892 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    919 batch_limit:   8677 Loss:   0.3891 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    920 batch_limit:   8677 Loss:   0.3889 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    921 batch_limit:   8677 Loss:   0.3887 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    922 batch_limit:   8677 Loss:   0.3886 Training Acc:    90.10  6736.97    99.07%\n",
      "Epoch    923 batch_limit:   8677 Loss:   0.3884 Training Acc:    91.09  6737.96    99.09%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    924 batch_limit:   8677 Loss:   0.3882 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    925 batch_limit:   8677 Loss:   0.3881 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    926 batch_limit:   8677 Loss:   0.3879 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    927 batch_limit:   8677 Loss:   0.3877 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    928 batch_limit:   8677 Loss:   0.3876 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    929 batch_limit:   8677 Loss:   0.3874 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    930 batch_limit:   8677 Loss:   0.3872 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    931 batch_limit:   8677 Loss:   0.3871 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    932 batch_limit:   8677 Loss:   0.3869 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    933 batch_limit:   8677 Loss:   0.3867 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    934 batch_limit:   8677 Loss:   0.3866 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    935 batch_limit:   8677 Loss:   0.3864 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    936 batch_limit:   8677 Loss:   0.3862 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    937 batch_limit:   8677 Loss:   0.3861 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    938 batch_limit:   8677 Loss:   0.3859 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    939 batch_limit:   8677 Loss:   0.3857 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    940 batch_limit:   8677 Loss:   0.3856 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    941 batch_limit:   8677 Loss:   0.3854 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    942 batch_limit:   8677 Loss:   0.3853 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    943 batch_limit:   8677 Loss:   0.3851 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    944 batch_limit:   8677 Loss:   0.3849 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    945 batch_limit:   8677 Loss:   0.3848 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    946 batch_limit:   8677 Loss:   0.3846 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    947 batch_limit:   8677 Loss:   0.3844 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    948 batch_limit:   8677 Loss:   0.3843 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    949 batch_limit:   8677 Loss:   0.3841 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    950 batch_limit:   8677 Loss:   0.3839 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    951 batch_limit:   8677 Loss:   0.3838 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    952 batch_limit:   8677 Loss:   0.3836 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    953 batch_limit:   8677 Loss:   0.3835 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    954 batch_limit:   8677 Loss:   0.3833 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    955 batch_limit:   8677 Loss:   0.3831 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    956 batch_limit:   8677 Loss:   0.3830 Training Acc:    91.09  6737.96    99.09%\n",
      "Epoch    957 batch_limit:   8677 Loss:   0.3828 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    958 batch_limit:   8677 Loss:   0.3827 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    959 batch_limit:   8677 Loss:   0.3825 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    960 batch_limit:   8677 Loss:   0.3823 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    961 batch_limit:   8677 Loss:   0.3822 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    962 batch_limit:   8677 Loss:   0.3820 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    963 batch_limit:   8677 Loss:   0.3818 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    964 batch_limit:   8677 Loss:   0.3817 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    965 batch_limit:   8677 Loss:   0.3815 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    966 batch_limit:   8677 Loss:   0.3814 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    967 batch_limit:   8677 Loss:   0.3812 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    968 batch_limit:   8677 Loss:   0.3811 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    969 batch_limit:   8677 Loss:   0.3809 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    970 batch_limit:   8677 Loss:   0.3807 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    971 batch_limit:   8677 Loss:   0.3806 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    972 batch_limit:   8677 Loss:   0.3804 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    973 batch_limit:   8677 Loss:   0.3803 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    974 batch_limit:   8677 Loss:   0.3801 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    975 batch_limit:   8677 Loss:   0.3799 Training Acc:    92.08  6738.95    99.10%\n",
      "Epoch    976 batch_limit:   8677 Loss:   0.3798 Training Acc:    92.08  6739.74    99.11%\n",
      "Epoch    977 batch_limit:   8677 Loss:   0.3796 Training Acc:    92.08  6739.74    99.11%\n",
      "Epoch    978 batch_limit:   8677 Loss:   0.3795 Training Acc:    92.08  6739.74    99.11%\n",
      "Epoch    979 batch_limit:   8677 Loss:   0.3793 Training Acc:    92.08  6739.74    99.11%\n",
      "Epoch    980 batch_limit:   8677 Loss:   0.3792 Training Acc:    92.08  6739.74    99.11%\n",
      "Epoch    981 batch_limit:   8677 Loss:   0.3790 Training Acc:    92.08  6740.52    99.13%\n",
      "Epoch    982 batch_limit:   8677 Loss:   0.3788 Training Acc:    92.08  6740.52    99.13%\n",
      "Epoch    983 batch_limit:   8677 Loss:   0.3787 Training Acc:    92.08  6740.52    99.13%\n",
      "Epoch    984 batch_limit:   8677 Loss:   0.3785 Training Acc:    92.08  6740.52    99.13%\n",
      "Epoch    985 batch_limit:   8677 Loss:   0.3784 Training Acc:    92.08  6740.52    99.13%\n",
      "Epoch    986 batch_limit:   8677 Loss:   0.3782 Training Acc:    92.08  6740.52    99.13%\n",
      "Epoch    987 batch_limit:   8677 Loss:   0.3781 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    988 batch_limit:   8677 Loss:   0.3779 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    989 batch_limit:   8677 Loss:   0.3777 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    990 batch_limit:   8677 Loss:   0.3776 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    991 batch_limit:   8677 Loss:   0.3774 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    992 batch_limit:   8677 Loss:   0.3773 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    993 batch_limit:   8677 Loss:   0.3771 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    994 batch_limit:   8677 Loss:   0.3770 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    995 batch_limit:   8677 Loss:   0.3768 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    996 batch_limit:   8677 Loss:   0.3767 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    997 batch_limit:   8677 Loss:   0.3765 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    998 batch_limit:   8677 Loss:   0.3764 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch    999 batch_limit:   8677 Loss:   0.3762 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch   1000 batch_limit:   8677 Loss:   0.3760 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch   1001 batch_limit:   8677 Loss:   0.3759 Training Acc:    92.08  6741.30    99.14%\n",
      "Epoch   1002 batch_limit:   8677 Loss:   0.3757 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1003 batch_limit:   8677 Loss:   0.3756 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1004 batch_limit:   8677 Loss:   0.3754 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1005 batch_limit:   8677 Loss:   0.3753 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1006 batch_limit:   8677 Loss:   0.3751 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1007 batch_limit:   8677 Loss:   0.3750 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1008 batch_limit:   8677 Loss:   0.3748 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1009 batch_limit:   8677 Loss:   0.3747 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1010 batch_limit:   8677 Loss:   0.3745 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1011 batch_limit:   8677 Loss:   0.3744 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1012 batch_limit:   8677 Loss:   0.3742 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1013 batch_limit:   8677 Loss:   0.3740 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1014 batch_limit:   8677 Loss:   0.3739 Training Acc:    93.07  6742.29    99.15%\n",
      "Epoch   1015 batch_limit:   8677 Loss:   0.3737 Training Acc:    93.07  6742.29    99.15%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1016 batch_limit:   8677 Loss:   0.3736 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1017 batch_limit:   8677 Loss:   0.3734 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1018 batch_limit:   8677 Loss:   0.3733 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1019 batch_limit:   8677 Loss:   0.3731 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1020 batch_limit:   8677 Loss:   0.3730 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1021 batch_limit:   8677 Loss:   0.3728 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1022 batch_limit:   8677 Loss:   0.3727 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1023 batch_limit:   8677 Loss:   0.3725 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1024 batch_limit:   8677 Loss:   0.3724 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1025 batch_limit:   8677 Loss:   0.3722 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1026 batch_limit:   8677 Loss:   0.3721 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1027 batch_limit:   8677 Loss:   0.3719 Training Acc:    93.07  6743.07    99.16%\n",
      "Epoch   1028 batch_limit:   8677 Loss:   0.3718 Training Acc:    93.07  6743.85    99.17%\n",
      "Epoch   1029 batch_limit:   8677 Loss:   0.3716 Training Acc:    93.07  6743.85    99.17%\n",
      "Epoch   1030 batch_limit:   8677 Loss:   0.3715 Training Acc:    93.07  6744.63    99.19%\n",
      "Epoch   1031 batch_limit:   8677 Loss:   0.3713 Training Acc:    93.07  6744.63    99.19%\n",
      "Epoch   1032 batch_limit:   8677 Loss:   0.3711 Training Acc:    93.07  6744.63    99.19%\n",
      "Epoch   1033 batch_limit:   8677 Loss:   0.3710 Training Acc:    93.07  6744.63    99.19%\n",
      "Epoch   1034 batch_limit:   8677 Loss:   0.3708 Training Acc:    93.07  6744.63    99.19%\n",
      "Epoch   1035 batch_limit:   8677 Loss:   0.3707 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1036 batch_limit:   8677 Loss:   0.3705 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1037 batch_limit:   8677 Loss:   0.3704 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1038 batch_limit:   8677 Loss:   0.3702 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1039 batch_limit:   8677 Loss:   0.3701 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1040 batch_limit:   8677 Loss:   0.3699 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1041 batch_limit:   8677 Loss:   0.3698 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1042 batch_limit:   8677 Loss:   0.3696 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1043 batch_limit:   8677 Loss:   0.3695 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1044 batch_limit:   8677 Loss:   0.3693 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1045 batch_limit:   8677 Loss:   0.3692 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1046 batch_limit:   8677 Loss:   0.3690 Training Acc:    93.07  6745.41    99.20%\n",
      "Epoch   1047 batch_limit:   8677 Loss:   0.3689 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1048 batch_limit:   8677 Loss:   0.3687 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1049 batch_limit:   8677 Loss:   0.3686 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1050 batch_limit:   8677 Loss:   0.3684 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1051 batch_limit:   8677 Loss:   0.3682 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1052 batch_limit:   8677 Loss:   0.3681 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1053 batch_limit:   8677 Loss:   0.3679 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1054 batch_limit:   8677 Loss:   0.3678 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1055 batch_limit:   8677 Loss:   0.3676 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1056 batch_limit:   8677 Loss:   0.3675 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1057 batch_limit:   8677 Loss:   0.3673 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1058 batch_limit:   8677 Loss:   0.3672 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1059 batch_limit:   8677 Loss:   0.3670 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1060 batch_limit:   8677 Loss:   0.3669 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1061 batch_limit:   8677 Loss:   0.3667 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1062 batch_limit:   8677 Loss:   0.3665 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1063 batch_limit:   8677 Loss:   0.3664 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1064 batch_limit:   8677 Loss:   0.3662 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1065 batch_limit:   8677 Loss:   0.3661 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1066 batch_limit:   8677 Loss:   0.3659 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1067 batch_limit:   8677 Loss:   0.3658 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1068 batch_limit:   8677 Loss:   0.3656 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1069 batch_limit:   8677 Loss:   0.3655 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1070 batch_limit:   8677 Loss:   0.3653 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1071 batch_limit:   8677 Loss:   0.3651 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1072 batch_limit:   8677 Loss:   0.3650 Training Acc:    93.07  6746.19    99.21%\n",
      "Epoch   1073 batch_limit:   8677 Loss:   0.3648 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1074 batch_limit:   8677 Loss:   0.3647 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1075 batch_limit:   8677 Loss:   0.3645 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1076 batch_limit:   8677 Loss:   0.3644 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1077 batch_limit:   8677 Loss:   0.3642 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1078 batch_limit:   8677 Loss:   0.3640 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1079 batch_limit:   8677 Loss:   0.3639 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1080 batch_limit:   8677 Loss:   0.3637 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1081 batch_limit:   8677 Loss:   0.3636 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1082 batch_limit:   8677 Loss:   0.3634 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1083 batch_limit:   8677 Loss:   0.3633 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1084 batch_limit:   8677 Loss:   0.3631 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1085 batch_limit:   8677 Loss:   0.3629 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1086 batch_limit:   8677 Loss:   0.3628 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1087 batch_limit:   8677 Loss:   0.3626 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1088 batch_limit:   8677 Loss:   0.3625 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1089 batch_limit:   8677 Loss:   0.3623 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1090 batch_limit:   8677 Loss:   0.3621 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1091 batch_limit:   8677 Loss:   0.3620 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1092 batch_limit:   8677 Loss:   0.3618 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1093 batch_limit:   8677 Loss:   0.3617 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1094 batch_limit:   8677 Loss:   0.3615 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1095 batch_limit:   8677 Loss:   0.3613 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1096 batch_limit:   8677 Loss:   0.3612 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1097 batch_limit:   8677 Loss:   0.3610 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1098 batch_limit:   8677 Loss:   0.3608 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1099 batch_limit:   8677 Loss:   0.3607 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1100 batch_limit:   8677 Loss:   0.3605 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1101 batch_limit:   8677 Loss:   0.3604 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1102 batch_limit:   8677 Loss:   0.3602 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1103 batch_limit:   8677 Loss:   0.3600 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1104 batch_limit:   8677 Loss:   0.3599 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1105 batch_limit:   8677 Loss:   0.3597 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1106 batch_limit:   8677 Loss:   0.3595 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1107 batch_limit:   8677 Loss:   0.3594 Training Acc:    94.06  6747.18    99.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1108 batch_limit:   8677 Loss:   0.3592 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1109 batch_limit:   8677 Loss:   0.3590 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1110 batch_limit:   8677 Loss:   0.3589 Training Acc:    94.06  6747.18    99.22%\n",
      "Epoch   1111 batch_limit:   8677 Loss:   0.3587 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1112 batch_limit:   8677 Loss:   0.3585 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1113 batch_limit:   8677 Loss:   0.3584 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1114 batch_limit:   8677 Loss:   0.3582 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1115 batch_limit:   8677 Loss:   0.3580 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1116 batch_limit:   8677 Loss:   0.3579 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1117 batch_limit:   8677 Loss:   0.3577 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1118 batch_limit:   8677 Loss:   0.3575 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1119 batch_limit:   8677 Loss:   0.3574 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1120 batch_limit:   8677 Loss:   0.3572 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1121 batch_limit:   8677 Loss:   0.3570 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1122 batch_limit:   8677 Loss:   0.3568 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1123 batch_limit:   8677 Loss:   0.3567 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1124 batch_limit:   8677 Loss:   0.3565 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1125 batch_limit:   8677 Loss:   0.3563 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1126 batch_limit:   8677 Loss:   0.3562 Training Acc:    94.06  6747.97    99.23%\n",
      "Epoch   1127 batch_limit:   8677 Loss:   0.3560 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1128 batch_limit:   8677 Loss:   0.3558 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1129 batch_limit:   8677 Loss:   0.3556 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1130 batch_limit:   8677 Loss:   0.3555 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1131 batch_limit:   8677 Loss:   0.3553 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1132 batch_limit:   8677 Loss:   0.3551 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1133 batch_limit:   8677 Loss:   0.3549 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1134 batch_limit:   8677 Loss:   0.3548 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1135 batch_limit:   8677 Loss:   0.3546 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1136 batch_limit:   8677 Loss:   0.3544 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1137 batch_limit:   8677 Loss:   0.3543 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1138 batch_limit:   8677 Loss:   0.3541 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1139 batch_limit:   8677 Loss:   0.3539 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1140 batch_limit:   8677 Loss:   0.3537 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1141 batch_limit:   8677 Loss:   0.3535 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1142 batch_limit:   8677 Loss:   0.3534 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1143 batch_limit:   8677 Loss:   0.3532 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1144 batch_limit:   8677 Loss:   0.3530 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1145 batch_limit:   8677 Loss:   0.3528 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1146 batch_limit:   8677 Loss:   0.3527 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1147 batch_limit:   8677 Loss:   0.3525 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1148 batch_limit:   8677 Loss:   0.3523 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1149 batch_limit:   8677 Loss:   0.3521 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1150 batch_limit:   8677 Loss:   0.3519 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1151 batch_limit:   8677 Loss:   0.3518 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1152 batch_limit:   8677 Loss:   0.3516 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1153 batch_limit:   8677 Loss:   0.3514 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1154 batch_limit:   8677 Loss:   0.3512 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1155 batch_limit:   8677 Loss:   0.3510 Training Acc:    94.06  6748.75    99.25%\n",
      "Epoch   1156 batch_limit:   8677 Loss:   0.3509 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1157 batch_limit:   8677 Loss:   0.3507 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1158 batch_limit:   8677 Loss:   0.3505 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1159 batch_limit:   8677 Loss:   0.3503 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1160 batch_limit:   8677 Loss:   0.3501 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1161 batch_limit:   8677 Loss:   0.3500 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1162 batch_limit:   8677 Loss:   0.3498 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1163 batch_limit:   8677 Loss:   0.3496 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1164 batch_limit:   8677 Loss:   0.3494 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1165 batch_limit:   8677 Loss:   0.3492 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1166 batch_limit:   8677 Loss:   0.3490 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1167 batch_limit:   8677 Loss:   0.3489 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1168 batch_limit:   8677 Loss:   0.3487 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1169 batch_limit:   8677 Loss:   0.3485 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1170 batch_limit:   8677 Loss:   0.3483 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1171 batch_limit:   8677 Loss:   0.3481 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1172 batch_limit:   8677 Loss:   0.3479 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1173 batch_limit:   8677 Loss:   0.3477 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1174 batch_limit:   8677 Loss:   0.3476 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1175 batch_limit:   8677 Loss:   0.3474 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1176 batch_limit:   8677 Loss:   0.3472 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1177 batch_limit:   8677 Loss:   0.3470 Training Acc:    95.05  6749.74    99.26%\n",
      "Epoch   1178 batch_limit:   8677 Loss:   0.3468 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1179 batch_limit:   8677 Loss:   0.3466 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1180 batch_limit:   8677 Loss:   0.3464 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1181 batch_limit:   8677 Loss:   0.3462 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1182 batch_limit:   8677 Loss:   0.3461 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1183 batch_limit:   8677 Loss:   0.3459 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1184 batch_limit:   8677 Loss:   0.3457 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1185 batch_limit:   8677 Loss:   0.3455 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1186 batch_limit:   8677 Loss:   0.3453 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1187 batch_limit:   8677 Loss:   0.3451 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1188 batch_limit:   8677 Loss:   0.3449 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1189 batch_limit:   8677 Loss:   0.3447 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1190 batch_limit:   8677 Loss:   0.3445 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1191 batch_limit:   8677 Loss:   0.3443 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1192 batch_limit:   8677 Loss:   0.3442 Training Acc:    95.05  6750.52    99.27%\n",
      "Epoch   1193 batch_limit:   8677 Loss:   0.3440 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1194 batch_limit:   8677 Loss:   0.3438 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1195 batch_limit:   8677 Loss:   0.3436 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1196 batch_limit:   8677 Loss:   0.3434 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1197 batch_limit:   8677 Loss:   0.3432 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1198 batch_limit:   8677 Loss:   0.3430 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1199 batch_limit:   8677 Loss:   0.3428 Training Acc:    95.05  6751.30    99.28%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1200 batch_limit:   8677 Loss:   0.3426 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1201 batch_limit:   8677 Loss:   0.3424 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1202 batch_limit:   8677 Loss:   0.3422 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1203 batch_limit:   8677 Loss:   0.3420 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1204 batch_limit:   8677 Loss:   0.3419 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1205 batch_limit:   8677 Loss:   0.3417 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1206 batch_limit:   8677 Loss:   0.3415 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1207 batch_limit:   8677 Loss:   0.3413 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1208 batch_limit:   8677 Loss:   0.3411 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1209 batch_limit:   8677 Loss:   0.3409 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1210 batch_limit:   8677 Loss:   0.3407 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1211 batch_limit:   8677 Loss:   0.3405 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1212 batch_limit:   8677 Loss:   0.3403 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1213 batch_limit:   8677 Loss:   0.3401 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1214 batch_limit:   8677 Loss:   0.3399 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1215 batch_limit:   8677 Loss:   0.3397 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1216 batch_limit:   8677 Loss:   0.3395 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1217 batch_limit:   8677 Loss:   0.3393 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1218 batch_limit:   8677 Loss:   0.3391 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1219 batch_limit:   8677 Loss:   0.3389 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1220 batch_limit:   8677 Loss:   0.3388 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1221 batch_limit:   8677 Loss:   0.3386 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1222 batch_limit:   8677 Loss:   0.3384 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1223 batch_limit:   8677 Loss:   0.3382 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1224 batch_limit:   8677 Loss:   0.3380 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1225 batch_limit:   8677 Loss:   0.3378 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1226 batch_limit:   8677 Loss:   0.3376 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1227 batch_limit:   8677 Loss:   0.3374 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1228 batch_limit:   8677 Loss:   0.3372 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1229 batch_limit:   8677 Loss:   0.3370 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1230 batch_limit:   8677 Loss:   0.3368 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1231 batch_limit:   8677 Loss:   0.3366 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1232 batch_limit:   8677 Loss:   0.3364 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1233 batch_limit:   8677 Loss:   0.3362 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1234 batch_limit:   8677 Loss:   0.3360 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1235 batch_limit:   8677 Loss:   0.3358 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1236 batch_limit:   8677 Loss:   0.3356 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1237 batch_limit:   8677 Loss:   0.3354 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1238 batch_limit:   8677 Loss:   0.3353 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1239 batch_limit:   8677 Loss:   0.3351 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1240 batch_limit:   8677 Loss:   0.3349 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1241 batch_limit:   8677 Loss:   0.3347 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1242 batch_limit:   8677 Loss:   0.3345 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1243 batch_limit:   8677 Loss:   0.3343 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1244 batch_limit:   8677 Loss:   0.3341 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1245 batch_limit:   8677 Loss:   0.3339 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1246 batch_limit:   8677 Loss:   0.3337 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1247 batch_limit:   8677 Loss:   0.3335 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1248 batch_limit:   8677 Loss:   0.3333 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1249 batch_limit:   8677 Loss:   0.3331 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1250 batch_limit:   8677 Loss:   0.3329 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1251 batch_limit:   8677 Loss:   0.3327 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1252 batch_limit:   8677 Loss:   0.3325 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1253 batch_limit:   8677 Loss:   0.3323 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1254 batch_limit:   8677 Loss:   0.3321 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1255 batch_limit:   8677 Loss:   0.3320 Training Acc:    95.05  6751.30    99.28%\n",
      "Epoch   1256 batch_limit:   8677 Loss:   0.3318 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1257 batch_limit:   8677 Loss:   0.3316 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1258 batch_limit:   8677 Loss:   0.3314 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1259 batch_limit:   8677 Loss:   0.3312 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1260 batch_limit:   8677 Loss:   0.3310 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1261 batch_limit:   8677 Loss:   0.3308 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1262 batch_limit:   8677 Loss:   0.3306 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1263 batch_limit:   8677 Loss:   0.3304 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1264 batch_limit:   8677 Loss:   0.3302 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1265 batch_limit:   8677 Loss:   0.3300 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1266 batch_limit:   8677 Loss:   0.3298 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1267 batch_limit:   8677 Loss:   0.3296 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1268 batch_limit:   8677 Loss:   0.3295 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1269 batch_limit:   8677 Loss:   0.3293 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1270 batch_limit:   8677 Loss:   0.3291 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1271 batch_limit:   8677 Loss:   0.3289 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1272 batch_limit:   8677 Loss:   0.3287 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1273 batch_limit:   8677 Loss:   0.3285 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1274 batch_limit:   8677 Loss:   0.3283 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1275 batch_limit:   8677 Loss:   0.3281 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1276 batch_limit:   8677 Loss:   0.3279 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1277 batch_limit:   8677 Loss:   0.3277 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1278 batch_limit:   8677 Loss:   0.3275 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1279 batch_limit:   8677 Loss:   0.3274 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1280 batch_limit:   8677 Loss:   0.3272 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1281 batch_limit:   8677 Loss:   0.3270 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1282 batch_limit:   8677 Loss:   0.3268 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1283 batch_limit:   8677 Loss:   0.3266 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1284 batch_limit:   8677 Loss:   0.3264 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1285 batch_limit:   8677 Loss:   0.3262 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1286 batch_limit:   8677 Loss:   0.3260 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1287 batch_limit:   8677 Loss:   0.3259 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1288 batch_limit:   8677 Loss:   0.3257 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1289 batch_limit:   8677 Loss:   0.3255 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1290 batch_limit:   8677 Loss:   0.3253 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1291 batch_limit:   8677 Loss:   0.3251 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1292 batch_limit:   8677 Loss:   0.3249 Training Acc:    96.04  6752.29    99.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1293 batch_limit:   8677 Loss:   0.3247 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1294 batch_limit:   8677 Loss:   0.3245 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1295 batch_limit:   8677 Loss:   0.3244 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1296 batch_limit:   8677 Loss:   0.3242 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1297 batch_limit:   8677 Loss:   0.3240 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1298 batch_limit:   8677 Loss:   0.3238 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1299 batch_limit:   8677 Loss:   0.3236 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1300 batch_limit:   8677 Loss:   0.3234 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1301 batch_limit:   8677 Loss:   0.3233 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1302 batch_limit:   8677 Loss:   0.3231 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1303 batch_limit:   8677 Loss:   0.3229 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1304 batch_limit:   8677 Loss:   0.3227 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1305 batch_limit:   8677 Loss:   0.3225 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1306 batch_limit:   8677 Loss:   0.3223 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1307 batch_limit:   8677 Loss:   0.3222 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1308 batch_limit:   8677 Loss:   0.3220 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1309 batch_limit:   8677 Loss:   0.3218 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1310 batch_limit:   8677 Loss:   0.3216 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1311 batch_limit:   8677 Loss:   0.3214 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1312 batch_limit:   8677 Loss:   0.3212 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1313 batch_limit:   8677 Loss:   0.3211 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1314 batch_limit:   8677 Loss:   0.3209 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1315 batch_limit:   8677 Loss:   0.3207 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1316 batch_limit:   8677 Loss:   0.3205 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1317 batch_limit:   8677 Loss:   0.3203 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1318 batch_limit:   8677 Loss:   0.3202 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1319 batch_limit:   8677 Loss:   0.3200 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1320 batch_limit:   8677 Loss:   0.3198 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1321 batch_limit:   8677 Loss:   0.3196 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1322 batch_limit:   8677 Loss:   0.3195 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1323 batch_limit:   8677 Loss:   0.3193 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1324 batch_limit:   8677 Loss:   0.3191 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1325 batch_limit:   8677 Loss:   0.3189 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1326 batch_limit:   8677 Loss:   0.3187 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1327 batch_limit:   8677 Loss:   0.3186 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1328 batch_limit:   8677 Loss:   0.3184 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1329 batch_limit:   8677 Loss:   0.3182 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1330 batch_limit:   8677 Loss:   0.3180 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1331 batch_limit:   8677 Loss:   0.3179 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1332 batch_limit:   8677 Loss:   0.3177 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1333 batch_limit:   8677 Loss:   0.3175 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1334 batch_limit:   8677 Loss:   0.3173 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1335 batch_limit:   8677 Loss:   0.3172 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1336 batch_limit:   8677 Loss:   0.3170 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1337 batch_limit:   8677 Loss:   0.3168 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1338 batch_limit:   8677 Loss:   0.3167 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1339 batch_limit:   8677 Loss:   0.3165 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1340 batch_limit:   8677 Loss:   0.3163 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1341 batch_limit:   8677 Loss:   0.3161 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1342 batch_limit:   8677 Loss:   0.3160 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1343 batch_limit:   8677 Loss:   0.3158 Training Acc:    96.04  6752.29    99.30%\n",
      "Epoch   1344 batch_limit:   8677 Loss:   0.3156 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1345 batch_limit:   8677 Loss:   0.3155 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1346 batch_limit:   8677 Loss:   0.3153 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1347 batch_limit:   8677 Loss:   0.3151 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1348 batch_limit:   8677 Loss:   0.3149 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1349 batch_limit:   8677 Loss:   0.3148 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1350 batch_limit:   8677 Loss:   0.3146 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1351 batch_limit:   8677 Loss:   0.3144 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1352 batch_limit:   8677 Loss:   0.3143 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1353 batch_limit:   8677 Loss:   0.3141 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1354 batch_limit:   8677 Loss:   0.3139 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1355 batch_limit:   8677 Loss:   0.3138 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1356 batch_limit:   8677 Loss:   0.3136 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1357 batch_limit:   8677 Loss:   0.3134 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1358 batch_limit:   8677 Loss:   0.3133 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1359 batch_limit:   8677 Loss:   0.3131 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1360 batch_limit:   8677 Loss:   0.3129 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1361 batch_limit:   8677 Loss:   0.3128 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1362 batch_limit:   8677 Loss:   0.3126 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1363 batch_limit:   8677 Loss:   0.3125 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1364 batch_limit:   8677 Loss:   0.3123 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1365 batch_limit:   8677 Loss:   0.3121 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1366 batch_limit:   8677 Loss:   0.3120 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1367 batch_limit:   8677 Loss:   0.3118 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1368 batch_limit:   8677 Loss:   0.3116 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1369 batch_limit:   8677 Loss:   0.3115 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1370 batch_limit:   8677 Loss:   0.3113 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1371 batch_limit:   8677 Loss:   0.3111 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1372 batch_limit:   8677 Loss:   0.3110 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1373 batch_limit:   8677 Loss:   0.3108 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1374 batch_limit:   8677 Loss:   0.3107 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1375 batch_limit:   8677 Loss:   0.3105 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1376 batch_limit:   8677 Loss:   0.3103 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1377 batch_limit:   8677 Loss:   0.3102 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1378 batch_limit:   8677 Loss:   0.3100 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1379 batch_limit:   8677 Loss:   0.3099 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1380 batch_limit:   8677 Loss:   0.3097 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1381 batch_limit:   8677 Loss:   0.3095 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1382 batch_limit:   8677 Loss:   0.3094 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1383 batch_limit:   8677 Loss:   0.3092 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1384 batch_limit:   8677 Loss:   0.3091 Training Acc:    96.04  6753.07    99.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1385 batch_limit:   8677 Loss:   0.3089 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1386 batch_limit:   8677 Loss:   0.3088 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1387 batch_limit:   8677 Loss:   0.3086 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1388 batch_limit:   8677 Loss:   0.3085 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1389 batch_limit:   8677 Loss:   0.3083 Training Acc:    96.04  6753.07    99.31%\n",
      "Epoch   1390 batch_limit:   8677 Loss:   0.3081 Training Acc:    97.03  6754.84    99.34%\n",
      "Epoch   1391 batch_limit:   8677 Loss:   0.3080 Training Acc:    97.03  6754.84    99.34%\n",
      "Epoch   1392 batch_limit:   8677 Loss:   0.3078 Training Acc:    97.03  6754.84    99.34%\n",
      "Epoch   1393 batch_limit:   8677 Loss:   0.3077 Training Acc:    97.03  6754.84    99.34%\n",
      "Epoch   1394 batch_limit:   8677 Loss:   0.3075 Training Acc:    97.03  6754.84    99.34%\n",
      "Epoch   1395 batch_limit:   8677 Loss:   0.3074 Training Acc:    97.03  6754.84    99.34%\n",
      "Epoch   1396 batch_limit:   8677 Loss:   0.3072 Training Acc:    97.03  6754.84    99.34%\n",
      "Epoch   1397 batch_limit:   8677 Loss:   0.3071 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1398 batch_limit:   8677 Loss:   0.3069 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1399 batch_limit:   8677 Loss:   0.3068 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1400 batch_limit:   8677 Loss:   0.3066 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1401 batch_limit:   8677 Loss:   0.3065 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1402 batch_limit:   8677 Loss:   0.3063 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1403 batch_limit:   8677 Loss:   0.3061 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1404 batch_limit:   8677 Loss:   0.3060 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1405 batch_limit:   8677 Loss:   0.3058 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1406 batch_limit:   8677 Loss:   0.3057 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1407 batch_limit:   8677 Loss:   0.3055 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1408 batch_limit:   8677 Loss:   0.3054 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1409 batch_limit:   8677 Loss:   0.3052 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1410 batch_limit:   8677 Loss:   0.3051 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1411 batch_limit:   8677 Loss:   0.3050 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1412 batch_limit:   8677 Loss:   0.3048 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1413 batch_limit:   8677 Loss:   0.3047 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1414 batch_limit:   8677 Loss:   0.3045 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1415 batch_limit:   8677 Loss:   0.3044 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1416 batch_limit:   8677 Loss:   0.3042 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1417 batch_limit:   8677 Loss:   0.3041 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1418 batch_limit:   8677 Loss:   0.3039 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1419 batch_limit:   8677 Loss:   0.3038 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1420 batch_limit:   8677 Loss:   0.3036 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1421 batch_limit:   8677 Loss:   0.3035 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1422 batch_limit:   8677 Loss:   0.3033 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1423 batch_limit:   8677 Loss:   0.3032 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1424 batch_limit:   8677 Loss:   0.3030 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1425 batch_limit:   8677 Loss:   0.3029 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1426 batch_limit:   8677 Loss:   0.3028 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1427 batch_limit:   8677 Loss:   0.3026 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1428 batch_limit:   8677 Loss:   0.3025 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1429 batch_limit:   8677 Loss:   0.3023 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1430 batch_limit:   8677 Loss:   0.3022 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1431 batch_limit:   8677 Loss:   0.3020 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1432 batch_limit:   8677 Loss:   0.3019 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1433 batch_limit:   8677 Loss:   0.3018 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1434 batch_limit:   8677 Loss:   0.3016 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1435 batch_limit:   8677 Loss:   0.3015 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1436 batch_limit:   8677 Loss:   0.3013 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1437 batch_limit:   8677 Loss:   0.3012 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1438 batch_limit:   8677 Loss:   0.3011 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1439 batch_limit:   8677 Loss:   0.3009 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1440 batch_limit:   8677 Loss:   0.3008 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1441 batch_limit:   8677 Loss:   0.3006 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1442 batch_limit:   8677 Loss:   0.3005 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1443 batch_limit:   8677 Loss:   0.3004 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1444 batch_limit:   8677 Loss:   0.3002 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1445 batch_limit:   8677 Loss:   0.3001 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1446 batch_limit:   8677 Loss:   0.2999 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1447 batch_limit:   8677 Loss:   0.2998 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1448 batch_limit:   8677 Loss:   0.2997 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1449 batch_limit:   8677 Loss:   0.2995 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1450 batch_limit:   8677 Loss:   0.2994 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1451 batch_limit:   8677 Loss:   0.2992 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1452 batch_limit:   8677 Loss:   0.2991 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1453 batch_limit:   8677 Loss:   0.2990 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1454 batch_limit:   8677 Loss:   0.2988 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1455 batch_limit:   8677 Loss:   0.2987 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1456 batch_limit:   8677 Loss:   0.2986 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1457 batch_limit:   8677 Loss:   0.2984 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1458 batch_limit:   8677 Loss:   0.2983 Training Acc:    97.03  6755.62    99.35%\n",
      "Epoch   1459 batch_limit:   8677 Loss:   0.2982 Training Acc:    97.03  6756.40    99.36%\n",
      "Epoch   1460 batch_limit:   8677 Loss:   0.2980 Training Acc:    97.03  6756.40    99.36%\n",
      "Epoch   1461 batch_limit:   8677 Loss:   0.2979 Training Acc:    97.03  6756.40    99.36%\n",
      "Epoch   1462 batch_limit:   8677 Loss:   0.2978 Training Acc:    97.03  6756.40    99.36%\n",
      "Epoch   1463 batch_limit:   8677 Loss:   0.2976 Training Acc:    97.03  6756.40    99.36%\n",
      "Epoch   1464 batch_limit:   8677 Loss:   0.2975 Training Acc:    97.03  6756.40    99.36%\n",
      "Epoch   1465 batch_limit:   8677 Loss:   0.2974 Training Acc:    97.03  6756.40    99.36%\n",
      "Epoch   1466 batch_limit:   8677 Loss:   0.2972 Training Acc:    97.03  6756.40    99.36%\n",
      "Epoch   1467 batch_limit:   8677 Loss:   0.2971 Training Acc:    97.03  6756.40    99.36%\n",
      "Epoch   1468 batch_limit:   8677 Loss:   0.2970 Training Acc:    97.03  6756.40    99.36%\n",
      "Epoch   1469 batch_limit:   8677 Loss:   0.2968 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1470 batch_limit:   8677 Loss:   0.2967 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1471 batch_limit:   8677 Loss:   0.2966 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1472 batch_limit:   8677 Loss:   0.2964 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1473 batch_limit:   8677 Loss:   0.2963 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1474 batch_limit:   8677 Loss:   0.2962 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1475 batch_limit:   8677 Loss:   0.2960 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1476 batch_limit:   8677 Loss:   0.2959 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1477 batch_limit:   8677 Loss:   0.2958 Training Acc:    97.03  6757.19    99.37%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1478 batch_limit:   8677 Loss:   0.2957 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1479 batch_limit:   8677 Loss:   0.2955 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1480 batch_limit:   8677 Loss:   0.2954 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1481 batch_limit:   8677 Loss:   0.2953 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1482 batch_limit:   8677 Loss:   0.2951 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1483 batch_limit:   8677 Loss:   0.2950 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1484 batch_limit:   8677 Loss:   0.2949 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1485 batch_limit:   8677 Loss:   0.2948 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1486 batch_limit:   8677 Loss:   0.2946 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1487 batch_limit:   8677 Loss:   0.2945 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1488 batch_limit:   8677 Loss:   0.2944 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1489 batch_limit:   8677 Loss:   0.2942 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1490 batch_limit:   8677 Loss:   0.2941 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1491 batch_limit:   8677 Loss:   0.2940 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1492 batch_limit:   8677 Loss:   0.2939 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1493 batch_limit:   8677 Loss:   0.2937 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1494 batch_limit:   8677 Loss:   0.2936 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1495 batch_limit:   8677 Loss:   0.2935 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1496 batch_limit:   8677 Loss:   0.2934 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1497 batch_limit:   8677 Loss:   0.2932 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1498 batch_limit:   8677 Loss:   0.2931 Training Acc:    97.03  6757.19    99.37%\n",
      "Epoch   1499 batch_limit:   8677 Loss:   0.2930 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1500 batch_limit:   8677 Loss:   0.2929 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1501 batch_limit:   8677 Loss:   0.2927 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1502 batch_limit:   8677 Loss:   0.2926 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1503 batch_limit:   8677 Loss:   0.2925 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1504 batch_limit:   8677 Loss:   0.2924 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1505 batch_limit:   8677 Loss:   0.2922 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1506 batch_limit:   8677 Loss:   0.2921 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1507 batch_limit:   8677 Loss:   0.2920 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1508 batch_limit:   8677 Loss:   0.2919 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1509 batch_limit:   8677 Loss:   0.2918 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1510 batch_limit:   8677 Loss:   0.2916 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1511 batch_limit:   8677 Loss:   0.2915 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1512 batch_limit:   8677 Loss:   0.2914 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1513 batch_limit:   8677 Loss:   0.2913 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1514 batch_limit:   8677 Loss:   0.2911 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1515 batch_limit:   8677 Loss:   0.2910 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1516 batch_limit:   8677 Loss:   0.2909 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1517 batch_limit:   8677 Loss:   0.2908 Training Acc:    97.03  6757.97    99.38%\n",
      "Epoch   1518 batch_limit:   8677 Loss:   0.2907 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1519 batch_limit:   8677 Loss:   0.2905 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1520 batch_limit:   8677 Loss:   0.2904 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1521 batch_limit:   8677 Loss:   0.2903 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1522 batch_limit:   8677 Loss:   0.2902 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1523 batch_limit:   8677 Loss:   0.2901 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1524 batch_limit:   8677 Loss:   0.2900 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1525 batch_limit:   8677 Loss:   0.2898 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1526 batch_limit:   8677 Loss:   0.2897 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1527 batch_limit:   8677 Loss:   0.2896 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1528 batch_limit:   8677 Loss:   0.2895 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1529 batch_limit:   8677 Loss:   0.2894 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1530 batch_limit:   8677 Loss:   0.2892 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1531 batch_limit:   8677 Loss:   0.2891 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1532 batch_limit:   8677 Loss:   0.2890 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1533 batch_limit:   8677 Loss:   0.2889 Training Acc:    97.03  6758.75    99.39%\n",
      "Epoch   1534 batch_limit:   8677 Loss:   0.2888 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1535 batch_limit:   8677 Loss:   0.2887 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1536 batch_limit:   8677 Loss:   0.2885 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1537 batch_limit:   8677 Loss:   0.2884 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1538 batch_limit:   8677 Loss:   0.2883 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1539 batch_limit:   8677 Loss:   0.2882 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1540 batch_limit:   8677 Loss:   0.2881 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1541 batch_limit:   8677 Loss:   0.2880 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1542 batch_limit:   8677 Loss:   0.2879 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1543 batch_limit:   8677 Loss:   0.2877 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1544 batch_limit:   8677 Loss:   0.2876 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1545 batch_limit:   8677 Loss:   0.2875 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1546 batch_limit:   8677 Loss:   0.2874 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1547 batch_limit:   8677 Loss:   0.2873 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1548 batch_limit:   8677 Loss:   0.2872 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1549 batch_limit:   8677 Loss:   0.2871 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1550 batch_limit:   8677 Loss:   0.2869 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1551 batch_limit:   8677 Loss:   0.2868 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1552 batch_limit:   8677 Loss:   0.2867 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1553 batch_limit:   8677 Loss:   0.2866 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1554 batch_limit:   8677 Loss:   0.2865 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1555 batch_limit:   8677 Loss:   0.2864 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1556 batch_limit:   8677 Loss:   0.2863 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1557 batch_limit:   8677 Loss:   0.2862 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1558 batch_limit:   8677 Loss:   0.2860 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1559 batch_limit:   8677 Loss:   0.2859 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1560 batch_limit:   8677 Loss:   0.2858 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1561 batch_limit:   8677 Loss:   0.2857 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1562 batch_limit:   8677 Loss:   0.2856 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1563 batch_limit:   8677 Loss:   0.2855 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1564 batch_limit:   8677 Loss:   0.2854 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1565 batch_limit:   8677 Loss:   0.2853 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1566 batch_limit:   8677 Loss:   0.2852 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1567 batch_limit:   8677 Loss:   0.2851 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1568 batch_limit:   8677 Loss:   0.2849 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1569 batch_limit:   8677 Loss:   0.2848 Training Acc:    97.03  6759.53    99.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1570 batch_limit:   8677 Loss:   0.2847 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1571 batch_limit:   8677 Loss:   0.2846 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1572 batch_limit:   8677 Loss:   0.2845 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1573 batch_limit:   8677 Loss:   0.2844 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1574 batch_limit:   8677 Loss:   0.2843 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1575 batch_limit:   8677 Loss:   0.2842 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1576 batch_limit:   8677 Loss:   0.2841 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1577 batch_limit:   8677 Loss:   0.2840 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1578 batch_limit:   8677 Loss:   0.2839 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1579 batch_limit:   8677 Loss:   0.2837 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1580 batch_limit:   8677 Loss:   0.2836 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1581 batch_limit:   8677 Loss:   0.2835 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1582 batch_limit:   8677 Loss:   0.2834 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1583 batch_limit:   8677 Loss:   0.2833 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1584 batch_limit:   8677 Loss:   0.2832 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1585 batch_limit:   8677 Loss:   0.2831 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1586 batch_limit:   8677 Loss:   0.2830 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1587 batch_limit:   8677 Loss:   0.2829 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1588 batch_limit:   8677 Loss:   0.2828 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1589 batch_limit:   8677 Loss:   0.2827 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1590 batch_limit:   8677 Loss:   0.2826 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1591 batch_limit:   8677 Loss:   0.2825 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1592 batch_limit:   8677 Loss:   0.2824 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1593 batch_limit:   8677 Loss:   0.2823 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1594 batch_limit:   8677 Loss:   0.2822 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1595 batch_limit:   8677 Loss:   0.2821 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1596 batch_limit:   8677 Loss:   0.2819 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1597 batch_limit:   8677 Loss:   0.2818 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1598 batch_limit:   8677 Loss:   0.2817 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1599 batch_limit:   8677 Loss:   0.2816 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1600 batch_limit:   8677 Loss:   0.2815 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1601 batch_limit:   8677 Loss:   0.2814 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1602 batch_limit:   8677 Loss:   0.2813 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1603 batch_limit:   8677 Loss:   0.2812 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1604 batch_limit:   8677 Loss:   0.2811 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1605 batch_limit:   8677 Loss:   0.2810 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1606 batch_limit:   8677 Loss:   0.2809 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1607 batch_limit:   8677 Loss:   0.2808 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1608 batch_limit:   8677 Loss:   0.2807 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1609 batch_limit:   8677 Loss:   0.2806 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1610 batch_limit:   8677 Loss:   0.2805 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1611 batch_limit:   8677 Loss:   0.2804 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1612 batch_limit:   8677 Loss:   0.2803 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1613 batch_limit:   8677 Loss:   0.2802 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1614 batch_limit:   8677 Loss:   0.2801 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1615 batch_limit:   8677 Loss:   0.2800 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1616 batch_limit:   8677 Loss:   0.2799 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1617 batch_limit:   8677 Loss:   0.2798 Training Acc:    97.03  6759.53    99.40%\n",
      "Epoch   1618 batch_limit:   8677 Loss:   0.2797 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1619 batch_limit:   8677 Loss:   0.2796 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1620 batch_limit:   8677 Loss:   0.2795 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1621 batch_limit:   8677 Loss:   0.2794 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1622 batch_limit:   8677 Loss:   0.2793 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1623 batch_limit:   8677 Loss:   0.2792 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1624 batch_limit:   8677 Loss:   0.2791 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1625 batch_limit:   8677 Loss:   0.2790 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1626 batch_limit:   8677 Loss:   0.2789 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1627 batch_limit:   8677 Loss:   0.2788 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1628 batch_limit:   8677 Loss:   0.2787 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1629 batch_limit:   8677 Loss:   0.2786 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1630 batch_limit:   8677 Loss:   0.2785 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1631 batch_limit:   8677 Loss:   0.2784 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1632 batch_limit:   8677 Loss:   0.2783 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1633 batch_limit:   8677 Loss:   0.2782 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1634 batch_limit:   8677 Loss:   0.2781 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1635 batch_limit:   8677 Loss:   0.2780 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1636 batch_limit:   8677 Loss:   0.2779 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1637 batch_limit:   8677 Loss:   0.2778 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1638 batch_limit:   8677 Loss:   0.2777 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1639 batch_limit:   8677 Loss:   0.2776 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1640 batch_limit:   8677 Loss:   0.2775 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1641 batch_limit:   8677 Loss:   0.2774 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1642 batch_limit:   8677 Loss:   0.2773 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1643 batch_limit:   8677 Loss:   0.2772 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1644 batch_limit:   8677 Loss:   0.2771 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1645 batch_limit:   8677 Loss:   0.2770 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1646 batch_limit:   8677 Loss:   0.2769 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1647 batch_limit:   8677 Loss:   0.2768 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1648 batch_limit:   8677 Loss:   0.2767 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1649 batch_limit:   8677 Loss:   0.2766 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1650 batch_limit:   8677 Loss:   0.2765 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1651 batch_limit:   8677 Loss:   0.2764 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1652 batch_limit:   8677 Loss:   0.2763 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1653 batch_limit:   8677 Loss:   0.2762 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1654 batch_limit:   8677 Loss:   0.2761 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1655 batch_limit:   8677 Loss:   0.2760 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1656 batch_limit:   8677 Loss:   0.2759 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1657 batch_limit:   8677 Loss:   0.2758 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1658 batch_limit:   8677 Loss:   0.2757 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1659 batch_limit:   8677 Loss:   0.2756 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1660 batch_limit:   8677 Loss:   0.2755 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1661 batch_limit:   8677 Loss:   0.2754 Training Acc:    97.03  6760.31    99.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1662 batch_limit:   8677 Loss:   0.2753 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1663 batch_limit:   8677 Loss:   0.2752 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1664 batch_limit:   8677 Loss:   0.2751 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1665 batch_limit:   8677 Loss:   0.2750 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1666 batch_limit:   8677 Loss:   0.2749 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1667 batch_limit:   8677 Loss:   0.2748 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1668 batch_limit:   8677 Loss:   0.2747 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1669 batch_limit:   8677 Loss:   0.2746 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1670 batch_limit:   8677 Loss:   0.2745 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1671 batch_limit:   8677 Loss:   0.2744 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1672 batch_limit:   8677 Loss:   0.2743 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1673 batch_limit:   8677 Loss:   0.2742 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1674 batch_limit:   8677 Loss:   0.2742 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1675 batch_limit:   8677 Loss:   0.2741 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1676 batch_limit:   8677 Loss:   0.2740 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1677 batch_limit:   8677 Loss:   0.2739 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1678 batch_limit:   8677 Loss:   0.2738 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1679 batch_limit:   8677 Loss:   0.2737 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1680 batch_limit:   8677 Loss:   0.2736 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1681 batch_limit:   8677 Loss:   0.2735 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1682 batch_limit:   8677 Loss:   0.2734 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1683 batch_limit:   8677 Loss:   0.2733 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1684 batch_limit:   8677 Loss:   0.2732 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1685 batch_limit:   8677 Loss:   0.2731 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1686 batch_limit:   8677 Loss:   0.2730 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1687 batch_limit:   8677 Loss:   0.2729 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1688 batch_limit:   8677 Loss:   0.2728 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1689 batch_limit:   8677 Loss:   0.2727 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1690 batch_limit:   8677 Loss:   0.2726 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1691 batch_limit:   8677 Loss:   0.2725 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1692 batch_limit:   8677 Loss:   0.2724 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1693 batch_limit:   8677 Loss:   0.2723 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1694 batch_limit:   8677 Loss:   0.2722 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1695 batch_limit:   8677 Loss:   0.2722 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1696 batch_limit:   8677 Loss:   0.2721 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1697 batch_limit:   8677 Loss:   0.2720 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1698 batch_limit:   8677 Loss:   0.2719 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1699 batch_limit:   8677 Loss:   0.2718 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1700 batch_limit:   8677 Loss:   0.2717 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1701 batch_limit:   8677 Loss:   0.2716 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1702 batch_limit:   8677 Loss:   0.2715 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1703 batch_limit:   8677 Loss:   0.2714 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1704 batch_limit:   8677 Loss:   0.2713 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1705 batch_limit:   8677 Loss:   0.2712 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1706 batch_limit:   8677 Loss:   0.2711 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1707 batch_limit:   8677 Loss:   0.2710 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1708 batch_limit:   8677 Loss:   0.2709 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1709 batch_limit:   8677 Loss:   0.2708 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1710 batch_limit:   8677 Loss:   0.2707 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1711 batch_limit:   8677 Loss:   0.2707 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1712 batch_limit:   8677 Loss:   0.2706 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1713 batch_limit:   8677 Loss:   0.2705 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1714 batch_limit:   8677 Loss:   0.2704 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1715 batch_limit:   8677 Loss:   0.2703 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1716 batch_limit:   8677 Loss:   0.2702 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1717 batch_limit:   8677 Loss:   0.2701 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1718 batch_limit:   8677 Loss:   0.2700 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1719 batch_limit:   8677 Loss:   0.2699 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1720 batch_limit:   8677 Loss:   0.2698 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1721 batch_limit:   8677 Loss:   0.2697 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1722 batch_limit:   8677 Loss:   0.2696 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1723 batch_limit:   8677 Loss:   0.2695 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1724 batch_limit:   8677 Loss:   0.2694 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1725 batch_limit:   8677 Loss:   0.2693 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1726 batch_limit:   8677 Loss:   0.2693 Training Acc:    97.03  6760.31    99.42%\n",
      "Epoch   1727 batch_limit:   8677 Loss:   0.2692 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1728 batch_limit:   8677 Loss:   0.2691 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1729 batch_limit:   8677 Loss:   0.2690 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1730 batch_limit:   8677 Loss:   0.2689 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1731 batch_limit:   8677 Loss:   0.2688 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1732 batch_limit:   8677 Loss:   0.2687 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1733 batch_limit:   8677 Loss:   0.2686 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1734 batch_limit:   8677 Loss:   0.2685 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1735 batch_limit:   8677 Loss:   0.2684 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1736 batch_limit:   8677 Loss:   0.2683 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1737 batch_limit:   8677 Loss:   0.2682 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1738 batch_limit:   8677 Loss:   0.2681 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1739 batch_limit:   8677 Loss:   0.2681 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1740 batch_limit:   8677 Loss:   0.2680 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1741 batch_limit:   8677 Loss:   0.2679 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1742 batch_limit:   8677 Loss:   0.2678 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1743 batch_limit:   8677 Loss:   0.2677 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1744 batch_limit:   8677 Loss:   0.2676 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1745 batch_limit:   8677 Loss:   0.2675 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1746 batch_limit:   8677 Loss:   0.2674 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1747 batch_limit:   8677 Loss:   0.2673 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1748 batch_limit:   8677 Loss:   0.2672 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1749 batch_limit:   8677 Loss:   0.2671 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1750 batch_limit:   8677 Loss:   0.2670 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1751 batch_limit:   8677 Loss:   0.2669 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1752 batch_limit:   8677 Loss:   0.2669 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1753 batch_limit:   8677 Loss:   0.2668 Training Acc:    98.02  6761.30    99.43%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1754 batch_limit:   8677 Loss:   0.2667 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1755 batch_limit:   8677 Loss:   0.2666 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1756 batch_limit:   8677 Loss:   0.2665 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1757 batch_limit:   8677 Loss:   0.2664 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1758 batch_limit:   8677 Loss:   0.2663 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1759 batch_limit:   8677 Loss:   0.2662 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1760 batch_limit:   8677 Loss:   0.2661 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1761 batch_limit:   8677 Loss:   0.2660 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1762 batch_limit:   8677 Loss:   0.2659 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1763 batch_limit:   8677 Loss:   0.2658 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1764 batch_limit:   8677 Loss:   0.2658 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1765 batch_limit:   8677 Loss:   0.2657 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1766 batch_limit:   8677 Loss:   0.2656 Training Acc:    98.02  6761.30    99.43%\n",
      "Epoch   1767 batch_limit:   8677 Loss:   0.2655 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1768 batch_limit:   8677 Loss:   0.2654 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1769 batch_limit:   8677 Loss:   0.2653 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1770 batch_limit:   8677 Loss:   0.2652 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1771 batch_limit:   8677 Loss:   0.2651 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1772 batch_limit:   8677 Loss:   0.2650 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1773 batch_limit:   8677 Loss:   0.2649 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1774 batch_limit:   8677 Loss:   0.2648 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1775 batch_limit:   8677 Loss:   0.2648 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1776 batch_limit:   8677 Loss:   0.2647 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1777 batch_limit:   8677 Loss:   0.2646 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1778 batch_limit:   8677 Loss:   0.2645 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1779 batch_limit:   8677 Loss:   0.2644 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1780 batch_limit:   8677 Loss:   0.2643 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1781 batch_limit:   8677 Loss:   0.2642 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1782 batch_limit:   8677 Loss:   0.2641 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1783 batch_limit:   8677 Loss:   0.2640 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1784 batch_limit:   8677 Loss:   0.2639 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1785 batch_limit:   8677 Loss:   0.2639 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1786 batch_limit:   8677 Loss:   0.2638 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1787 batch_limit:   8677 Loss:   0.2637 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1788 batch_limit:   8677 Loss:   0.2636 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1789 batch_limit:   8677 Loss:   0.2635 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1790 batch_limit:   8677 Loss:   0.2634 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1791 batch_limit:   8677 Loss:   0.2633 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1792 batch_limit:   8677 Loss:   0.2632 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1793 batch_limit:   8677 Loss:   0.2631 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1794 batch_limit:   8677 Loss:   0.2630 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1795 batch_limit:   8677 Loss:   0.2629 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1796 batch_limit:   8677 Loss:   0.2629 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1797 batch_limit:   8677 Loss:   0.2628 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1798 batch_limit:   8677 Loss:   0.2627 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1799 batch_limit:   8677 Loss:   0.2626 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1800 batch_limit:   8677 Loss:   0.2625 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1801 batch_limit:   8677 Loss:   0.2624 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1802 batch_limit:   8677 Loss:   0.2623 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1803 batch_limit:   8677 Loss:   0.2622 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1804 batch_limit:   8677 Loss:   0.2621 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1805 batch_limit:   8677 Loss:   0.2620 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1806 batch_limit:   8677 Loss:   0.2620 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1807 batch_limit:   8677 Loss:   0.2619 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1808 batch_limit:   8677 Loss:   0.2618 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1809 batch_limit:   8677 Loss:   0.2617 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1810 batch_limit:   8677 Loss:   0.2616 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1811 batch_limit:   8677 Loss:   0.2615 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1812 batch_limit:   8677 Loss:   0.2614 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1813 batch_limit:   8677 Loss:   0.2613 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1814 batch_limit:   8677 Loss:   0.2612 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1815 batch_limit:   8677 Loss:   0.2612 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1816 batch_limit:   8677 Loss:   0.2611 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1817 batch_limit:   8677 Loss:   0.2610 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1818 batch_limit:   8677 Loss:   0.2609 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1819 batch_limit:   8677 Loss:   0.2608 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1820 batch_limit:   8677 Loss:   0.2607 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1821 batch_limit:   8677 Loss:   0.2606 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1822 batch_limit:   8677 Loss:   0.2605 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1823 batch_limit:   8677 Loss:   0.2604 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1824 batch_limit:   8677 Loss:   0.2603 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1825 batch_limit:   8677 Loss:   0.2603 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1826 batch_limit:   8677 Loss:   0.2602 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1827 batch_limit:   8677 Loss:   0.2601 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1828 batch_limit:   8677 Loss:   0.2600 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1829 batch_limit:   8677 Loss:   0.2599 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1830 batch_limit:   8677 Loss:   0.2598 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1831 batch_limit:   8677 Loss:   0.2597 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1832 batch_limit:   8677 Loss:   0.2596 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1833 batch_limit:   8677 Loss:   0.2595 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1834 batch_limit:   8677 Loss:   0.2595 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1835 batch_limit:   8677 Loss:   0.2594 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1836 batch_limit:   8677 Loss:   0.2593 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1837 batch_limit:   8677 Loss:   0.2592 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1838 batch_limit:   8677 Loss:   0.2591 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1839 batch_limit:   8677 Loss:   0.2590 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1840 batch_limit:   8677 Loss:   0.2589 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1841 batch_limit:   8677 Loss:   0.2588 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1842 batch_limit:   8677 Loss:   0.2587 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1843 batch_limit:   8677 Loss:   0.2587 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1844 batch_limit:   8677 Loss:   0.2586 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1845 batch_limit:   8677 Loss:   0.2585 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1846 batch_limit:   8677 Loss:   0.2584 Training Acc:    98.02  6762.08    99.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1847 batch_limit:   8677 Loss:   0.2583 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1848 batch_limit:   8677 Loss:   0.2582 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1849 batch_limit:   8677 Loss:   0.2581 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1850 batch_limit:   8677 Loss:   0.2580 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1851 batch_limit:   8677 Loss:   0.2579 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1852 batch_limit:   8677 Loss:   0.2579 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1853 batch_limit:   8677 Loss:   0.2578 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1854 batch_limit:   8677 Loss:   0.2577 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1855 batch_limit:   8677 Loss:   0.2576 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1856 batch_limit:   8677 Loss:   0.2575 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1857 batch_limit:   8677 Loss:   0.2574 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1858 batch_limit:   8677 Loss:   0.2573 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1859 batch_limit:   8677 Loss:   0.2572 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1860 batch_limit:   8677 Loss:   0.2572 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1861 batch_limit:   8677 Loss:   0.2571 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1862 batch_limit:   8677 Loss:   0.2570 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1863 batch_limit:   8677 Loss:   0.2569 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1864 batch_limit:   8677 Loss:   0.2568 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1865 batch_limit:   8677 Loss:   0.2567 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1866 batch_limit:   8677 Loss:   0.2566 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1867 batch_limit:   8677 Loss:   0.2565 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1868 batch_limit:   8677 Loss:   0.2564 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1869 batch_limit:   8677 Loss:   0.2564 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1870 batch_limit:   8677 Loss:   0.2563 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1871 batch_limit:   8677 Loss:   0.2562 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1872 batch_limit:   8677 Loss:   0.2561 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1873 batch_limit:   8677 Loss:   0.2560 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1874 batch_limit:   8677 Loss:   0.2559 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1875 batch_limit:   8677 Loss:   0.2558 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1876 batch_limit:   8677 Loss:   0.2557 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1877 batch_limit:   8677 Loss:   0.2557 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1878 batch_limit:   8677 Loss:   0.2556 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1879 batch_limit:   8677 Loss:   0.2555 Training Acc:    98.02  6762.08    99.44%\n",
      "Epoch   1880 batch_limit:   8677 Loss:   0.2554 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1881 batch_limit:   8677 Loss:   0.2553 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1882 batch_limit:   8677 Loss:   0.2552 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1883 batch_limit:   8677 Loss:   0.2551 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1884 batch_limit:   8677 Loss:   0.2550 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1885 batch_limit:   8677 Loss:   0.2549 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1886 batch_limit:   8677 Loss:   0.2549 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1887 batch_limit:   8677 Loss:   0.2548 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1888 batch_limit:   8677 Loss:   0.2547 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1889 batch_limit:   8677 Loss:   0.2546 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1890 batch_limit:   8677 Loss:   0.2545 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1891 batch_limit:   8677 Loss:   0.2544 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1892 batch_limit:   8677 Loss:   0.2543 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1893 batch_limit:   8677 Loss:   0.2543 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1894 batch_limit:   8677 Loss:   0.2542 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1895 batch_limit:   8677 Loss:   0.2541 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1896 batch_limit:   8677 Loss:   0.2540 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1897 batch_limit:   8677 Loss:   0.2539 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1898 batch_limit:   8677 Loss:   0.2538 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1899 batch_limit:   8677 Loss:   0.2537 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1900 batch_limit:   8677 Loss:   0.2536 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1901 batch_limit:   8677 Loss:   0.2536 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1902 batch_limit:   8677 Loss:   0.2535 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1903 batch_limit:   8677 Loss:   0.2534 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1904 batch_limit:   8677 Loss:   0.2533 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1905 batch_limit:   8677 Loss:   0.2532 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1906 batch_limit:   8677 Loss:   0.2531 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1907 batch_limit:   8677 Loss:   0.2530 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1908 batch_limit:   8677 Loss:   0.2529 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1909 batch_limit:   8677 Loss:   0.2529 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1910 batch_limit:   8677 Loss:   0.2528 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1911 batch_limit:   8677 Loss:   0.2527 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1912 batch_limit:   8677 Loss:   0.2526 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1913 batch_limit:   8677 Loss:   0.2525 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1914 batch_limit:   8677 Loss:   0.2524 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1915 batch_limit:   8677 Loss:   0.2523 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1916 batch_limit:   8677 Loss:   0.2522 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1917 batch_limit:   8677 Loss:   0.2522 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1918 batch_limit:   8677 Loss:   0.2521 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1919 batch_limit:   8677 Loss:   0.2520 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1920 batch_limit:   8677 Loss:   0.2519 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1921 batch_limit:   8677 Loss:   0.2518 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1922 batch_limit:   8677 Loss:   0.2517 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1923 batch_limit:   8677 Loss:   0.2516 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1924 batch_limit:   8677 Loss:   0.2516 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1925 batch_limit:   8677 Loss:   0.2515 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1926 batch_limit:   8677 Loss:   0.2514 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1927 batch_limit:   8677 Loss:   0.2513 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1928 batch_limit:   8677 Loss:   0.2512 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1929 batch_limit:   8677 Loss:   0.2511 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1930 batch_limit:   8677 Loss:   0.2510 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1931 batch_limit:   8677 Loss:   0.2509 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1932 batch_limit:   8677 Loss:   0.2509 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1933 batch_limit:   8677 Loss:   0.2508 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1934 batch_limit:   8677 Loss:   0.2507 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1935 batch_limit:   8677 Loss:   0.2506 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1936 batch_limit:   8677 Loss:   0.2505 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1937 batch_limit:   8677 Loss:   0.2504 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1938 batch_limit:   8677 Loss:   0.2503 Training Acc:    98.02  6762.86    99.45%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1939 batch_limit:   8677 Loss:   0.2503 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1940 batch_limit:   8677 Loss:   0.2502 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1941 batch_limit:   8677 Loss:   0.2501 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1942 batch_limit:   8677 Loss:   0.2500 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1943 batch_limit:   8677 Loss:   0.2499 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1944 batch_limit:   8677 Loss:   0.2498 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1945 batch_limit:   8677 Loss:   0.2497 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1946 batch_limit:   8677 Loss:   0.2497 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1947 batch_limit:   8677 Loss:   0.2496 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1948 batch_limit:   8677 Loss:   0.2495 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1949 batch_limit:   8677 Loss:   0.2494 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1950 batch_limit:   8677 Loss:   0.2493 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1951 batch_limit:   8677 Loss:   0.2492 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1952 batch_limit:   8677 Loss:   0.2491 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1953 batch_limit:   8677 Loss:   0.2491 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1954 batch_limit:   8677 Loss:   0.2490 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1955 batch_limit:   8677 Loss:   0.2489 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1956 batch_limit:   8677 Loss:   0.2488 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1957 batch_limit:   8677 Loss:   0.2487 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1958 batch_limit:   8677 Loss:   0.2486 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1959 batch_limit:   8677 Loss:   0.2485 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1960 batch_limit:   8677 Loss:   0.2485 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1961 batch_limit:   8677 Loss:   0.2484 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1962 batch_limit:   8677 Loss:   0.2483 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1963 batch_limit:   8677 Loss:   0.2482 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1964 batch_limit:   8677 Loss:   0.2481 Training Acc:    98.02  6762.86    99.45%\n",
      "Epoch   1965 batch_limit:   8677 Loss:   0.2480 Training Acc:    98.02  6763.64    99.47%\n",
      "Epoch   1966 batch_limit:   8677 Loss:   0.2479 Training Acc:    98.02  6763.64    99.47%\n",
      "Epoch   1967 batch_limit:   8677 Loss:   0.2479 Training Acc:    98.02  6763.64    99.47%\n",
      "Epoch   1968 batch_limit:   8677 Loss:   0.2478 Training Acc:    98.02  6763.64    99.47%\n",
      "Epoch   1969 batch_limit:   8677 Loss:   0.2477 Training Acc:    98.02  6763.64    99.47%\n",
      "Epoch   1970 batch_limit:   8677 Loss:   0.2476 Training Acc:    98.02  6763.64    99.47%\n",
      "Epoch   1971 batch_limit:   8677 Loss:   0.2475 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1972 batch_limit:   8677 Loss:   0.2474 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1973 batch_limit:   8677 Loss:   0.2473 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1974 batch_limit:   8677 Loss:   0.2473 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1975 batch_limit:   8677 Loss:   0.2472 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1976 batch_limit:   8677 Loss:   0.2471 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1977 batch_limit:   8677 Loss:   0.2470 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1978 batch_limit:   8677 Loss:   0.2469 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1979 batch_limit:   8677 Loss:   0.2468 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1980 batch_limit:   8677 Loss:   0.2467 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1981 batch_limit:   8677 Loss:   0.2467 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1982 batch_limit:   8677 Loss:   0.2466 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1983 batch_limit:   8677 Loss:   0.2465 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1984 batch_limit:   8677 Loss:   0.2464 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1985 batch_limit:   8677 Loss:   0.2463 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1986 batch_limit:   8677 Loss:   0.2462 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1987 batch_limit:   8677 Loss:   0.2461 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1988 batch_limit:   8677 Loss:   0.2461 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1989 batch_limit:   8677 Loss:   0.2460 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1990 batch_limit:   8677 Loss:   0.2459 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1991 batch_limit:   8677 Loss:   0.2458 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1992 batch_limit:   8677 Loss:   0.2457 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1993 batch_limit:   8677 Loss:   0.2456 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1994 batch_limit:   8677 Loss:   0.2456 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1995 batch_limit:   8677 Loss:   0.2455 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1996 batch_limit:   8677 Loss:   0.2454 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1997 batch_limit:   8677 Loss:   0.2453 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1998 batch_limit:   8677 Loss:   0.2452 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   1999 batch_limit:   8677 Loss:   0.2451 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2000 batch_limit:   8677 Loss:   0.2450 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2001 batch_limit:   8677 Loss:   0.2450 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2002 batch_limit:   8677 Loss:   0.2449 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2003 batch_limit:   8677 Loss:   0.2448 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2004 batch_limit:   8677 Loss:   0.2447 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2005 batch_limit:   8677 Loss:   0.2446 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2006 batch_limit:   8677 Loss:   0.2445 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2007 batch_limit:   8677 Loss:   0.2444 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2008 batch_limit:   8677 Loss:   0.2444 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2009 batch_limit:   8677 Loss:   0.2443 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2010 batch_limit:   8677 Loss:   0.2442 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2011 batch_limit:   8677 Loss:   0.2441 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2012 batch_limit:   8677 Loss:   0.2440 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2013 batch_limit:   8677 Loss:   0.2439 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2014 batch_limit:   8677 Loss:   0.2439 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2015 batch_limit:   8677 Loss:   0.2438 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2016 batch_limit:   8677 Loss:   0.2437 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2017 batch_limit:   8677 Loss:   0.2436 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2018 batch_limit:   8677 Loss:   0.2435 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2019 batch_limit:   8677 Loss:   0.2434 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2020 batch_limit:   8677 Loss:   0.2433 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2021 batch_limit:   8677 Loss:   0.2433 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2022 batch_limit:   8677 Loss:   0.2432 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2023 batch_limit:   8677 Loss:   0.2431 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2024 batch_limit:   8677 Loss:   0.2430 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2025 batch_limit:   8677 Loss:   0.2429 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2026 batch_limit:   8677 Loss:   0.2428 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2027 batch_limit:   8677 Loss:   0.2428 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2028 batch_limit:   8677 Loss:   0.2427 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2029 batch_limit:   8677 Loss:   0.2426 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2030 batch_limit:   8677 Loss:   0.2425 Training Acc:    98.02  6764.43    99.48%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2031 batch_limit:   8677 Loss:   0.2424 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2032 batch_limit:   8677 Loss:   0.2423 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2033 batch_limit:   8677 Loss:   0.2422 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2034 batch_limit:   8677 Loss:   0.2422 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2035 batch_limit:   8677 Loss:   0.2421 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2036 batch_limit:   8677 Loss:   0.2420 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2037 batch_limit:   8677 Loss:   0.2419 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2038 batch_limit:   8677 Loss:   0.2418 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2039 batch_limit:   8677 Loss:   0.2417 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2040 batch_limit:   8677 Loss:   0.2417 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2041 batch_limit:   8677 Loss:   0.2416 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2042 batch_limit:   8677 Loss:   0.2415 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2043 batch_limit:   8677 Loss:   0.2414 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2044 batch_limit:   8677 Loss:   0.2413 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2045 batch_limit:   8677 Loss:   0.2412 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2046 batch_limit:   8677 Loss:   0.2411 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2047 batch_limit:   8677 Loss:   0.2411 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2048 batch_limit:   8677 Loss:   0.2410 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2049 batch_limit:   8677 Loss:   0.2409 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2050 batch_limit:   8677 Loss:   0.2408 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2051 batch_limit:   8677 Loss:   0.2407 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2052 batch_limit:   8677 Loss:   0.2406 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2053 batch_limit:   8677 Loss:   0.2406 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2054 batch_limit:   8677 Loss:   0.2405 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2055 batch_limit:   8677 Loss:   0.2404 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2056 batch_limit:   8677 Loss:   0.2403 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2057 batch_limit:   8677 Loss:   0.2402 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2058 batch_limit:   8677 Loss:   0.2401 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2059 batch_limit:   8677 Loss:   0.2400 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2060 batch_limit:   8677 Loss:   0.2400 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2061 batch_limit:   8677 Loss:   0.2399 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2062 batch_limit:   8677 Loss:   0.2398 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2063 batch_limit:   8677 Loss:   0.2397 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2064 batch_limit:   8677 Loss:   0.2396 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2065 batch_limit:   8677 Loss:   0.2395 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2066 batch_limit:   8677 Loss:   0.2395 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2067 batch_limit:   8677 Loss:   0.2394 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2068 batch_limit:   8677 Loss:   0.2393 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2069 batch_limit:   8677 Loss:   0.2392 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2070 batch_limit:   8677 Loss:   0.2391 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2071 batch_limit:   8677 Loss:   0.2390 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2072 batch_limit:   8677 Loss:   0.2390 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2073 batch_limit:   8677 Loss:   0.2389 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2074 batch_limit:   8677 Loss:   0.2388 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2075 batch_limit:   8677 Loss:   0.2387 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2076 batch_limit:   8677 Loss:   0.2386 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2077 batch_limit:   8677 Loss:   0.2385 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2078 batch_limit:   8677 Loss:   0.2384 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2079 batch_limit:   8677 Loss:   0.2384 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2080 batch_limit:   8677 Loss:   0.2383 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2081 batch_limit:   8677 Loss:   0.2382 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2082 batch_limit:   8677 Loss:   0.2381 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2083 batch_limit:   8677 Loss:   0.2380 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2084 batch_limit:   8677 Loss:   0.2379 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2085 batch_limit:   8677 Loss:   0.2379 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2086 batch_limit:   8677 Loss:   0.2378 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2087 batch_limit:   8677 Loss:   0.2377 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2088 batch_limit:   8677 Loss:   0.2376 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2089 batch_limit:   8677 Loss:   0.2375 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2090 batch_limit:   8677 Loss:   0.2374 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2091 batch_limit:   8677 Loss:   0.2373 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2092 batch_limit:   8677 Loss:   0.2373 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2093 batch_limit:   8677 Loss:   0.2372 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2094 batch_limit:   8677 Loss:   0.2371 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2095 batch_limit:   8677 Loss:   0.2370 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2096 batch_limit:   8677 Loss:   0.2369 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2097 batch_limit:   8677 Loss:   0.2368 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2098 batch_limit:   8677 Loss:   0.2368 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2099 batch_limit:   8677 Loss:   0.2367 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2100 batch_limit:   8677 Loss:   0.2366 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2101 batch_limit:   8677 Loss:   0.2365 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2102 batch_limit:   8677 Loss:   0.2364 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2103 batch_limit:   8677 Loss:   0.2363 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2104 batch_limit:   8677 Loss:   0.2362 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2105 batch_limit:   8677 Loss:   0.2362 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2106 batch_limit:   8677 Loss:   0.2361 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2107 batch_limit:   8677 Loss:   0.2360 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2108 batch_limit:   8677 Loss:   0.2359 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2109 batch_limit:   8677 Loss:   0.2358 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2110 batch_limit:   8677 Loss:   0.2357 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2111 batch_limit:   8677 Loss:   0.2356 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2112 batch_limit:   8677 Loss:   0.2356 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2113 batch_limit:   8677 Loss:   0.2355 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2114 batch_limit:   8677 Loss:   0.2354 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2115 batch_limit:   8677 Loss:   0.2353 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2116 batch_limit:   8677 Loss:   0.2352 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2117 batch_limit:   8677 Loss:   0.2351 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2118 batch_limit:   8677 Loss:   0.2351 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2119 batch_limit:   8677 Loss:   0.2350 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2120 batch_limit:   8677 Loss:   0.2349 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2121 batch_limit:   8677 Loss:   0.2348 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2122 batch_limit:   8677 Loss:   0.2347 Training Acc:    98.02  6764.43    99.48%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2123 batch_limit:   8677 Loss:   0.2346 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2124 batch_limit:   8677 Loss:   0.2345 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2125 batch_limit:   8677 Loss:   0.2345 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2126 batch_limit:   8677 Loss:   0.2344 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2127 batch_limit:   8677 Loss:   0.2343 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2128 batch_limit:   8677 Loss:   0.2342 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2129 batch_limit:   8677 Loss:   0.2341 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2130 batch_limit:   8677 Loss:   0.2340 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2131 batch_limit:   8677 Loss:   0.2339 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2132 batch_limit:   8677 Loss:   0.2339 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2133 batch_limit:   8677 Loss:   0.2338 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2134 batch_limit:   8677 Loss:   0.2337 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2135 batch_limit:   8677 Loss:   0.2336 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2136 batch_limit:   8677 Loss:   0.2335 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2137 batch_limit:   8677 Loss:   0.2334 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2138 batch_limit:   8677 Loss:   0.2333 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2139 batch_limit:   8677 Loss:   0.2333 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2140 batch_limit:   8677 Loss:   0.2332 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2141 batch_limit:   8677 Loss:   0.2331 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2142 batch_limit:   8677 Loss:   0.2330 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2143 batch_limit:   8677 Loss:   0.2329 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2144 batch_limit:   8677 Loss:   0.2328 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2145 batch_limit:   8677 Loss:   0.2327 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2146 batch_limit:   8677 Loss:   0.2327 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2147 batch_limit:   8677 Loss:   0.2326 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2148 batch_limit:   8677 Loss:   0.2325 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2149 batch_limit:   8677 Loss:   0.2324 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2150 batch_limit:   8677 Loss:   0.2323 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2151 batch_limit:   8677 Loss:   0.2322 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2152 batch_limit:   8677 Loss:   0.2321 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2153 batch_limit:   8677 Loss:   0.2321 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2154 batch_limit:   8677 Loss:   0.2320 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2155 batch_limit:   8677 Loss:   0.2319 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2156 batch_limit:   8677 Loss:   0.2318 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2157 batch_limit:   8677 Loss:   0.2317 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2158 batch_limit:   8677 Loss:   0.2316 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2159 batch_limit:   8677 Loss:   0.2315 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2160 batch_limit:   8677 Loss:   0.2314 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2161 batch_limit:   8677 Loss:   0.2314 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2162 batch_limit:   8677 Loss:   0.2313 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2163 batch_limit:   8677 Loss:   0.2312 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2164 batch_limit:   8677 Loss:   0.2311 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2165 batch_limit:   8677 Loss:   0.2310 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2166 batch_limit:   8677 Loss:   0.2309 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2167 batch_limit:   8677 Loss:   0.2308 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2168 batch_limit:   8677 Loss:   0.2307 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2169 batch_limit:   8677 Loss:   0.2307 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2170 batch_limit:   8677 Loss:   0.2306 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2171 batch_limit:   8677 Loss:   0.2305 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2172 batch_limit:   8677 Loss:   0.2304 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2173 batch_limit:   8677 Loss:   0.2303 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2174 batch_limit:   8677 Loss:   0.2302 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2175 batch_limit:   8677 Loss:   0.2301 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2176 batch_limit:   8677 Loss:   0.2301 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2177 batch_limit:   8677 Loss:   0.2300 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2178 batch_limit:   8677 Loss:   0.2299 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2179 batch_limit:   8677 Loss:   0.2298 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2180 batch_limit:   8677 Loss:   0.2297 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2181 batch_limit:   8677 Loss:   0.2296 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2182 batch_limit:   8677 Loss:   0.2295 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2183 batch_limit:   8677 Loss:   0.2294 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2184 batch_limit:   8677 Loss:   0.2293 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2185 batch_limit:   8677 Loss:   0.2293 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2186 batch_limit:   8677 Loss:   0.2292 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2187 batch_limit:   8677 Loss:   0.2291 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2188 batch_limit:   8677 Loss:   0.2290 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2189 batch_limit:   8677 Loss:   0.2289 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2190 batch_limit:   8677 Loss:   0.2288 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2191 batch_limit:   8677 Loss:   0.2287 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2192 batch_limit:   8677 Loss:   0.2286 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2193 batch_limit:   8677 Loss:   0.2286 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2194 batch_limit:   8677 Loss:   0.2285 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2195 batch_limit:   8677 Loss:   0.2284 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2196 batch_limit:   8677 Loss:   0.2283 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2197 batch_limit:   8677 Loss:   0.2282 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2198 batch_limit:   8677 Loss:   0.2281 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2199 batch_limit:   8677 Loss:   0.2280 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2200 batch_limit:   8677 Loss:   0.2279 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2201 batch_limit:   8677 Loss:   0.2278 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2202 batch_limit:   8677 Loss:   0.2278 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2203 batch_limit:   8677 Loss:   0.2277 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2204 batch_limit:   8677 Loss:   0.2276 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2205 batch_limit:   8677 Loss:   0.2275 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2206 batch_limit:   8677 Loss:   0.2274 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2207 batch_limit:   8677 Loss:   0.2273 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2208 batch_limit:   8677 Loss:   0.2272 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2209 batch_limit:   8677 Loss:   0.2271 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2210 batch_limit:   8677 Loss:   0.2270 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2211 batch_limit:   8677 Loss:   0.2269 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2212 batch_limit:   8677 Loss:   0.2269 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2213 batch_limit:   8677 Loss:   0.2268 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2214 batch_limit:   8677 Loss:   0.2267 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2215 batch_limit:   8677 Loss:   0.2266 Training Acc:    98.02  6764.43    99.48%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2216 batch_limit:   8677 Loss:   0.2265 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2217 batch_limit:   8677 Loss:   0.2264 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2218 batch_limit:   8677 Loss:   0.2263 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2219 batch_limit:   8677 Loss:   0.2262 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2220 batch_limit:   8677 Loss:   0.2261 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2221 batch_limit:   8677 Loss:   0.2260 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2222 batch_limit:   8677 Loss:   0.2260 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2223 batch_limit:   8677 Loss:   0.2259 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2224 batch_limit:   8677 Loss:   0.2258 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2225 batch_limit:   8677 Loss:   0.2257 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2226 batch_limit:   8677 Loss:   0.2256 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2227 batch_limit:   8677 Loss:   0.2255 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2228 batch_limit:   8677 Loss:   0.2254 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2229 batch_limit:   8677 Loss:   0.2253 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2230 batch_limit:   8677 Loss:   0.2252 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2231 batch_limit:   8677 Loss:   0.2251 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2232 batch_limit:   8677 Loss:   0.2250 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2233 batch_limit:   8677 Loss:   0.2249 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2234 batch_limit:   8677 Loss:   0.2249 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2235 batch_limit:   8677 Loss:   0.2248 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2236 batch_limit:   8677 Loss:   0.2247 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2237 batch_limit:   8677 Loss:   0.2246 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2238 batch_limit:   8677 Loss:   0.2245 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2239 batch_limit:   8677 Loss:   0.2244 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2240 batch_limit:   8677 Loss:   0.2243 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2241 batch_limit:   8677 Loss:   0.2242 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2242 batch_limit:   8677 Loss:   0.2241 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2243 batch_limit:   8677 Loss:   0.2240 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2244 batch_limit:   8677 Loss:   0.2239 Training Acc:    98.02  6764.43    99.48%\n",
      "Epoch   2245 batch_limit:   8677 Loss:   0.2238 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2246 batch_limit:   8677 Loss:   0.2237 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2247 batch_limit:   8677 Loss:   0.2237 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2248 batch_limit:   8677 Loss:   0.2236 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2249 batch_limit:   8677 Loss:   0.2235 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2250 batch_limit:   8677 Loss:   0.2234 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2251 batch_limit:   8677 Loss:   0.2233 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2252 batch_limit:   8677 Loss:   0.2232 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2253 batch_limit:   8677 Loss:   0.2231 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2254 batch_limit:   8677 Loss:   0.2230 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2255 batch_limit:   8677 Loss:   0.2229 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2256 batch_limit:   8677 Loss:   0.2228 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2257 batch_limit:   8677 Loss:   0.2227 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2258 batch_limit:   8677 Loss:   0.2226 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2259 batch_limit:   8677 Loss:   0.2225 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2260 batch_limit:   8677 Loss:   0.2224 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2261 batch_limit:   8677 Loss:   0.2223 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2262 batch_limit:   8677 Loss:   0.2222 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2263 batch_limit:   8677 Loss:   0.2222 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2264 batch_limit:   8677 Loss:   0.2221 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2265 batch_limit:   8677 Loss:   0.2220 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2266 batch_limit:   8677 Loss:   0.2219 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2267 batch_limit:   8677 Loss:   0.2218 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2268 batch_limit:   8677 Loss:   0.2217 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2269 batch_limit:   8677 Loss:   0.2216 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2270 batch_limit:   8677 Loss:   0.2215 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2271 batch_limit:   8677 Loss:   0.2214 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2272 batch_limit:   8677 Loss:   0.2213 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2273 batch_limit:   8677 Loss:   0.2212 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2274 batch_limit:   8677 Loss:   0.2211 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2275 batch_limit:   8677 Loss:   0.2210 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2276 batch_limit:   8677 Loss:   0.2209 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2277 batch_limit:   8677 Loss:   0.2208 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2278 batch_limit:   8677 Loss:   0.2207 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2279 batch_limit:   8677 Loss:   0.2206 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2280 batch_limit:   8677 Loss:   0.2205 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2281 batch_limit:   8677 Loss:   0.2204 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2282 batch_limit:   8677 Loss:   0.2203 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2283 batch_limit:   8677 Loss:   0.2202 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2284 batch_limit:   8677 Loss:   0.2201 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2285 batch_limit:   8677 Loss:   0.2200 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2286 batch_limit:   8677 Loss:   0.2199 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2287 batch_limit:   8677 Loss:   0.2198 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2288 batch_limit:   8677 Loss:   0.2197 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2289 batch_limit:   8677 Loss:   0.2196 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2290 batch_limit:   8677 Loss:   0.2195 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2291 batch_limit:   8677 Loss:   0.2195 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2292 batch_limit:   8677 Loss:   0.2194 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2293 batch_limit:   8677 Loss:   0.2193 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2294 batch_limit:   8677 Loss:   0.2192 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2295 batch_limit:   8677 Loss:   0.2191 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2296 batch_limit:   8677 Loss:   0.2190 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2297 batch_limit:   8677 Loss:   0.2189 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2298 batch_limit:   8677 Loss:   0.2188 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2299 batch_limit:   8677 Loss:   0.2187 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2300 batch_limit:   8677 Loss:   0.2186 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2301 batch_limit:   8677 Loss:   0.2185 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2302 batch_limit:   8677 Loss:   0.2184 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2303 batch_limit:   8677 Loss:   0.2183 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2304 batch_limit:   8677 Loss:   0.2182 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2305 batch_limit:   8677 Loss:   0.2181 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2306 batch_limit:   8677 Loss:   0.2180 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2307 batch_limit:   8677 Loss:   0.2179 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2308 batch_limit:   8677 Loss:   0.2178 Training Acc:    99.01  6765.42    99.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2309 batch_limit:   8677 Loss:   0.2177 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2310 batch_limit:   8677 Loss:   0.2176 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2311 batch_limit:   8677 Loss:   0.2175 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2312 batch_limit:   8677 Loss:   0.2174 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2313 batch_limit:   8677 Loss:   0.2173 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2314 batch_limit:   8677 Loss:   0.2172 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2315 batch_limit:   8677 Loss:   0.2171 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2316 batch_limit:   8677 Loss:   0.2170 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2317 batch_limit:   8677 Loss:   0.2169 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2318 batch_limit:   8677 Loss:   0.2168 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2319 batch_limit:   8677 Loss:   0.2166 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2320 batch_limit:   8677 Loss:   0.2165 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2321 batch_limit:   8677 Loss:   0.2164 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2322 batch_limit:   8677 Loss:   0.2163 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2323 batch_limit:   8677 Loss:   0.2162 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2324 batch_limit:   8677 Loss:   0.2161 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2325 batch_limit:   8677 Loss:   0.2160 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2326 batch_limit:   8677 Loss:   0.2159 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2327 batch_limit:   8677 Loss:   0.2158 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2328 batch_limit:   8677 Loss:   0.2157 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2329 batch_limit:   8677 Loss:   0.2156 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2330 batch_limit:   8677 Loss:   0.2155 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2331 batch_limit:   8677 Loss:   0.2154 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2332 batch_limit:   8677 Loss:   0.2153 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2333 batch_limit:   8677 Loss:   0.2152 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2334 batch_limit:   8677 Loss:   0.2151 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2335 batch_limit:   8677 Loss:   0.2150 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2336 batch_limit:   8677 Loss:   0.2149 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2337 batch_limit:   8677 Loss:   0.2148 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2338 batch_limit:   8677 Loss:   0.2147 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2339 batch_limit:   8677 Loss:   0.2146 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2340 batch_limit:   8677 Loss:   0.2145 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2341 batch_limit:   8677 Loss:   0.2144 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2342 batch_limit:   8677 Loss:   0.2143 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2343 batch_limit:   8677 Loss:   0.2142 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2344 batch_limit:   8677 Loss:   0.2140 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2345 batch_limit:   8677 Loss:   0.2139 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2346 batch_limit:   8677 Loss:   0.2138 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2347 batch_limit:   8677 Loss:   0.2137 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2348 batch_limit:   8677 Loss:   0.2136 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2349 batch_limit:   8677 Loss:   0.2135 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2350 batch_limit:   8677 Loss:   0.2134 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2351 batch_limit:   8677 Loss:   0.2133 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2352 batch_limit:   8677 Loss:   0.2132 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2353 batch_limit:   8677 Loss:   0.2131 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2354 batch_limit:   8677 Loss:   0.2130 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2355 batch_limit:   8677 Loss:   0.2129 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2356 batch_limit:   8677 Loss:   0.2128 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2357 batch_limit:   8677 Loss:   0.2127 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2358 batch_limit:   8677 Loss:   0.2125 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2359 batch_limit:   8677 Loss:   0.2124 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2360 batch_limit:   8677 Loss:   0.2123 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2361 batch_limit:   8677 Loss:   0.2122 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2362 batch_limit:   8677 Loss:   0.2121 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2363 batch_limit:   8677 Loss:   0.2120 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2364 batch_limit:   8677 Loss:   0.2119 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2365 batch_limit:   8677 Loss:   0.2118 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2366 batch_limit:   8677 Loss:   0.2117 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2367 batch_limit:   8677 Loss:   0.2116 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2368 batch_limit:   8677 Loss:   0.2115 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2369 batch_limit:   8677 Loss:   0.2113 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2370 batch_limit:   8677 Loss:   0.2112 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2371 batch_limit:   8677 Loss:   0.2111 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2372 batch_limit:   8677 Loss:   0.2110 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2373 batch_limit:   8677 Loss:   0.2109 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2374 batch_limit:   8677 Loss:   0.2108 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2375 batch_limit:   8677 Loss:   0.2107 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2376 batch_limit:   8677 Loss:   0.2106 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2377 batch_limit:   8677 Loss:   0.2105 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2378 batch_limit:   8677 Loss:   0.2103 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2379 batch_limit:   8677 Loss:   0.2102 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2380 batch_limit:   8677 Loss:   0.2101 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2381 batch_limit:   8677 Loss:   0.2100 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2382 batch_limit:   8677 Loss:   0.2099 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2383 batch_limit:   8677 Loss:   0.2098 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2384 batch_limit:   8677 Loss:   0.2097 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2385 batch_limit:   8677 Loss:   0.2096 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2386 batch_limit:   8677 Loss:   0.2094 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2387 batch_limit:   8677 Loss:   0.2093 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2388 batch_limit:   8677 Loss:   0.2092 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2389 batch_limit:   8677 Loss:   0.2091 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2390 batch_limit:   8677 Loss:   0.2090 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2391 batch_limit:   8677 Loss:   0.2089 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2392 batch_limit:   8677 Loss:   0.2088 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2393 batch_limit:   8677 Loss:   0.2086 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2394 batch_limit:   8677 Loss:   0.2085 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2395 batch_limit:   8677 Loss:   0.2084 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2396 batch_limit:   8677 Loss:   0.2083 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2397 batch_limit:   8677 Loss:   0.2082 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2398 batch_limit:   8677 Loss:   0.2081 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2399 batch_limit:   8677 Loss:   0.2080 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2400 batch_limit:   8677 Loss:   0.2078 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2401 batch_limit:   8677 Loss:   0.2077 Training Acc:    99.01  6765.42    99.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2402 batch_limit:   8677 Loss:   0.2076 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2403 batch_limit:   8677 Loss:   0.2075 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2404 batch_limit:   8677 Loss:   0.2074 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2405 batch_limit:   8677 Loss:   0.2073 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2406 batch_limit:   8677 Loss:   0.2071 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2407 batch_limit:   8677 Loss:   0.2070 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2408 batch_limit:   8677 Loss:   0.2069 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2409 batch_limit:   8677 Loss:   0.2068 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2410 batch_limit:   8677 Loss:   0.2067 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2411 batch_limit:   8677 Loss:   0.2066 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2412 batch_limit:   8677 Loss:   0.2064 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2413 batch_limit:   8677 Loss:   0.2063 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2414 batch_limit:   8677 Loss:   0.2062 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2415 batch_limit:   8677 Loss:   0.2061 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2416 batch_limit:   8677 Loss:   0.2060 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2417 batch_limit:   8677 Loss:   0.2058 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2418 batch_limit:   8677 Loss:   0.2057 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2419 batch_limit:   8677 Loss:   0.2056 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2420 batch_limit:   8677 Loss:   0.2055 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2421 batch_limit:   8677 Loss:   0.2054 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2422 batch_limit:   8677 Loss:   0.2052 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2423 batch_limit:   8677 Loss:   0.2051 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2424 batch_limit:   8677 Loss:   0.2050 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2425 batch_limit:   8677 Loss:   0.2049 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2426 batch_limit:   8677 Loss:   0.2048 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2427 batch_limit:   8677 Loss:   0.2046 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2428 batch_limit:   8677 Loss:   0.2045 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2429 batch_limit:   8677 Loss:   0.2044 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2430 batch_limit:   8677 Loss:   0.2043 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2431 batch_limit:   8677 Loss:   0.2041 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2432 batch_limit:   8677 Loss:   0.2040 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2433 batch_limit:   8677 Loss:   0.2039 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2434 batch_limit:   8677 Loss:   0.2038 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2435 batch_limit:   8677 Loss:   0.2037 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2436 batch_limit:   8677 Loss:   0.2035 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2437 batch_limit:   8677 Loss:   0.2034 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2438 batch_limit:   8677 Loss:   0.2033 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2439 batch_limit:   8677 Loss:   0.2032 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2440 batch_limit:   8677 Loss:   0.2030 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2441 batch_limit:   8677 Loss:   0.2029 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2442 batch_limit:   8677 Loss:   0.2028 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2443 batch_limit:   8677 Loss:   0.2027 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2444 batch_limit:   8677 Loss:   0.2025 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2445 batch_limit:   8677 Loss:   0.2024 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2446 batch_limit:   8677 Loss:   0.2023 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2447 batch_limit:   8677 Loss:   0.2022 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2448 batch_limit:   8677 Loss:   0.2020 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2449 batch_limit:   8677 Loss:   0.2019 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2450 batch_limit:   8677 Loss:   0.2018 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2451 batch_limit:   8677 Loss:   0.2017 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2452 batch_limit:   8677 Loss:   0.2015 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2453 batch_limit:   8677 Loss:   0.2014 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2454 batch_limit:   8677 Loss:   0.2013 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2455 batch_limit:   8677 Loss:   0.2011 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2456 batch_limit:   8677 Loss:   0.2010 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2457 batch_limit:   8677 Loss:   0.2009 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2458 batch_limit:   8677 Loss:   0.2008 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2459 batch_limit:   8677 Loss:   0.2006 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2460 batch_limit:   8677 Loss:   0.2005 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2461 batch_limit:   8677 Loss:   0.2004 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2462 batch_limit:   8677 Loss:   0.2003 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2463 batch_limit:   8677 Loss:   0.2001 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2464 batch_limit:   8677 Loss:   0.2000 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2465 batch_limit:   8677 Loss:   0.1999 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2466 batch_limit:   8677 Loss:   0.1997 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2467 batch_limit:   8677 Loss:   0.1996 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2468 batch_limit:   8677 Loss:   0.1995 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2469 batch_limit:   8677 Loss:   0.1993 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2470 batch_limit:   8677 Loss:   0.1992 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2471 batch_limit:   8677 Loss:   0.1991 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2472 batch_limit:   8677 Loss:   0.1990 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2473 batch_limit:   8677 Loss:   0.1988 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2474 batch_limit:   8677 Loss:   0.1987 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2475 batch_limit:   8677 Loss:   0.1986 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2476 batch_limit:   8677 Loss:   0.1984 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2477 batch_limit:   8677 Loss:   0.1983 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2478 batch_limit:   8677 Loss:   0.1982 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2479 batch_limit:   8677 Loss:   0.1980 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2480 batch_limit:   8677 Loss:   0.1979 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2481 batch_limit:   8677 Loss:   0.1978 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2482 batch_limit:   8677 Loss:   0.1976 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2483 batch_limit:   8677 Loss:   0.1975 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2484 batch_limit:   8677 Loss:   0.1974 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2485 batch_limit:   8677 Loss:   0.1972 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2486 batch_limit:   8677 Loss:   0.1971 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2487 batch_limit:   8677 Loss:   0.1970 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2488 batch_limit:   8677 Loss:   0.1968 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2489 batch_limit:   8677 Loss:   0.1967 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2490 batch_limit:   8677 Loss:   0.1966 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2491 batch_limit:   8677 Loss:   0.1964 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2492 batch_limit:   8677 Loss:   0.1963 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2493 batch_limit:   8677 Loss:   0.1962 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2494 batch_limit:   8677 Loss:   0.1960 Training Acc:    99.01  6765.42    99.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2495 batch_limit:   8677 Loss:   0.1959 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2496 batch_limit:   8677 Loss:   0.1957 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2497 batch_limit:   8677 Loss:   0.1956 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2498 batch_limit:   8677 Loss:   0.1955 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2499 batch_limit:   8677 Loss:   0.1953 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2500 batch_limit:   8677 Loss:   0.1952 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2501 batch_limit:   8677 Loss:   0.1951 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2502 batch_limit:   8677 Loss:   0.1949 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2503 batch_limit:   8677 Loss:   0.1948 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2504 batch_limit:   8677 Loss:   0.1947 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2505 batch_limit:   8677 Loss:   0.1945 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2506 batch_limit:   8677 Loss:   0.1944 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2507 batch_limit:   8677 Loss:   0.1942 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2508 batch_limit:   8677 Loss:   0.1941 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2509 batch_limit:   8677 Loss:   0.1940 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2510 batch_limit:   8677 Loss:   0.1938 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2511 batch_limit:   8677 Loss:   0.1937 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2512 batch_limit:   8677 Loss:   0.1935 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2513 batch_limit:   8677 Loss:   0.1934 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2514 batch_limit:   8677 Loss:   0.1933 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2515 batch_limit:   8677 Loss:   0.1931 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2516 batch_limit:   8677 Loss:   0.1930 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2517 batch_limit:   8677 Loss:   0.1928 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2518 batch_limit:   8677 Loss:   0.1927 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2519 batch_limit:   8677 Loss:   0.1926 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2520 batch_limit:   8677 Loss:   0.1924 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2521 batch_limit:   8677 Loss:   0.1923 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2522 batch_limit:   8677 Loss:   0.1921 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2523 batch_limit:   8677 Loss:   0.1920 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2524 batch_limit:   8677 Loss:   0.1919 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2525 batch_limit:   8677 Loss:   0.1917 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2526 batch_limit:   8677 Loss:   0.1916 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2527 batch_limit:   8677 Loss:   0.1914 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2528 batch_limit:   8677 Loss:   0.1913 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2529 batch_limit:   8677 Loss:   0.1912 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2530 batch_limit:   8677 Loss:   0.1910 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2531 batch_limit:   8677 Loss:   0.1909 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2532 batch_limit:   8677 Loss:   0.1907 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2533 batch_limit:   8677 Loss:   0.1906 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2534 batch_limit:   8677 Loss:   0.1904 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2535 batch_limit:   8677 Loss:   0.1903 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2536 batch_limit:   8677 Loss:   0.1901 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2537 batch_limit:   8677 Loss:   0.1900 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2538 batch_limit:   8677 Loss:   0.1899 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2539 batch_limit:   8677 Loss:   0.1897 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2540 batch_limit:   8677 Loss:   0.1896 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2541 batch_limit:   8677 Loss:   0.1894 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2542 batch_limit:   8677 Loss:   0.1893 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2543 batch_limit:   8677 Loss:   0.1891 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2544 batch_limit:   8677 Loss:   0.1890 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2545 batch_limit:   8677 Loss:   0.1888 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2546 batch_limit:   8677 Loss:   0.1887 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2547 batch_limit:   8677 Loss:   0.1886 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2548 batch_limit:   8677 Loss:   0.1884 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2549 batch_limit:   8677 Loss:   0.1883 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2550 batch_limit:   8677 Loss:   0.1881 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2551 batch_limit:   8677 Loss:   0.1880 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2552 batch_limit:   8677 Loss:   0.1878 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2553 batch_limit:   8677 Loss:   0.1877 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2554 batch_limit:   8677 Loss:   0.1875 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2555 batch_limit:   8677 Loss:   0.1874 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2556 batch_limit:   8677 Loss:   0.1872 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2557 batch_limit:   8677 Loss:   0.1871 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2558 batch_limit:   8677 Loss:   0.1869 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2559 batch_limit:   8677 Loss:   0.1868 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2560 batch_limit:   8677 Loss:   0.1867 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2561 batch_limit:   8677 Loss:   0.1865 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2562 batch_limit:   8677 Loss:   0.1864 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2563 batch_limit:   8677 Loss:   0.1862 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2564 batch_limit:   8677 Loss:   0.1861 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2565 batch_limit:   8677 Loss:   0.1859 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2566 batch_limit:   8677 Loss:   0.1858 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2567 batch_limit:   8677 Loss:   0.1856 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2568 batch_limit:   8677 Loss:   0.1855 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2569 batch_limit:   8677 Loss:   0.1853 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2570 batch_limit:   8677 Loss:   0.1852 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2571 batch_limit:   8677 Loss:   0.1850 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2572 batch_limit:   8677 Loss:   0.1849 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2573 batch_limit:   8677 Loss:   0.1847 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2574 batch_limit:   8677 Loss:   0.1846 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2575 batch_limit:   8677 Loss:   0.1844 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2576 batch_limit:   8677 Loss:   0.1843 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2577 batch_limit:   8677 Loss:   0.1841 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2578 batch_limit:   8677 Loss:   0.1840 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2579 batch_limit:   8677 Loss:   0.1838 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2580 batch_limit:   8677 Loss:   0.1837 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2581 batch_limit:   8677 Loss:   0.1835 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2582 batch_limit:   8677 Loss:   0.1834 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2583 batch_limit:   8677 Loss:   0.1832 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2584 batch_limit:   8677 Loss:   0.1831 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2585 batch_limit:   8677 Loss:   0.1829 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2586 batch_limit:   8677 Loss:   0.1828 Training Acc:    99.01  6764.63    99.48%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2587 batch_limit:   8677 Loss:   0.1826 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2588 batch_limit:   8677 Loss:   0.1825 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2589 batch_limit:   8677 Loss:   0.1823 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2590 batch_limit:   8677 Loss:   0.1822 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2591 batch_limit:   8677 Loss:   0.1820 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2592 batch_limit:   8677 Loss:   0.1819 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2593 batch_limit:   8677 Loss:   0.1817 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2594 batch_limit:   8677 Loss:   0.1816 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2595 batch_limit:   8677 Loss:   0.1814 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2596 batch_limit:   8677 Loss:   0.1813 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2597 batch_limit:   8677 Loss:   0.1811 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2598 batch_limit:   8677 Loss:   0.1810 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2599 batch_limit:   8677 Loss:   0.1808 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2600 batch_limit:   8677 Loss:   0.1807 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2601 batch_limit:   8677 Loss:   0.1805 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2602 batch_limit:   8677 Loss:   0.1804 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2603 batch_limit:   8677 Loss:   0.1802 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2604 batch_limit:   8677 Loss:   0.1801 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2605 batch_limit:   8677 Loss:   0.1799 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2606 batch_limit:   8677 Loss:   0.1798 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2607 batch_limit:   8677 Loss:   0.1796 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2608 batch_limit:   8677 Loss:   0.1795 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2609 batch_limit:   8677 Loss:   0.1793 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2610 batch_limit:   8677 Loss:   0.1792 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2611 batch_limit:   8677 Loss:   0.1790 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2612 batch_limit:   8677 Loss:   0.1789 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2613 batch_limit:   8677 Loss:   0.1787 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2614 batch_limit:   8677 Loss:   0.1786 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2615 batch_limit:   8677 Loss:   0.1784 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2616 batch_limit:   8677 Loss:   0.1782 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2617 batch_limit:   8677 Loss:   0.1781 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2618 batch_limit:   8677 Loss:   0.1779 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2619 batch_limit:   8677 Loss:   0.1778 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2620 batch_limit:   8677 Loss:   0.1776 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2621 batch_limit:   8677 Loss:   0.1775 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2622 batch_limit:   8677 Loss:   0.1773 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2623 batch_limit:   8677 Loss:   0.1772 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2624 batch_limit:   8677 Loss:   0.1770 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2625 batch_limit:   8677 Loss:   0.1769 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2626 batch_limit:   8677 Loss:   0.1767 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2627 batch_limit:   8677 Loss:   0.1766 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2628 batch_limit:   8677 Loss:   0.1764 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2629 batch_limit:   8677 Loss:   0.1763 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2630 batch_limit:   8677 Loss:   0.1761 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2631 batch_limit:   8677 Loss:   0.1760 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2632 batch_limit:   8677 Loss:   0.1758 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2633 batch_limit:   8677 Loss:   0.1757 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2634 batch_limit:   8677 Loss:   0.1755 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2635 batch_limit:   8677 Loss:   0.1754 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2636 batch_limit:   8677 Loss:   0.1752 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2637 batch_limit:   8677 Loss:   0.1751 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2638 batch_limit:   8677 Loss:   0.1749 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2639 batch_limit:   8677 Loss:   0.1748 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2640 batch_limit:   8677 Loss:   0.1746 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2641 batch_limit:   8677 Loss:   0.1745 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2642 batch_limit:   8677 Loss:   0.1743 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2643 batch_limit:   8677 Loss:   0.1742 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2644 batch_limit:   8677 Loss:   0.1740 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2645 batch_limit:   8677 Loss:   0.1738 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2646 batch_limit:   8677 Loss:   0.1737 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2647 batch_limit:   8677 Loss:   0.1735 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2648 batch_limit:   8677 Loss:   0.1734 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2649 batch_limit:   8677 Loss:   0.1732 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2650 batch_limit:   8677 Loss:   0.1731 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2651 batch_limit:   8677 Loss:   0.1729 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2652 batch_limit:   8677 Loss:   0.1728 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2653 batch_limit:   8677 Loss:   0.1726 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2654 batch_limit:   8677 Loss:   0.1725 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2655 batch_limit:   8677 Loss:   0.1723 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2656 batch_limit:   8677 Loss:   0.1722 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2657 batch_limit:   8677 Loss:   0.1720 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2658 batch_limit:   8677 Loss:   0.1719 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2659 batch_limit:   8677 Loss:   0.1717 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2660 batch_limit:   8677 Loss:   0.1716 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2661 batch_limit:   8677 Loss:   0.1714 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2662 batch_limit:   8677 Loss:   0.1713 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2663 batch_limit:   8677 Loss:   0.1711 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2664 batch_limit:   8677 Loss:   0.1710 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2665 batch_limit:   8677 Loss:   0.1708 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2666 batch_limit:   8677 Loss:   0.1707 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2667 batch_limit:   8677 Loss:   0.1705 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2668 batch_limit:   8677 Loss:   0.1704 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2669 batch_limit:   8677 Loss:   0.1703 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2670 batch_limit:   8677 Loss:   0.1701 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2671 batch_limit:   8677 Loss:   0.1700 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2672 batch_limit:   8677 Loss:   0.1698 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2673 batch_limit:   8677 Loss:   0.1697 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2674 batch_limit:   8677 Loss:   0.1695 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2675 batch_limit:   8677 Loss:   0.1694 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2676 batch_limit:   8677 Loss:   0.1692 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2677 batch_limit:   8677 Loss:   0.1691 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2678 batch_limit:   8677 Loss:   0.1689 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2679 batch_limit:   8677 Loss:   0.1688 Training Acc:    99.01  6764.63    99.48%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2680 batch_limit:   8677 Loss:   0.1686 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2681 batch_limit:   8677 Loss:   0.1685 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2682 batch_limit:   8677 Loss:   0.1683 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2683 batch_limit:   8677 Loss:   0.1682 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2684 batch_limit:   8677 Loss:   0.1680 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2685 batch_limit:   8677 Loss:   0.1679 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2686 batch_limit:   8677 Loss:   0.1677 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2687 batch_limit:   8677 Loss:   0.1676 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2688 batch_limit:   8677 Loss:   0.1675 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2689 batch_limit:   8677 Loss:   0.1673 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2690 batch_limit:   8677 Loss:   0.1672 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2691 batch_limit:   8677 Loss:   0.1670 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2692 batch_limit:   8677 Loss:   0.1669 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2693 batch_limit:   8677 Loss:   0.1667 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2694 batch_limit:   8677 Loss:   0.1666 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2695 batch_limit:   8677 Loss:   0.1664 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2696 batch_limit:   8677 Loss:   0.1663 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2697 batch_limit:   8677 Loss:   0.1662 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2698 batch_limit:   8677 Loss:   0.1660 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2699 batch_limit:   8677 Loss:   0.1659 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2700 batch_limit:   8677 Loss:   0.1657 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2701 batch_limit:   8677 Loss:   0.1656 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2702 batch_limit:   8677 Loss:   0.1654 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2703 batch_limit:   8677 Loss:   0.1653 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2704 batch_limit:   8677 Loss:   0.1652 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2705 batch_limit:   8677 Loss:   0.1650 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2706 batch_limit:   8677 Loss:   0.1649 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2707 batch_limit:   8677 Loss:   0.1647 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2708 batch_limit:   8677 Loss:   0.1646 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2709 batch_limit:   8677 Loss:   0.1644 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2710 batch_limit:   8677 Loss:   0.1643 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2711 batch_limit:   8677 Loss:   0.1642 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2712 batch_limit:   8677 Loss:   0.1640 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2713 batch_limit:   8677 Loss:   0.1639 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2714 batch_limit:   8677 Loss:   0.1637 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2715 batch_limit:   8677 Loss:   0.1636 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2716 batch_limit:   8677 Loss:   0.1635 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2717 batch_limit:   8677 Loss:   0.1633 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2718 batch_limit:   8677 Loss:   0.1632 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2719 batch_limit:   8677 Loss:   0.1630 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2720 batch_limit:   8677 Loss:   0.1629 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2721 batch_limit:   8677 Loss:   0.1628 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2722 batch_limit:   8677 Loss:   0.1626 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2723 batch_limit:   8677 Loss:   0.1625 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2724 batch_limit:   8677 Loss:   0.1623 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2725 batch_limit:   8677 Loss:   0.1622 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2726 batch_limit:   8677 Loss:   0.1621 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2727 batch_limit:   8677 Loss:   0.1619 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2728 batch_limit:   8677 Loss:   0.1618 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2729 batch_limit:   8677 Loss:   0.1617 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2730 batch_limit:   8677 Loss:   0.1615 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2731 batch_limit:   8677 Loss:   0.1614 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2732 batch_limit:   8677 Loss:   0.1613 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2733 batch_limit:   8677 Loss:   0.1611 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2734 batch_limit:   8677 Loss:   0.1610 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2735 batch_limit:   8677 Loss:   0.1608 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2736 batch_limit:   8677 Loss:   0.1607 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2737 batch_limit:   8677 Loss:   0.1606 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2738 batch_limit:   8677 Loss:   0.1604 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2739 batch_limit:   8677 Loss:   0.1603 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2740 batch_limit:   8677 Loss:   0.1602 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2741 batch_limit:   8677 Loss:   0.1600 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2742 batch_limit:   8677 Loss:   0.1599 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2743 batch_limit:   8677 Loss:   0.1598 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2744 batch_limit:   8677 Loss:   0.1596 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2745 batch_limit:   8677 Loss:   0.1595 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2746 batch_limit:   8677 Loss:   0.1594 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2747 batch_limit:   8677 Loss:   0.1592 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2748 batch_limit:   8677 Loss:   0.1591 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2749 batch_limit:   8677 Loss:   0.1590 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2750 batch_limit:   8677 Loss:   0.1589 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2751 batch_limit:   8677 Loss:   0.1587 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2752 batch_limit:   8677 Loss:   0.1586 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2753 batch_limit:   8677 Loss:   0.1585 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2754 batch_limit:   8677 Loss:   0.1583 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2755 batch_limit:   8677 Loss:   0.1582 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2756 batch_limit:   8677 Loss:   0.1581 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2757 batch_limit:   8677 Loss:   0.1579 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2758 batch_limit:   8677 Loss:   0.1578 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2759 batch_limit:   8677 Loss:   0.1577 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2760 batch_limit:   8677 Loss:   0.1576 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2761 batch_limit:   8677 Loss:   0.1574 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2762 batch_limit:   8677 Loss:   0.1573 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2763 batch_limit:   8677 Loss:   0.1572 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2764 batch_limit:   8677 Loss:   0.1571 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2765 batch_limit:   8677 Loss:   0.1569 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2766 batch_limit:   8677 Loss:   0.1568 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2767 batch_limit:   8677 Loss:   0.1567 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2768 batch_limit:   8677 Loss:   0.1565 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2769 batch_limit:   8677 Loss:   0.1564 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2770 batch_limit:   8677 Loss:   0.1563 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2771 batch_limit:   8677 Loss:   0.1562 Training Acc:    99.01  6764.63    99.48%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2772 batch_limit:   8677 Loss:   0.1560 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2773 batch_limit:   8677 Loss:   0.1559 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2774 batch_limit:   8677 Loss:   0.1558 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2775 batch_limit:   8677 Loss:   0.1557 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2776 batch_limit:   8677 Loss:   0.1555 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2777 batch_limit:   8677 Loss:   0.1554 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2778 batch_limit:   8677 Loss:   0.1553 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2779 batch_limit:   8677 Loss:   0.1552 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2780 batch_limit:   8677 Loss:   0.1551 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2781 batch_limit:   8677 Loss:   0.1549 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2782 batch_limit:   8677 Loss:   0.1548 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2783 batch_limit:   8677 Loss:   0.1547 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2784 batch_limit:   8677 Loss:   0.1546 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2785 batch_limit:   8677 Loss:   0.1544 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2786 batch_limit:   8677 Loss:   0.1543 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2787 batch_limit:   8677 Loss:   0.1542 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2788 batch_limit:   8677 Loss:   0.1541 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2789 batch_limit:   8677 Loss:   0.1540 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2790 batch_limit:   8677 Loss:   0.1538 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2791 batch_limit:   8677 Loss:   0.1537 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2792 batch_limit:   8677 Loss:   0.1536 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2793 batch_limit:   8677 Loss:   0.1535 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2794 batch_limit:   8677 Loss:   0.1534 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2795 batch_limit:   8677 Loss:   0.1533 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2796 batch_limit:   8677 Loss:   0.1531 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2797 batch_limit:   8677 Loss:   0.1530 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2798 batch_limit:   8677 Loss:   0.1529 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2799 batch_limit:   8677 Loss:   0.1528 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2800 batch_limit:   8677 Loss:   0.1527 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2801 batch_limit:   8677 Loss:   0.1525 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2802 batch_limit:   8677 Loss:   0.1524 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2803 batch_limit:   8677 Loss:   0.1523 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2804 batch_limit:   8677 Loss:   0.1522 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2805 batch_limit:   8677 Loss:   0.1521 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2806 batch_limit:   8677 Loss:   0.1520 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2807 batch_limit:   8677 Loss:   0.1519 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2808 batch_limit:   8677 Loss:   0.1517 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2809 batch_limit:   8677 Loss:   0.1516 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2810 batch_limit:   8677 Loss:   0.1515 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2811 batch_limit:   8677 Loss:   0.1514 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2812 batch_limit:   8677 Loss:   0.1513 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2813 batch_limit:   8677 Loss:   0.1512 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2814 batch_limit:   8677 Loss:   0.1511 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2815 batch_limit:   8677 Loss:   0.1509 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2816 batch_limit:   8677 Loss:   0.1508 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2817 batch_limit:   8677 Loss:   0.1507 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2818 batch_limit:   8677 Loss:   0.1506 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2819 batch_limit:   8677 Loss:   0.1505 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2820 batch_limit:   8677 Loss:   0.1504 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2821 batch_limit:   8677 Loss:   0.1503 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2822 batch_limit:   8677 Loss:   0.1502 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2823 batch_limit:   8677 Loss:   0.1500 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2824 batch_limit:   8677 Loss:   0.1499 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2825 batch_limit:   8677 Loss:   0.1498 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2826 batch_limit:   8677 Loss:   0.1497 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2827 batch_limit:   8677 Loss:   0.1496 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2828 batch_limit:   8677 Loss:   0.1495 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2829 batch_limit:   8677 Loss:   0.1494 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2830 batch_limit:   8677 Loss:   0.1493 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2831 batch_limit:   8677 Loss:   0.1492 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2832 batch_limit:   8677 Loss:   0.1491 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2833 batch_limit:   8677 Loss:   0.1489 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2834 batch_limit:   8677 Loss:   0.1488 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2835 batch_limit:   8677 Loss:   0.1487 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2836 batch_limit:   8677 Loss:   0.1486 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2837 batch_limit:   8677 Loss:   0.1485 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2838 batch_limit:   8677 Loss:   0.1484 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2839 batch_limit:   8677 Loss:   0.1483 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2840 batch_limit:   8677 Loss:   0.1482 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2841 batch_limit:   8677 Loss:   0.1481 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2842 batch_limit:   8677 Loss:   0.1480 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2843 batch_limit:   8677 Loss:   0.1479 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2844 batch_limit:   8677 Loss:   0.1478 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2845 batch_limit:   8677 Loss:   0.1477 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2846 batch_limit:   8677 Loss:   0.1476 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2847 batch_limit:   8677 Loss:   0.1475 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2848 batch_limit:   8677 Loss:   0.1473 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2849 batch_limit:   8677 Loss:   0.1472 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2850 batch_limit:   8677 Loss:   0.1471 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2851 batch_limit:   8677 Loss:   0.1470 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2852 batch_limit:   8677 Loss:   0.1469 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2853 batch_limit:   8677 Loss:   0.1468 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2854 batch_limit:   8677 Loss:   0.1467 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2855 batch_limit:   8677 Loss:   0.1466 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2856 batch_limit:   8677 Loss:   0.1465 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2857 batch_limit:   8677 Loss:   0.1464 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2858 batch_limit:   8677 Loss:   0.1463 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2859 batch_limit:   8677 Loss:   0.1462 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2860 batch_limit:   8677 Loss:   0.1461 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2861 batch_limit:   8677 Loss:   0.1460 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2862 batch_limit:   8677 Loss:   0.1459 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2863 batch_limit:   8677 Loss:   0.1458 Training Acc:    99.01  6764.63    99.48%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2864 batch_limit:   8677 Loss:   0.1457 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2865 batch_limit:   8677 Loss:   0.1456 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2866 batch_limit:   8677 Loss:   0.1455 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2867 batch_limit:   8677 Loss:   0.1454 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2868 batch_limit:   8677 Loss:   0.1453 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2869 batch_limit:   8677 Loss:   0.1452 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2870 batch_limit:   8677 Loss:   0.1451 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2871 batch_limit:   8677 Loss:   0.1450 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2872 batch_limit:   8677 Loss:   0.1449 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2873 batch_limit:   8677 Loss:   0.1448 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2874 batch_limit:   8677 Loss:   0.1447 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2875 batch_limit:   8677 Loss:   0.1446 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2876 batch_limit:   8677 Loss:   0.1445 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2877 batch_limit:   8677 Loss:   0.1444 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2878 batch_limit:   8677 Loss:   0.1443 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2879 batch_limit:   8677 Loss:   0.1442 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2880 batch_limit:   8677 Loss:   0.1441 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2881 batch_limit:   8677 Loss:   0.1440 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2882 batch_limit:   8677 Loss:   0.1439 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2883 batch_limit:   8677 Loss:   0.1438 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2884 batch_limit:   8677 Loss:   0.1437 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2885 batch_limit:   8677 Loss:   0.1436 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2886 batch_limit:   8677 Loss:   0.1435 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2887 batch_limit:   8677 Loss:   0.1434 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2888 batch_limit:   8677 Loss:   0.1433 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2889 batch_limit:   8677 Loss:   0.1432 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2890 batch_limit:   8677 Loss:   0.1431 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2891 batch_limit:   8677 Loss:   0.1430 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2892 batch_limit:   8677 Loss:   0.1429 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2893 batch_limit:   8677 Loss:   0.1428 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2894 batch_limit:   8677 Loss:   0.1427 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2895 batch_limit:   8677 Loss:   0.1426 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2896 batch_limit:   8677 Loss:   0.1425 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2897 batch_limit:   8677 Loss:   0.1425 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2898 batch_limit:   8677 Loss:   0.1424 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2899 batch_limit:   8677 Loss:   0.1423 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2900 batch_limit:   8677 Loss:   0.1422 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2901 batch_limit:   8677 Loss:   0.1421 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2902 batch_limit:   8677 Loss:   0.1420 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2903 batch_limit:   8677 Loss:   0.1419 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2904 batch_limit:   8677 Loss:   0.1418 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2905 batch_limit:   8677 Loss:   0.1417 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2906 batch_limit:   8677 Loss:   0.1416 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2907 batch_limit:   8677 Loss:   0.1415 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2908 batch_limit:   8677 Loss:   0.1414 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2909 batch_limit:   8677 Loss:   0.1413 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2910 batch_limit:   8677 Loss:   0.1412 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2911 batch_limit:   8677 Loss:   0.1411 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2912 batch_limit:   8677 Loss:   0.1410 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2913 batch_limit:   8677 Loss:   0.1410 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2914 batch_limit:   8677 Loss:   0.1408 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2915 batch_limit:   8677 Loss:   0.1408 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2916 batch_limit:   8677 Loss:   0.1406 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2917 batch_limit:   8677 Loss:   0.1406 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2918 batch_limit:   8677 Loss:   0.1405 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2919 batch_limit:   8677 Loss:   0.1404 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2920 batch_limit:   8677 Loss:   0.1403 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2921 batch_limit:   8677 Loss:   0.1402 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2922 batch_limit:   8677 Loss:   0.1401 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2923 batch_limit:   8677 Loss:   0.1400 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2924 batch_limit:   8677 Loss:   0.1399 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2925 batch_limit:   8677 Loss:   0.1399 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2926 batch_limit:   8677 Loss:   0.1397 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2927 batch_limit:   8677 Loss:   0.1397 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2928 batch_limit:   8677 Loss:   0.1395 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   2929 batch_limit:   8677 Loss:   0.1395 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2930 batch_limit:   8677 Loss:   0.1393 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2931 batch_limit:   8677 Loss:   0.1393 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2932 batch_limit:   8677 Loss:   0.1391 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2933 batch_limit:   8677 Loss:   0.1392 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2934 batch_limit:   8677 Loss:   0.1390 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2935 batch_limit:   8677 Loss:   0.1390 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2936 batch_limit:   8677 Loss:   0.1388 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2937 batch_limit:   8677 Loss:   0.1388 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2938 batch_limit:   8677 Loss:   0.1386 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2939 batch_limit:   8677 Loss:   0.1387 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2940 batch_limit:   8677 Loss:   0.1384 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2941 batch_limit:   8677 Loss:   0.1385 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2942 batch_limit:   8677 Loss:   0.1382 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2943 batch_limit:   8677 Loss:   0.1384 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2944 batch_limit:   8677 Loss:   0.1380 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2945 batch_limit:   8677 Loss:   0.1382 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2946 batch_limit:   8677 Loss:   0.1378 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2947 batch_limit:   8677 Loss:   0.1381 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2948 batch_limit:   8677 Loss:   0.1376 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2949 batch_limit:   8677 Loss:   0.1379 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2950 batch_limit:   8677 Loss:   0.1374 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2951 batch_limit:   8677 Loss:   0.1378 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2952 batch_limit:   8677 Loss:   0.1372 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2953 batch_limit:   8677 Loss:   0.1377 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2954 batch_limit:   8677 Loss:   0.1370 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2955 batch_limit:   8677 Loss:   0.1376 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2956 batch_limit:   8677 Loss:   0.1368 Training Acc:    99.01  6765.42    99.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2957 batch_limit:   8677 Loss:   0.1375 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2958 batch_limit:   8677 Loss:   0.1366 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2959 batch_limit:   8677 Loss:   0.1373 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2960 batch_limit:   8677 Loss:   0.1364 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2961 batch_limit:   8677 Loss:   0.1372 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2962 batch_limit:   8677 Loss:   0.1363 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2963 batch_limit:   8677 Loss:   0.1371 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2964 batch_limit:   8677 Loss:   0.1361 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2965 batch_limit:   8677 Loss:   0.1369 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2966 batch_limit:   8677 Loss:   0.1359 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2967 batch_limit:   8677 Loss:   0.1367 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2968 batch_limit:   8677 Loss:   0.1357 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2969 batch_limit:   8677 Loss:   0.1366 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2970 batch_limit:   8677 Loss:   0.1356 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2971 batch_limit:   8677 Loss:   0.1364 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2972 batch_limit:   8677 Loss:   0.1354 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2973 batch_limit:   8677 Loss:   0.1362 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2974 batch_limit:   8677 Loss:   0.1352 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2975 batch_limit:   8677 Loss:   0.1360 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2976 batch_limit:   8677 Loss:   0.1351 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2977 batch_limit:   8677 Loss:   0.1358 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2978 batch_limit:   8677 Loss:   0.1349 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2979 batch_limit:   8677 Loss:   0.1356 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2980 batch_limit:   8677 Loss:   0.1347 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2981 batch_limit:   8677 Loss:   0.1354 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2982 batch_limit:   8677 Loss:   0.1346 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2983 batch_limit:   8677 Loss:   0.1353 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2984 batch_limit:   8677 Loss:   0.1344 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2985 batch_limit:   8677 Loss:   0.1351 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2986 batch_limit:   8677 Loss:   0.1342 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2987 batch_limit:   8677 Loss:   0.1349 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2988 batch_limit:   8677 Loss:   0.1341 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2989 batch_limit:   8677 Loss:   0.1347 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2990 batch_limit:   8677 Loss:   0.1339 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2991 batch_limit:   8677 Loss:   0.1346 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2992 batch_limit:   8677 Loss:   0.1337 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2993 batch_limit:   8677 Loss:   0.1344 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2994 batch_limit:   8677 Loss:   0.1336 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2995 batch_limit:   8677 Loss:   0.1343 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2996 batch_limit:   8677 Loss:   0.1334 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2997 batch_limit:   8677 Loss:   0.1341 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2998 batch_limit:   8677 Loss:   0.1332 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   2999 batch_limit:   8677 Loss:   0.1339 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3000 batch_limit:   8677 Loss:   0.1330 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3001 batch_limit:   8677 Loss:   0.1338 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3002 batch_limit:   8677 Loss:   0.1329 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3003 batch_limit:   8677 Loss:   0.1336 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3004 batch_limit:   8677 Loss:   0.1327 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3005 batch_limit:   8677 Loss:   0.1335 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3006 batch_limit:   8677 Loss:   0.1325 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3007 batch_limit:   8677 Loss:   0.1333 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3008 batch_limit:   8677 Loss:   0.1324 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3009 batch_limit:   8677 Loss:   0.1332 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3010 batch_limit:   8677 Loss:   0.1322 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3011 batch_limit:   8677 Loss:   0.1330 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3012 batch_limit:   8677 Loss:   0.1320 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3013 batch_limit:   8677 Loss:   0.1329 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3014 batch_limit:   8677 Loss:   0.1319 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3015 batch_limit:   8677 Loss:   0.1327 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3016 batch_limit:   8677 Loss:   0.1317 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3017 batch_limit:   8677 Loss:   0.1326 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3018 batch_limit:   8677 Loss:   0.1316 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3019 batch_limit:   8677 Loss:   0.1324 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3020 batch_limit:   8677 Loss:   0.1314 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3021 batch_limit:   8677 Loss:   0.1323 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3022 batch_limit:   8677 Loss:   0.1312 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3023 batch_limit:   8677 Loss:   0.1321 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3024 batch_limit:   8677 Loss:   0.1311 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3025 batch_limit:   8677 Loss:   0.1320 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3026 batch_limit:   8677 Loss:   0.1309 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3027 batch_limit:   8677 Loss:   0.1318 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3028 batch_limit:   8677 Loss:   0.1307 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3029 batch_limit:   8677 Loss:   0.1316 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3030 batch_limit:   8677 Loss:   0.1306 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3031 batch_limit:   8677 Loss:   0.1315 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3032 batch_limit:   8677 Loss:   0.1304 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3033 batch_limit:   8677 Loss:   0.1313 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3034 batch_limit:   8677 Loss:   0.1303 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3035 batch_limit:   8677 Loss:   0.1312 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3036 batch_limit:   8677 Loss:   0.1301 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3037 batch_limit:   8677 Loss:   0.1310 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3038 batch_limit:   8677 Loss:   0.1299 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3039 batch_limit:   8677 Loss:   0.1309 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3040 batch_limit:   8677 Loss:   0.1298 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3041 batch_limit:   8677 Loss:   0.1307 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3042 batch_limit:   8677 Loss:   0.1296 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3043 batch_limit:   8677 Loss:   0.1306 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3044 batch_limit:   8677 Loss:   0.1295 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3045 batch_limit:   8677 Loss:   0.1304 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3046 batch_limit:   8677 Loss:   0.1293 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3047 batch_limit:   8677 Loss:   0.1303 Training Acc:    99.01  6765.42    99.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3048 batch_limit:   8677 Loss:   0.1291 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3049 batch_limit:   8677 Loss:   0.1301 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3050 batch_limit:   8677 Loss:   0.1290 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3051 batch_limit:   8677 Loss:   0.1300 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3052 batch_limit:   8677 Loss:   0.1288 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3053 batch_limit:   8677 Loss:   0.1298 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3054 batch_limit:   8677 Loss:   0.1287 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3055 batch_limit:   8677 Loss:   0.1297 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3056 batch_limit:   8677 Loss:   0.1285 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3057 batch_limit:   8677 Loss:   0.1295 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3058 batch_limit:   8677 Loss:   0.1284 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3059 batch_limit:   8677 Loss:   0.1294 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3060 batch_limit:   8677 Loss:   0.1282 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3061 batch_limit:   8677 Loss:   0.1292 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3062 batch_limit:   8677 Loss:   0.1281 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3063 batch_limit:   8677 Loss:   0.1291 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3064 batch_limit:   8677 Loss:   0.1279 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3065 batch_limit:   8677 Loss:   0.1289 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3066 batch_limit:   8677 Loss:   0.1277 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3067 batch_limit:   8677 Loss:   0.1288 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3068 batch_limit:   8677 Loss:   0.1276 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3069 batch_limit:   8677 Loss:   0.1286 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3070 batch_limit:   8677 Loss:   0.1274 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3071 batch_limit:   8677 Loss:   0.1285 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3072 batch_limit:   8677 Loss:   0.1273 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3073 batch_limit:   8677 Loss:   0.1284 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3074 batch_limit:   8677 Loss:   0.1271 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3075 batch_limit:   8677 Loss:   0.1282 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3076 batch_limit:   8677 Loss:   0.1270 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3077 batch_limit:   8677 Loss:   0.1281 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3078 batch_limit:   8677 Loss:   0.1268 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3079 batch_limit:   8677 Loss:   0.1279 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3080 batch_limit:   8677 Loss:   0.1267 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3081 batch_limit:   8677 Loss:   0.1278 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3082 batch_limit:   8677 Loss:   0.1265 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3083 batch_limit:   8677 Loss:   0.1276 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3084 batch_limit:   8677 Loss:   0.1264 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3085 batch_limit:   8677 Loss:   0.1275 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3086 batch_limit:   8677 Loss:   0.1262 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3087 batch_limit:   8677 Loss:   0.1273 Training Acc:    99.01  6765.42    99.49%\n",
      "Epoch   3088 batch_limit:   8677 Loss:   0.1261 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3089 batch_limit:   8677 Loss:   0.1272 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3090 batch_limit:   8677 Loss:   0.1259 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3091 batch_limit:   8677 Loss:   0.1271 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3092 batch_limit:   8677 Loss:   0.1258 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3093 batch_limit:   8677 Loss:   0.1269 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3094 batch_limit:   8677 Loss:   0.1256 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3095 batch_limit:   8677 Loss:   0.1268 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3096 batch_limit:   8677 Loss:   0.1255 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3097 batch_limit:   8677 Loss:   0.1266 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3098 batch_limit:   8677 Loss:   0.1253 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3099 batch_limit:   8677 Loss:   0.1265 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3100 batch_limit:   8677 Loss:   0.1252 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3101 batch_limit:   8677 Loss:   0.1264 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3102 batch_limit:   8677 Loss:   0.1250 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3103 batch_limit:   8677 Loss:   0.1262 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3104 batch_limit:   8677 Loss:   0.1249 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3105 batch_limit:   8677 Loss:   0.1261 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3106 batch_limit:   8677 Loss:   0.1247 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3107 batch_limit:   8677 Loss:   0.1259 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3108 batch_limit:   8677 Loss:   0.1246 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3109 batch_limit:   8677 Loss:   0.1258 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3110 batch_limit:   8677 Loss:   0.1244 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3111 batch_limit:   8677 Loss:   0.1257 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3112 batch_limit:   8677 Loss:   0.1243 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3113 batch_limit:   8677 Loss:   0.1255 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3114 batch_limit:   8677 Loss:   0.1241 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3115 batch_limit:   8677 Loss:   0.1254 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3116 batch_limit:   8677 Loss:   0.1240 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3117 batch_limit:   8677 Loss:   0.1252 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3118 batch_limit:   8677 Loss:   0.1238 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3119 batch_limit:   8677 Loss:   0.1251 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3120 batch_limit:   8677 Loss:   0.1237 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3121 batch_limit:   8677 Loss:   0.1250 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3122 batch_limit:   8677 Loss:   0.1235 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3123 batch_limit:   8677 Loss:   0.1248 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3124 batch_limit:   8677 Loss:   0.1234 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3125 batch_limit:   8677 Loss:   0.1247 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3126 batch_limit:   8677 Loss:   0.1233 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3127 batch_limit:   8677 Loss:   0.1246 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3128 batch_limit:   8677 Loss:   0.1231 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3129 batch_limit:   8677 Loss:   0.1244 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3130 batch_limit:   8677 Loss:   0.1230 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3131 batch_limit:   8677 Loss:   0.1243 Training Acc:    99.01  6764.63    99.48%\n",
      "Epoch   3132 batch_limit:   8677 Loss:   0.1228 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3133 batch_limit:   8677 Loss:   0.1241 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3134 batch_limit:   8677 Loss:   0.1227 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3135 batch_limit:   8677 Loss:   0.1240 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3136 batch_limit:   8677 Loss:   0.1225 Training Acc:    99.01  6763.85    99.47%\n",
      "Epoch   3137 batch_limit:   8677 Loss:   0.1239 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3138 batch_limit:   8677 Loss:   0.1224 Training Acc:    99.01  6763.07    99.46%\n",
      "Epoch   3139 batch_limit:   8677 Loss:   0.1237 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3140 batch_limit:   8677 Loss:   0.1223 Training Acc:    99.01  6763.07    99.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3141 batch_limit:   8677 Loss:   0.1236 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3142 batch_limit:   8677 Loss:   0.1221 Training Acc:    99.01  6763.07    99.46%\n",
      "Epoch   3143 batch_limit:   8677 Loss:   0.1235 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3144 batch_limit:   8677 Loss:   0.1220 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3145 batch_limit:   8677 Loss:   0.1233 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3146 batch_limit:   8677 Loss:   0.1218 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3147 batch_limit:   8677 Loss:   0.1232 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3148 batch_limit:   8677 Loss:   0.1217 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3149 batch_limit:   8677 Loss:   0.1231 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3150 batch_limit:   8677 Loss:   0.1216 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3151 batch_limit:   8677 Loss:   0.1229 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3152 batch_limit:   8677 Loss:   0.1214 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3153 batch_limit:   8677 Loss:   0.1228 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3154 batch_limit:   8677 Loss:   0.1213 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3155 batch_limit:   8677 Loss:   0.1227 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3156 batch_limit:   8677 Loss:   0.1211 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3157 batch_limit:   8677 Loss:   0.1226 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3158 batch_limit:   8677 Loss:   0.1210 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3159 batch_limit:   8677 Loss:   0.1224 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3160 batch_limit:   8677 Loss:   0.1209 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3161 batch_limit:   8677 Loss:   0.1223 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3162 batch_limit:   8677 Loss:   0.1207 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3163 batch_limit:   8677 Loss:   0.1222 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3164 batch_limit:   8677 Loss:   0.1206 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3165 batch_limit:   8677 Loss:   0.1220 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3166 batch_limit:   8677 Loss:   0.1204 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3167 batch_limit:   8677 Loss:   0.1219 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3168 batch_limit:   8677 Loss:   0.1203 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3169 batch_limit:   8677 Loss:   0.1218 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3170 batch_limit:   8677 Loss:   0.1202 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3171 batch_limit:   8677 Loss:   0.1216 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3172 batch_limit:   8677 Loss:   0.1200 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3173 batch_limit:   8677 Loss:   0.1215 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3174 batch_limit:   8677 Loss:   0.1199 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3175 batch_limit:   8677 Loss:   0.1214 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3176 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3177 batch_limit:   8677 Loss:   0.1213 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3178 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3179 batch_limit:   8677 Loss:   0.1211 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3180 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3181 batch_limit:   8677 Loss:   0.1210 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3182 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3183 batch_limit:   8677 Loss:   0.1209 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3184 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3185 batch_limit:   8677 Loss:   0.1207 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3186 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3187 batch_limit:   8677 Loss:   0.1206 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3188 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3189 batch_limit:   8677 Loss:   0.1205 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3190 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3191 batch_limit:   8677 Loss:   0.1204 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3192 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3193 batch_limit:   8677 Loss:   0.1202 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3194 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3195 batch_limit:   8677 Loss:   0.1201 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3196 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3197 batch_limit:   8677 Loss:   0.1200 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3198 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3199 batch_limit:   8677 Loss:   0.1199 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3200 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3201 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3202 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3203 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3204 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3205 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3206 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3207 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3208 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3209 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3210 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3211 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3212 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3213 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3214 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3215 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3216 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3217 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3218 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3219 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3220 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3221 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3222 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3223 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3224 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3225 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3226 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3227 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3228 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3229 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3230 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3231 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3232 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6763.28    99.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3233 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3234 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3235 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3236 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3237 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3238 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3239 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3240 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3241 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3242 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3243 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3244 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3245 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3246 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3247 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3248 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3249 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3250 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3251 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3252 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3253 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3254 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3255 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3256 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3257 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3258 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3259 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3260 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3261 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3262 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3263 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3264 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3265 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3266 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3267 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3268 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3269 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3270 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3271 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3272 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3273 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3274 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3275 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3276 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3277 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3278 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3279 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3280 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3281 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3282 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3283 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3284 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3285 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3286 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3287 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3288 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3289 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3290 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3291 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3292 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3293 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3294 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3295 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3296 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3297 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3298 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3299 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3300 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3301 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3302 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3303 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3304 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3305 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3306 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3307 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3308 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3309 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3310 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3311 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3312 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3313 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3314 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3315 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3316 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3317 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3318 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3319 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3320 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3321 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3322 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3323 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3324 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3325 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6763.28    99.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3326 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3327 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3328 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3329 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3330 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3331 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3332 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3333 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3334 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3335 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3336 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3337 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3338 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3339 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3340 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3341 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3342 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3343 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3344 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3345 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3346 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3347 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3348 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3349 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3350 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3351 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3352 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3353 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3354 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3355 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3356 batch_limit:   8677 Loss:   0.1091 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3357 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3358 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3359 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3360 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3361 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3362 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3363 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3364 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3365 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3366 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3367 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3368 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3369 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3370 batch_limit:   8677 Loss:   0.1084 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3371 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3372 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3373 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3374 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3375 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3376 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3377 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3378 batch_limit:   8677 Loss:   0.1080 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3379 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3380 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3381 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3382 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3383 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3384 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3385 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3386 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3387 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3388 batch_limit:   8677 Loss:   0.1075 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3389 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3390 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3391 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3392 batch_limit:   8677 Loss:   0.1073 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3393 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3394 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3395 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3396 batch_limit:   8677 Loss:   0.1071 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3397 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3398 batch_limit:   8677 Loss:   0.1070 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3399 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3400 batch_limit:   8677 Loss:   0.1069 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3401 batch_limit:   8677 Loss:   0.1091 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3402 batch_limit:   8677 Loss:   0.1068 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3403 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3404 batch_limit:   8677 Loss:   0.1068 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3405 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3406 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3407 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3408 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3409 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3410 batch_limit:   8677 Loss:   0.1065 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3411 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3412 batch_limit:   8677 Loss:   0.1064 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3413 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3414 batch_limit:   8677 Loss:   0.1063 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3415 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3416 batch_limit:   8677 Loss:   0.1062 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3417 batch_limit:   8677 Loss:   0.1084 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3418 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6761.72    99.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3419 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3420 batch_limit:   8677 Loss:   0.1060 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3421 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3422 batch_limit:   8677 Loss:   0.1059 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3423 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3424 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3425 batch_limit:   8677 Loss:   0.1080 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3426 batch_limit:   8677 Loss:   0.1057 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3427 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3428 batch_limit:   8677 Loss:   0.1056 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3429 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3430 batch_limit:   8677 Loss:   0.1055 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3431 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3432 batch_limit:   8677 Loss:   0.1054 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3433 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3434 batch_limit:   8677 Loss:   0.1054 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3435 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3436 batch_limit:   8677 Loss:   0.1053 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3437 batch_limit:   8677 Loss:   0.1075 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3438 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3439 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3440 batch_limit:   8677 Loss:   0.1051 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3441 batch_limit:   8677 Loss:   0.1073 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3442 batch_limit:   8677 Loss:   0.1050 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3443 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3444 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3445 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3446 batch_limit:   8677 Loss:   0.1048 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3447 batch_limit:   8677 Loss:   0.1071 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3448 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3449 batch_limit:   8677 Loss:   0.1070 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3450 batch_limit:   8677 Loss:   0.1046 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3451 batch_limit:   8677 Loss:   0.1069 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3452 batch_limit:   8677 Loss:   0.1045 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3453 batch_limit:   8677 Loss:   0.1068 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3454 batch_limit:   8677 Loss:   0.1044 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3455 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3456 batch_limit:   8677 Loss:   0.1044 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3457 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3458 batch_limit:   8677 Loss:   0.1043 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3459 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3460 batch_limit:   8677 Loss:   0.1042 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3461 batch_limit:   8677 Loss:   0.1065 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3462 batch_limit:   8677 Loss:   0.1041 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3463 batch_limit:   8677 Loss:   0.1064 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3464 batch_limit:   8677 Loss:   0.1040 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3465 batch_limit:   8677 Loss:   0.1063 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3466 batch_limit:   8677 Loss:   0.1039 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3467 batch_limit:   8677 Loss:   0.1062 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3468 batch_limit:   8677 Loss:   0.1038 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3469 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3470 batch_limit:   8677 Loss:   0.1037 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3471 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3472 batch_limit:   8677 Loss:   0.1037 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3473 batch_limit:   8677 Loss:   0.1060 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3474 batch_limit:   8677 Loss:   0.1036 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3475 batch_limit:   8677 Loss:   0.1059 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3476 batch_limit:   8677 Loss:   0.1035 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3477 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3478 batch_limit:   8677 Loss:   0.1034 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3479 batch_limit:   8677 Loss:   0.1057 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3480 batch_limit:   8677 Loss:   0.1033 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3481 batch_limit:   8677 Loss:   0.1056 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3482 batch_limit:   8677 Loss:   0.1032 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3483 batch_limit:   8677 Loss:   0.1056 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3484 batch_limit:   8677 Loss:   0.1031 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3485 batch_limit:   8677 Loss:   0.1055 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3486 batch_limit:   8677 Loss:   0.1030 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3487 batch_limit:   8677 Loss:   0.1054 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3488 batch_limit:   8677 Loss:   0.1030 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3489 batch_limit:   8677 Loss:   0.1053 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3490 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3491 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3492 batch_limit:   8677 Loss:   0.1028 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3493 batch_limit:   8677 Loss:   0.1051 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3494 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3495 batch_limit:   8677 Loss:   0.1051 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3496 batch_limit:   8677 Loss:   0.1026 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3497 batch_limit:   8677 Loss:   0.1050 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3498 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3499 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3500 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3501 batch_limit:   8677 Loss:   0.1048 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3502 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3503 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3504 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3505 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3506 batch_limit:   8677 Loss:   0.1022 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3507 batch_limit:   8677 Loss:   0.1046 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3508 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3509 batch_limit:   8677 Loss:   0.1045 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3510 batch_limit:   8677 Loss:   0.1020 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3511 batch_limit:   8677 Loss:   0.1044 Training Acc:   100.00  6762.50    99.45%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3512 batch_limit:   8677 Loss:   0.1019 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3513 batch_limit:   8677 Loss:   0.1043 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3514 batch_limit:   8677 Loss:   0.1019 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3515 batch_limit:   8677 Loss:   0.1042 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3516 batch_limit:   8677 Loss:   0.1018 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3517 batch_limit:   8677 Loss:   0.1042 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3518 batch_limit:   8677 Loss:   0.1017 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3519 batch_limit:   8677 Loss:   0.1041 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3520 batch_limit:   8677 Loss:   0.1016 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3521 batch_limit:   8677 Loss:   0.1040 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3522 batch_limit:   8677 Loss:   0.1015 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3523 batch_limit:   8677 Loss:   0.1039 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3524 batch_limit:   8677 Loss:   0.1014 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3525 batch_limit:   8677 Loss:   0.1039 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3526 batch_limit:   8677 Loss:   0.1014 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3527 batch_limit:   8677 Loss:   0.1038 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3528 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3529 batch_limit:   8677 Loss:   0.1037 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3530 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3531 batch_limit:   8677 Loss:   0.1036 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3532 batch_limit:   8677 Loss:   0.1011 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3533 batch_limit:   8677 Loss:   0.1035 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3534 batch_limit:   8677 Loss:   0.1010 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3535 batch_limit:   8677 Loss:   0.1035 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3536 batch_limit:   8677 Loss:   0.1009 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3537 batch_limit:   8677 Loss:   0.1034 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3538 batch_limit:   8677 Loss:   0.1009 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3539 batch_limit:   8677 Loss:   0.1033 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3540 batch_limit:   8677 Loss:   0.1008 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3541 batch_limit:   8677 Loss:   0.1032 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3542 batch_limit:   8677 Loss:   0.1007 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3543 batch_limit:   8677 Loss:   0.1031 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3544 batch_limit:   8677 Loss:   0.1006 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3545 batch_limit:   8677 Loss:   0.1031 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3546 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3547 batch_limit:   8677 Loss:   0.1030 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3548 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3549 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3550 batch_limit:   8677 Loss:   0.1004 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3551 batch_limit:   8677 Loss:   0.1028 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3552 batch_limit:   8677 Loss:   0.1003 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3553 batch_limit:   8677 Loss:   0.1028 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3554 batch_limit:   8677 Loss:   0.1002 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3555 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3556 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3557 batch_limit:   8677 Loss:   0.1026 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3558 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3559 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3560 batch_limit:   8677 Loss:   0.1000 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3561 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3562 batch_limit:   8677 Loss:   0.0999 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3563 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3564 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3565 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3566 batch_limit:   8677 Loss:   0.0997 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3567 batch_limit:   8677 Loss:   0.1022 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3568 batch_limit:   8677 Loss:   0.0997 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3569 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3570 batch_limit:   8677 Loss:   0.0996 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3571 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3572 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3573 batch_limit:   8677 Loss:   0.1020 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3574 batch_limit:   8677 Loss:   0.0994 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3575 batch_limit:   8677 Loss:   0.1019 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3576 batch_limit:   8677 Loss:   0.0993 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3577 batch_limit:   8677 Loss:   0.1018 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3578 batch_limit:   8677 Loss:   0.0993 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3579 batch_limit:   8677 Loss:   0.1018 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3580 batch_limit:   8677 Loss:   0.0992 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3581 batch_limit:   8677 Loss:   0.1017 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3582 batch_limit:   8677 Loss:   0.0991 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3583 batch_limit:   8677 Loss:   0.1016 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3584 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3585 batch_limit:   8677 Loss:   0.1015 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3586 batch_limit:   8677 Loss:   0.0989 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3587 batch_limit:   8677 Loss:   0.1015 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3588 batch_limit:   8677 Loss:   0.0989 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3589 batch_limit:   8677 Loss:   0.1014 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3590 batch_limit:   8677 Loss:   0.0988 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3591 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3592 batch_limit:   8677 Loss:   0.0987 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3593 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3594 batch_limit:   8677 Loss:   0.0986 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3595 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3596 batch_limit:   8677 Loss:   0.0985 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3597 batch_limit:   8677 Loss:   0.1011 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3598 batch_limit:   8677 Loss:   0.0985 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3599 batch_limit:   8677 Loss:   0.1010 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3600 batch_limit:   8677 Loss:   0.0984 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3601 batch_limit:   8677 Loss:   0.1009 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3602 batch_limit:   8677 Loss:   0.0983 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3603 batch_limit:   8677 Loss:   0.1009 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3604 batch_limit:   8677 Loss:   0.0982 Training Acc:   100.00  6761.72    99.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3605 batch_limit:   8677 Loss:   0.1008 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3606 batch_limit:   8677 Loss:   0.0982 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3607 batch_limit:   8677 Loss:   0.1007 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3608 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3609 batch_limit:   8677 Loss:   0.1006 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3610 batch_limit:   8677 Loss:   0.0980 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3611 batch_limit:   8677 Loss:   0.1006 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3612 batch_limit:   8677 Loss:   0.0979 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3613 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3614 batch_limit:   8677 Loss:   0.0979 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3615 batch_limit:   8677 Loss:   0.1004 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3616 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3617 batch_limit:   8677 Loss:   0.1003 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3618 batch_limit:   8677 Loss:   0.0977 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3619 batch_limit:   8677 Loss:   0.1003 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3620 batch_limit:   8677 Loss:   0.0976 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3621 batch_limit:   8677 Loss:   0.1002 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3622 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3623 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3624 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3625 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3626 batch_limit:   8677 Loss:   0.0974 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3627 batch_limit:   8677 Loss:   0.1000 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3628 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3629 batch_limit:   8677 Loss:   0.0999 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3630 batch_limit:   8677 Loss:   0.0972 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3631 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3632 batch_limit:   8677 Loss:   0.0972 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3633 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3634 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3635 batch_limit:   8677 Loss:   0.0997 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3636 batch_limit:   8677 Loss:   0.0970 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3637 batch_limit:   8677 Loss:   0.0996 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3638 batch_limit:   8677 Loss:   0.0969 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3639 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3640 batch_limit:   8677 Loss:   0.0969 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3641 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3642 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3643 batch_limit:   8677 Loss:   0.0994 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3644 batch_limit:   8677 Loss:   0.0967 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3645 batch_limit:   8677 Loss:   0.0993 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3646 batch_limit:   8677 Loss:   0.0966 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3647 batch_limit:   8677 Loss:   0.0993 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3648 batch_limit:   8677 Loss:   0.0966 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3649 batch_limit:   8677 Loss:   0.0992 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3650 batch_limit:   8677 Loss:   0.0965 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3651 batch_limit:   8677 Loss:   0.0991 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3652 batch_limit:   8677 Loss:   0.0964 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3653 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3654 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3655 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3656 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3657 batch_limit:   8677 Loss:   0.0989 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3658 batch_limit:   8677 Loss:   0.0962 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3659 batch_limit:   8677 Loss:   0.0988 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3660 batch_limit:   8677 Loss:   0.0961 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3661 batch_limit:   8677 Loss:   0.0988 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3662 batch_limit:   8677 Loss:   0.0960 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3663 batch_limit:   8677 Loss:   0.0987 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3664 batch_limit:   8677 Loss:   0.0960 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3665 batch_limit:   8677 Loss:   0.0986 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3666 batch_limit:   8677 Loss:   0.0959 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3667 batch_limit:   8677 Loss:   0.0986 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3668 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3669 batch_limit:   8677 Loss:   0.0985 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3670 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3671 batch_limit:   8677 Loss:   0.0984 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3672 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3673 batch_limit:   8677 Loss:   0.0984 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3674 batch_limit:   8677 Loss:   0.0956 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3675 batch_limit:   8677 Loss:   0.0983 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3676 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3677 batch_limit:   8677 Loss:   0.0982 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3678 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3679 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3680 batch_limit:   8677 Loss:   0.0954 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3681 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3682 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3683 batch_limit:   8677 Loss:   0.0980 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3684 batch_limit:   8677 Loss:   0.0952 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3685 batch_limit:   8677 Loss:   0.0979 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3686 batch_limit:   8677 Loss:   0.0952 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3687 batch_limit:   8677 Loss:   0.0979 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3688 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6761.72    99.44%\n",
      "Epoch   3689 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3690 batch_limit:   8677 Loss:   0.0950 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3691 batch_limit:   8677 Loss:   0.0977 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3692 batch_limit:   8677 Loss:   0.0950 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3693 batch_limit:   8677 Loss:   0.0977 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3694 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3695 batch_limit:   8677 Loss:   0.0976 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3696 batch_limit:   8677 Loss:   0.0948 Training Acc:   100.00  6762.50    99.45%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3697 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3698 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3699 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3700 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3701 batch_limit:   8677 Loss:   0.0974 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3702 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3703 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3704 batch_limit:   8677 Loss:   0.0945 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3705 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3706 batch_limit:   8677 Loss:   0.0945 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3707 batch_limit:   8677 Loss:   0.0972 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3708 batch_limit:   8677 Loss:   0.0944 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3709 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3710 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3711 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3712 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3713 batch_limit:   8677 Loss:   0.0970 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3714 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3715 batch_limit:   8677 Loss:   0.0969 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3716 batch_limit:   8677 Loss:   0.0941 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3717 batch_limit:   8677 Loss:   0.0969 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3718 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3719 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3720 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3721 batch_limit:   8677 Loss:   0.0967 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3722 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3723 batch_limit:   8677 Loss:   0.0967 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3724 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3725 batch_limit:   8677 Loss:   0.0966 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3726 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3727 batch_limit:   8677 Loss:   0.0965 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3728 batch_limit:   8677 Loss:   0.0937 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3729 batch_limit:   8677 Loss:   0.0965 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3730 batch_limit:   8677 Loss:   0.0936 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3731 batch_limit:   8677 Loss:   0.0964 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3732 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3733 batch_limit:   8677 Loss:   0.0964 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3734 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3735 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3736 batch_limit:   8677 Loss:   0.0934 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3737 batch_limit:   8677 Loss:   0.0962 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3738 batch_limit:   8677 Loss:   0.0933 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3739 batch_limit:   8677 Loss:   0.0962 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3740 batch_limit:   8677 Loss:   0.0933 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3741 batch_limit:   8677 Loss:   0.0961 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3742 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3743 batch_limit:   8677 Loss:   0.0960 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3744 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3745 batch_limit:   8677 Loss:   0.0960 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3746 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3747 batch_limit:   8677 Loss:   0.0959 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3748 batch_limit:   8677 Loss:   0.0930 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3749 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3750 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3751 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3752 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3753 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3754 batch_limit:   8677 Loss:   0.0928 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3755 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3756 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3757 batch_limit:   8677 Loss:   0.0956 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3758 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3759 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3760 batch_limit:   8677 Loss:   0.0926 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3761 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3762 batch_limit:   8677 Loss:   0.0925 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3763 batch_limit:   8677 Loss:   0.0954 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3764 batch_limit:   8677 Loss:   0.0924 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3765 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3766 batch_limit:   8677 Loss:   0.0924 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3767 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3768 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3769 batch_limit:   8677 Loss:   0.0952 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3770 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3771 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3772 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3773 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3774 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3775 batch_limit:   8677 Loss:   0.0950 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3776 batch_limit:   8677 Loss:   0.0920 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3777 batch_limit:   8677 Loss:   0.0950 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3778 batch_limit:   8677 Loss:   0.0920 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3779 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3780 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3781 batch_limit:   8677 Loss:   0.0948 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3782 batch_limit:   8677 Loss:   0.0918 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3783 batch_limit:   8677 Loss:   0.0948 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3784 batch_limit:   8677 Loss:   0.0917 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3785 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3786 batch_limit:   8677 Loss:   0.0917 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3787 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3788 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6762.50    99.45%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3789 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3790 batch_limit:   8677 Loss:   0.0915 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3791 batch_limit:   8677 Loss:   0.0945 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3792 batch_limit:   8677 Loss:   0.0915 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3793 batch_limit:   8677 Loss:   0.0944 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3794 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3795 batch_limit:   8677 Loss:   0.0944 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3796 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3797 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3798 batch_limit:   8677 Loss:   0.0912 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3799 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3800 batch_limit:   8677 Loss:   0.0912 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3801 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3802 batch_limit:   8677 Loss:   0.0911 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3803 batch_limit:   8677 Loss:   0.0941 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3804 batch_limit:   8677 Loss:   0.0910 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3805 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3806 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3807 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3808 batch_limit:   8677 Loss:   0.0908 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3809 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3810 batch_limit:   8677 Loss:   0.0908 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3811 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3812 batch_limit:   8677 Loss:   0.0907 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3813 batch_limit:   8677 Loss:   0.0937 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3814 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3815 batch_limit:   8677 Loss:   0.0937 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3816 batch_limit:   8677 Loss:   0.0905 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3817 batch_limit:   8677 Loss:   0.0936 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3818 batch_limit:   8677 Loss:   0.0904 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3819 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3820 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3821 batch_limit:   8677 Loss:   0.0934 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3822 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3823 batch_limit:   8677 Loss:   0.0933 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3824 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3825 batch_limit:   8677 Loss:   0.0933 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3826 batch_limit:   8677 Loss:   0.0901 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3827 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3828 batch_limit:   8677 Loss:   0.0900 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3829 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3830 batch_limit:   8677 Loss:   0.0899 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3831 batch_limit:   8677 Loss:   0.0930 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3832 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6762.50    99.45%\n",
      "Epoch   3833 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3834 batch_limit:   8677 Loss:   0.0897 Training Acc:   100.00  6763.28    99.46%\n",
      "Epoch   3835 batch_limit:   8677 Loss:   0.0928 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3836 batch_limit:   8677 Loss:   0.0896 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3837 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3838 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3839 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3840 batch_limit:   8677 Loss:   0.0894 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3841 batch_limit:   8677 Loss:   0.0926 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3842 batch_limit:   8677 Loss:   0.0893 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3843 batch_limit:   8677 Loss:   0.0925 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3844 batch_limit:   8677 Loss:   0.0892 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3845 batch_limit:   8677 Loss:   0.0924 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3846 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3847 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3848 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3849 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3850 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3851 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3852 batch_limit:   8677 Loss:   0.0888 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3853 batch_limit:   8677 Loss:   0.0920 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3854 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3855 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3856 batch_limit:   8677 Loss:   0.0886 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3857 batch_limit:   8677 Loss:   0.0918 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3858 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3859 batch_limit:   8677 Loss:   0.0917 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3860 batch_limit:   8677 Loss:   0.0884 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3861 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3862 batch_limit:   8677 Loss:   0.0883 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3863 batch_limit:   8677 Loss:   0.0915 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3864 batch_limit:   8677 Loss:   0.0882 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3865 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3866 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3867 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3868 batch_limit:   8677 Loss:   0.0880 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3869 batch_limit:   8677 Loss:   0.0912 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3870 batch_limit:   8677 Loss:   0.0879 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3871 batch_limit:   8677 Loss:   0.0911 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3872 batch_limit:   8677 Loss:   0.0878 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3873 batch_limit:   8677 Loss:   0.0910 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3874 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3875 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3876 batch_limit:   8677 Loss:   0.0876 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3877 batch_limit:   8677 Loss:   0.0908 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3878 batch_limit:   8677 Loss:   0.0875 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3879 batch_limit:   8677 Loss:   0.0907 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3880 batch_limit:   8677 Loss:   0.0874 Training Acc:   100.00  6764.06    99.47%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3881 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3882 batch_limit:   8677 Loss:   0.0873 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3883 batch_limit:   8677 Loss:   0.0905 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3884 batch_limit:   8677 Loss:   0.0872 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3885 batch_limit:   8677 Loss:   0.0904 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3886 batch_limit:   8677 Loss:   0.0871 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3887 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3888 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3889 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3890 batch_limit:   8677 Loss:   0.0869 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3891 batch_limit:   8677 Loss:   0.0901 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3892 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3893 batch_limit:   8677 Loss:   0.0900 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3894 batch_limit:   8677 Loss:   0.0867 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3895 batch_limit:   8677 Loss:   0.0899 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3896 batch_limit:   8677 Loss:   0.0866 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3897 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3898 batch_limit:   8677 Loss:   0.0865 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3899 batch_limit:   8677 Loss:   0.0897 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3900 batch_limit:   8677 Loss:   0.0864 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3901 batch_limit:   8677 Loss:   0.0896 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3902 batch_limit:   8677 Loss:   0.0863 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3903 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3904 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3905 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3906 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3907 batch_limit:   8677 Loss:   0.0894 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3908 batch_limit:   8677 Loss:   0.0860 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3909 batch_limit:   8677 Loss:   0.0893 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3910 batch_limit:   8677 Loss:   0.0860 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3911 batch_limit:   8677 Loss:   0.0892 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3912 batch_limit:   8677 Loss:   0.0859 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3913 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   3914 batch_limit:   8677 Loss:   0.0858 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3915 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3916 batch_limit:   8677 Loss:   0.0857 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3917 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3918 batch_limit:   8677 Loss:   0.0856 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3919 batch_limit:   8677 Loss:   0.0888 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3920 batch_limit:   8677 Loss:   0.0855 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3921 batch_limit:   8677 Loss:   0.0888 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3922 batch_limit:   8677 Loss:   0.0854 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3923 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3924 batch_limit:   8677 Loss:   0.0853 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3925 batch_limit:   8677 Loss:   0.0886 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3926 batch_limit:   8677 Loss:   0.0852 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3927 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3928 batch_limit:   8677 Loss:   0.0852 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3929 batch_limit:   8677 Loss:   0.0884 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3930 batch_limit:   8677 Loss:   0.0851 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3931 batch_limit:   8677 Loss:   0.0883 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3932 batch_limit:   8677 Loss:   0.0850 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3933 batch_limit:   8677 Loss:   0.0883 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3934 batch_limit:   8677 Loss:   0.0849 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3935 batch_limit:   8677 Loss:   0.0882 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3936 batch_limit:   8677 Loss:   0.0848 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3937 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3938 batch_limit:   8677 Loss:   0.0847 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3939 batch_limit:   8677 Loss:   0.0880 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3940 batch_limit:   8677 Loss:   0.0847 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3941 batch_limit:   8677 Loss:   0.0879 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3942 batch_limit:   8677 Loss:   0.0846 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3943 batch_limit:   8677 Loss:   0.0879 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3944 batch_limit:   8677 Loss:   0.0845 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3945 batch_limit:   8677 Loss:   0.0878 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3946 batch_limit:   8677 Loss:   0.0844 Training Acc:   100.00  6764.06    99.47%\n",
      "Epoch   3947 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3948 batch_limit:   8677 Loss:   0.0843 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3949 batch_limit:   8677 Loss:   0.0876 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3950 batch_limit:   8677 Loss:   0.0843 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3951 batch_limit:   8677 Loss:   0.0876 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3952 batch_limit:   8677 Loss:   0.0842 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3953 batch_limit:   8677 Loss:   0.0875 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3954 batch_limit:   8677 Loss:   0.0841 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3955 batch_limit:   8677 Loss:   0.0874 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3956 batch_limit:   8677 Loss:   0.0840 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3957 batch_limit:   8677 Loss:   0.0873 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3958 batch_limit:   8677 Loss:   0.0840 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3959 batch_limit:   8677 Loss:   0.0873 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3960 batch_limit:   8677 Loss:   0.0839 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3961 batch_limit:   8677 Loss:   0.0872 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3962 batch_limit:   8677 Loss:   0.0838 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3963 batch_limit:   8677 Loss:   0.0871 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3964 batch_limit:   8677 Loss:   0.0837 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3965 batch_limit:   8677 Loss:   0.0871 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3966 batch_limit:   8677 Loss:   0.0837 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3967 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3968 batch_limit:   8677 Loss:   0.0836 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3969 batch_limit:   8677 Loss:   0.0869 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3970 batch_limit:   8677 Loss:   0.0835 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3971 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3972 batch_limit:   8677 Loss:   0.0835 Training Acc:   100.00  6764.84    99.48%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3973 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3974 batch_limit:   8677 Loss:   0.0834 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3975 batch_limit:   8677 Loss:   0.0867 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3976 batch_limit:   8677 Loss:   0.0833 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3977 batch_limit:   8677 Loss:   0.0866 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3978 batch_limit:   8677 Loss:   0.0833 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3979 batch_limit:   8677 Loss:   0.0866 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3980 batch_limit:   8677 Loss:   0.0832 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3981 batch_limit:   8677 Loss:   0.0865 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3982 batch_limit:   8677 Loss:   0.0831 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3983 batch_limit:   8677 Loss:   0.0865 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3984 batch_limit:   8677 Loss:   0.0831 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3985 batch_limit:   8677 Loss:   0.0864 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3986 batch_limit:   8677 Loss:   0.0830 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3987 batch_limit:   8677 Loss:   0.0863 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3988 batch_limit:   8677 Loss:   0.0829 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3989 batch_limit:   8677 Loss:   0.0863 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3990 batch_limit:   8677 Loss:   0.0829 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3991 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3992 batch_limit:   8677 Loss:   0.0828 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3993 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3994 batch_limit:   8677 Loss:   0.0827 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3995 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3996 batch_limit:   8677 Loss:   0.0827 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3997 batch_limit:   8677 Loss:   0.0860 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   3998 batch_limit:   8677 Loss:   0.0826 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   3999 batch_limit:   8677 Loss:   0.0860 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4000 batch_limit:   8677 Loss:   0.0826 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4001 batch_limit:   8677 Loss:   0.0859 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4002 batch_limit:   8677 Loss:   0.0825 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4003 batch_limit:   8677 Loss:   0.0859 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4004 batch_limit:   8677 Loss:   0.0824 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4005 batch_limit:   8677 Loss:   0.0858 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4006 batch_limit:   8677 Loss:   0.0824 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4007 batch_limit:   8677 Loss:   0.0857 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4008 batch_limit:   8677 Loss:   0.0823 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4009 batch_limit:   8677 Loss:   0.0857 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4010 batch_limit:   8677 Loss:   0.0823 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4011 batch_limit:   8677 Loss:   0.0856 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4012 batch_limit:   8677 Loss:   0.0822 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4013 batch_limit:   8677 Loss:   0.0856 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4014 batch_limit:   8677 Loss:   0.0821 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4015 batch_limit:   8677 Loss:   0.0855 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4016 batch_limit:   8677 Loss:   0.0821 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4017 batch_limit:   8677 Loss:   0.0855 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4018 batch_limit:   8677 Loss:   0.0820 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4019 batch_limit:   8677 Loss:   0.0854 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4020 batch_limit:   8677 Loss:   0.0820 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4021 batch_limit:   8677 Loss:   0.0854 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4022 batch_limit:   8677 Loss:   0.0819 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4023 batch_limit:   8677 Loss:   0.0853 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4024 batch_limit:   8677 Loss:   0.0819 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4025 batch_limit:   8677 Loss:   0.0853 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4026 batch_limit:   8677 Loss:   0.0818 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4027 batch_limit:   8677 Loss:   0.0852 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4028 batch_limit:   8677 Loss:   0.0818 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4029 batch_limit:   8677 Loss:   0.0852 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4030 batch_limit:   8677 Loss:   0.0817 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4031 batch_limit:   8677 Loss:   0.0851 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4032 batch_limit:   8677 Loss:   0.0817 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4033 batch_limit:   8677 Loss:   0.0851 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4034 batch_limit:   8677 Loss:   0.0816 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4035 batch_limit:   8677 Loss:   0.0850 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4036 batch_limit:   8677 Loss:   0.0816 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4037 batch_limit:   8677 Loss:   0.0850 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4038 batch_limit:   8677 Loss:   0.0815 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4039 batch_limit:   8677 Loss:   0.0849 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4040 batch_limit:   8677 Loss:   0.0815 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4041 batch_limit:   8677 Loss:   0.0849 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4042 batch_limit:   8677 Loss:   0.0814 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4043 batch_limit:   8677 Loss:   0.0848 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4044 batch_limit:   8677 Loss:   0.0814 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4045 batch_limit:   8677 Loss:   0.0848 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4046 batch_limit:   8677 Loss:   0.0813 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4047 batch_limit:   8677 Loss:   0.0847 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4048 batch_limit:   8677 Loss:   0.0813 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4049 batch_limit:   8677 Loss:   0.0847 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4050 batch_limit:   8677 Loss:   0.0812 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4051 batch_limit:   8677 Loss:   0.0846 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4052 batch_limit:   8677 Loss:   0.0812 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4053 batch_limit:   8677 Loss:   0.0846 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4054 batch_limit:   8677 Loss:   0.0811 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4055 batch_limit:   8677 Loss:   0.0845 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4056 batch_limit:   8677 Loss:   0.0811 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4057 batch_limit:   8677 Loss:   0.0845 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4058 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4059 batch_limit:   8677 Loss:   0.0845 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4060 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4061 batch_limit:   8677 Loss:   0.0844 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4062 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4063 batch_limit:   8677 Loss:   0.0844 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4064 batch_limit:   8677 Loss:   0.0809 Training Acc:   100.00  6764.84    99.48%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4065 batch_limit:   8677 Loss:   0.0843 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4066 batch_limit:   8677 Loss:   0.0809 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4067 batch_limit:   8677 Loss:   0.0843 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4068 batch_limit:   8677 Loss:   0.0808 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4069 batch_limit:   8677 Loss:   0.0842 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4070 batch_limit:   8677 Loss:   0.0808 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4071 batch_limit:   8677 Loss:   0.0842 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4072 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4073 batch_limit:   8677 Loss:   0.0842 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4074 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4075 batch_limit:   8677 Loss:   0.0841 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4076 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4077 batch_limit:   8677 Loss:   0.0841 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4078 batch_limit:   8677 Loss:   0.0806 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4079 batch_limit:   8677 Loss:   0.0840 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4080 batch_limit:   8677 Loss:   0.0806 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4081 batch_limit:   8677 Loss:   0.0840 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4082 batch_limit:   8677 Loss:   0.0805 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4083 batch_limit:   8677 Loss:   0.0840 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4084 batch_limit:   8677 Loss:   0.0805 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4085 batch_limit:   8677 Loss:   0.0839 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4086 batch_limit:   8677 Loss:   0.0805 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4087 batch_limit:   8677 Loss:   0.0839 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4088 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4089 batch_limit:   8677 Loss:   0.0839 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4090 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4091 batch_limit:   8677 Loss:   0.0838 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4092 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4093 batch_limit:   8677 Loss:   0.0838 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4094 batch_limit:   8677 Loss:   0.0803 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4095 batch_limit:   8677 Loss:   0.0837 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4096 batch_limit:   8677 Loss:   0.0803 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4097 batch_limit:   8677 Loss:   0.0837 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4098 batch_limit:   8677 Loss:   0.0802 Training Acc:   100.00  6764.84    99.48%\n",
      "Epoch   4099 batch_limit:   8677 Loss:   0.0837 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4100 batch_limit:   8677 Loss:   0.0802 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4101 batch_limit:   8677 Loss:   0.0836 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4102 batch_limit:   8677 Loss:   0.0802 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4103 batch_limit:   8677 Loss:   0.0836 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4104 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4105 batch_limit:   8677 Loss:   0.0836 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4106 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4107 batch_limit:   8677 Loss:   0.0835 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4108 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4109 batch_limit:   8677 Loss:   0.0835 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4110 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4111 batch_limit:   8677 Loss:   0.0834 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4112 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4113 batch_limit:   8677 Loss:   0.0834 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4114 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4115 batch_limit:   8677 Loss:   0.0834 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4116 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4117 batch_limit:   8677 Loss:   0.0833 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4118 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4119 batch_limit:   8677 Loss:   0.0833 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4120 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4121 batch_limit:   8677 Loss:   0.0833 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4122 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4123 batch_limit:   8677 Loss:   0.0832 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4124 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4125 batch_limit:   8677 Loss:   0.0832 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4126 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4127 batch_limit:   8677 Loss:   0.0832 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4128 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4129 batch_limit:   8677 Loss:   0.0831 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4130 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6765.62    99.49%\n",
      "Epoch   4131 batch_limit:   8677 Loss:   0.0831 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4132 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4133 batch_limit:   8677 Loss:   0.0830 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4134 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4135 batch_limit:   8677 Loss:   0.0830 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4136 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4137 batch_limit:   8677 Loss:   0.0830 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4138 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4139 batch_limit:   8677 Loss:   0.0829 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4140 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4141 batch_limit:   8677 Loss:   0.0829 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4142 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4143 batch_limit:   8677 Loss:   0.0829 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4144 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4145 batch_limit:   8677 Loss:   0.0828 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4146 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4147 batch_limit:   8677 Loss:   0.0828 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4148 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4149 batch_limit:   8677 Loss:   0.0827 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4150 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4151 batch_limit:   8677 Loss:   0.0827 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4152 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4153 batch_limit:   8677 Loss:   0.0827 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4154 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4155 batch_limit:   8677 Loss:   0.0826 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4156 batch_limit:   8677 Loss:   0.0792 Training Acc:   100.00  6766.41    99.51%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4157 batch_limit:   8677 Loss:   0.0826 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4158 batch_limit:   8677 Loss:   0.0792 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4159 batch_limit:   8677 Loss:   0.0825 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4160 batch_limit:   8677 Loss:   0.0791 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4161 batch_limit:   8677 Loss:   0.0825 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4162 batch_limit:   8677 Loss:   0.0791 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4163 batch_limit:   8677 Loss:   0.0825 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4164 batch_limit:   8677 Loss:   0.0791 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4165 batch_limit:   8677 Loss:   0.0824 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4166 batch_limit:   8677 Loss:   0.0790 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4167 batch_limit:   8677 Loss:   0.0824 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4168 batch_limit:   8677 Loss:   0.0790 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4169 batch_limit:   8677 Loss:   0.0824 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4170 batch_limit:   8677 Loss:   0.0790 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4171 batch_limit:   8677 Loss:   0.0823 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4172 batch_limit:   8677 Loss:   0.0789 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4173 batch_limit:   8677 Loss:   0.0823 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4174 batch_limit:   8677 Loss:   0.0789 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4175 batch_limit:   8677 Loss:   0.0822 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4176 batch_limit:   8677 Loss:   0.0788 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4177 batch_limit:   8677 Loss:   0.0822 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4178 batch_limit:   8677 Loss:   0.0788 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4179 batch_limit:   8677 Loss:   0.0821 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4180 batch_limit:   8677 Loss:   0.0788 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4181 batch_limit:   8677 Loss:   0.0821 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4182 batch_limit:   8677 Loss:   0.0787 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4183 batch_limit:   8677 Loss:   0.0821 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4184 batch_limit:   8677 Loss:   0.0787 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4185 batch_limit:   8677 Loss:   0.0820 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4186 batch_limit:   8677 Loss:   0.0786 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4187 batch_limit:   8677 Loss:   0.0820 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4188 batch_limit:   8677 Loss:   0.0786 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4189 batch_limit:   8677 Loss:   0.0819 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4190 batch_limit:   8677 Loss:   0.0786 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4191 batch_limit:   8677 Loss:   0.0819 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4192 batch_limit:   8677 Loss:   0.0785 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4193 batch_limit:   8677 Loss:   0.0819 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4194 batch_limit:   8677 Loss:   0.0785 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4195 batch_limit:   8677 Loss:   0.0818 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4196 batch_limit:   8677 Loss:   0.0785 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4197 batch_limit:   8677 Loss:   0.0818 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4198 batch_limit:   8677 Loss:   0.0784 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4199 batch_limit:   8677 Loss:   0.0817 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4200 batch_limit:   8677 Loss:   0.0784 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4201 batch_limit:   8677 Loss:   0.0817 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4202 batch_limit:   8677 Loss:   0.0783 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4203 batch_limit:   8677 Loss:   0.0817 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4204 batch_limit:   8677 Loss:   0.0783 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4205 batch_limit:   8677 Loss:   0.0816 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4206 batch_limit:   8677 Loss:   0.0783 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4207 batch_limit:   8677 Loss:   0.0816 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4208 batch_limit:   8677 Loss:   0.0782 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4209 batch_limit:   8677 Loss:   0.0815 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4210 batch_limit:   8677 Loss:   0.0782 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4211 batch_limit:   8677 Loss:   0.0815 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4212 batch_limit:   8677 Loss:   0.0781 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4213 batch_limit:   8677 Loss:   0.0815 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4214 batch_limit:   8677 Loss:   0.0781 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4215 batch_limit:   8677 Loss:   0.0814 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4216 batch_limit:   8677 Loss:   0.0781 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4217 batch_limit:   8677 Loss:   0.0814 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4218 batch_limit:   8677 Loss:   0.0780 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4219 batch_limit:   8677 Loss:   0.0813 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4220 batch_limit:   8677 Loss:   0.0780 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4221 batch_limit:   8677 Loss:   0.0813 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4222 batch_limit:   8677 Loss:   0.0780 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4223 batch_limit:   8677 Loss:   0.0813 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4224 batch_limit:   8677 Loss:   0.0779 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4225 batch_limit:   8677 Loss:   0.0812 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4226 batch_limit:   8677 Loss:   0.0779 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4227 batch_limit:   8677 Loss:   0.0812 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4228 batch_limit:   8677 Loss:   0.0778 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4229 batch_limit:   8677 Loss:   0.0811 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4230 batch_limit:   8677 Loss:   0.0778 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4231 batch_limit:   8677 Loss:   0.0811 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4232 batch_limit:   8677 Loss:   0.0778 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4233 batch_limit:   8677 Loss:   0.0811 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4234 batch_limit:   8677 Loss:   0.0777 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4235 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4236 batch_limit:   8677 Loss:   0.0777 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4237 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4238 batch_limit:   8677 Loss:   0.0777 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4239 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4240 batch_limit:   8677 Loss:   0.0776 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4241 batch_limit:   8677 Loss:   0.0809 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4242 batch_limit:   8677 Loss:   0.0776 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4243 batch_limit:   8677 Loss:   0.0809 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4244 batch_limit:   8677 Loss:   0.0776 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4245 batch_limit:   8677 Loss:   0.0808 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4246 batch_limit:   8677 Loss:   0.0775 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4247 batch_limit:   8677 Loss:   0.0808 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4248 batch_limit:   8677 Loss:   0.0775 Training Acc:   100.00  6766.41    99.51%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4249 batch_limit:   8677 Loss:   0.0808 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4250 batch_limit:   8677 Loss:   0.0775 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4251 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4252 batch_limit:   8677 Loss:   0.0774 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4253 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4254 batch_limit:   8677 Loss:   0.0774 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4255 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4256 batch_limit:   8677 Loss:   0.0774 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4257 batch_limit:   8677 Loss:   0.0806 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4258 batch_limit:   8677 Loss:   0.0773 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4259 batch_limit:   8677 Loss:   0.0806 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4260 batch_limit:   8677 Loss:   0.0773 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4261 batch_limit:   8677 Loss:   0.0806 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4262 batch_limit:   8677 Loss:   0.0773 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4263 batch_limit:   8677 Loss:   0.0805 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4264 batch_limit:   8677 Loss:   0.0772 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4265 batch_limit:   8677 Loss:   0.0805 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4266 batch_limit:   8677 Loss:   0.0772 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4267 batch_limit:   8677 Loss:   0.0805 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4268 batch_limit:   8677 Loss:   0.0772 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4269 batch_limit:   8677 Loss:   0.0805 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4270 batch_limit:   8677 Loss:   0.0771 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4271 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4272 batch_limit:   8677 Loss:   0.0771 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4273 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4274 batch_limit:   8677 Loss:   0.0771 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4275 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4276 batch_limit:   8677 Loss:   0.0771 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4277 batch_limit:   8677 Loss:   0.0803 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4278 batch_limit:   8677 Loss:   0.0770 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4279 batch_limit:   8677 Loss:   0.0803 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4280 batch_limit:   8677 Loss:   0.0770 Training Acc:   100.00  6766.41    99.51%\n",
      "Epoch   4281 batch_limit:   8677 Loss:   0.0803 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4282 batch_limit:   8677 Loss:   0.0770 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4283 batch_limit:   8677 Loss:   0.0803 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4284 batch_limit:   8677 Loss:   0.0770 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4285 batch_limit:   8677 Loss:   0.0802 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4286 batch_limit:   8677 Loss:   0.0769 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4287 batch_limit:   8677 Loss:   0.0802 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4288 batch_limit:   8677 Loss:   0.0769 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4289 batch_limit:   8677 Loss:   0.0802 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4290 batch_limit:   8677 Loss:   0.0769 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4291 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4292 batch_limit:   8677 Loss:   0.0769 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4293 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4294 batch_limit:   8677 Loss:   0.0768 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4295 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4296 batch_limit:   8677 Loss:   0.0768 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4297 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4298 batch_limit:   8677 Loss:   0.0768 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4299 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4300 batch_limit:   8677 Loss:   0.0768 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4301 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4302 batch_limit:   8677 Loss:   0.0767 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4303 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4304 batch_limit:   8677 Loss:   0.0767 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4305 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4306 batch_limit:   8677 Loss:   0.0767 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4307 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4308 batch_limit:   8677 Loss:   0.0767 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4309 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4310 batch_limit:   8677 Loss:   0.0766 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4311 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4312 batch_limit:   8677 Loss:   0.0766 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4313 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4314 batch_limit:   8677 Loss:   0.0766 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4315 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4316 batch_limit:   8677 Loss:   0.0766 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4317 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4318 batch_limit:   8677 Loss:   0.0766 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4319 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4320 batch_limit:   8677 Loss:   0.0765 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4321 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4322 batch_limit:   8677 Loss:   0.0765 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4323 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4324 batch_limit:   8677 Loss:   0.0765 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4325 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4326 batch_limit:   8677 Loss:   0.0765 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4327 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4328 batch_limit:   8677 Loss:   0.0765 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4329 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4330 batch_limit:   8677 Loss:   0.0765 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4331 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4332 batch_limit:   8677 Loss:   0.0764 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4333 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4334 batch_limit:   8677 Loss:   0.0764 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4335 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4336 batch_limit:   8677 Loss:   0.0764 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4337 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4338 batch_limit:   8677 Loss:   0.0764 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4339 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4340 batch_limit:   8677 Loss:   0.0764 Training Acc:   100.00  6767.19    99.52%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4341 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4342 batch_limit:   8677 Loss:   0.0764 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4343 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4344 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4345 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4346 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4347 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4348 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4349 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4350 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4351 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4352 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4353 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4354 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4355 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4356 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4357 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4358 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4359 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4360 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4361 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4362 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4363 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4364 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4365 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4366 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4367 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4368 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4369 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4370 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4371 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4372 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4373 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4374 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4375 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4376 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4377 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4378 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4379 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4380 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4381 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4382 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4383 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4384 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4385 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4386 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4387 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4388 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4389 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4390 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4391 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4392 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4393 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4394 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4395 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4396 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4397 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4398 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4399 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4400 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4401 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4402 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4403 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4404 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4405 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4406 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4407 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4408 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4409 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4410 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4411 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4412 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4413 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4414 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4415 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4416 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4417 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4418 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4419 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4420 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4421 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4422 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4423 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4424 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4425 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4426 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4427 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4428 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4429 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4430 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4431 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4432 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4433 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4434 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4435 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4436 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4437 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4438 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4439 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4440 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.19    99.52%\n",
      "Epoch   4441 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4442 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6767.97    99.53%\n",
      "Epoch   4443 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4444 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4445 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4446 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4447 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4448 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4449 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4450 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4451 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4452 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4453 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4454 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4455 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4456 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4457 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4458 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4459 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4460 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4461 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6768.75    99.54%\n",
      "Epoch   4462 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4463 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4464 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4465 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4466 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4467 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4468 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4469 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4470 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4471 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4472 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4473 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4474 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4475 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4476 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4477 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4478 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4479 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4480 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4481 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4482 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4483 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4484 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4485 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4486 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4487 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4488 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4489 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4490 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4491 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4492 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4493 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4494 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4495 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4496 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4497 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4498 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4499 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4500 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4501 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4502 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4503 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4504 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4505 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4506 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4507 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4508 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4509 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4510 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4511 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4512 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4513 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4514 batch_limit:   8677 Loss:   0.0758 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4515 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4516 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4517 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4518 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4519 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4520 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4521 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4522 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4523 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4524 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4525 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4526 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4527 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4528 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4529 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4530 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4531 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4532 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4533 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4534 batch_limit:   8677 Loss:   0.0759 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4535 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4536 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4537 batch_limit:   8677 Loss:   0.0802 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4538 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4539 batch_limit:   8677 Loss:   0.0802 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4540 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4541 batch_limit:   8677 Loss:   0.0802 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4542 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4543 batch_limit:   8677 Loss:   0.0803 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4544 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4545 batch_limit:   8677 Loss:   0.0803 Training Acc:   100.00  6770.31    99.56%\n",
      "Epoch   4546 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4547 batch_limit:   8677 Loss:   0.0803 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4548 batch_limit:   8677 Loss:   0.0760 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4549 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4550 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4551 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4552 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4553 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4554 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4555 batch_limit:   8677 Loss:   0.0805 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4556 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4557 batch_limit:   8677 Loss:   0.0805 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4558 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4559 batch_limit:   8677 Loss:   0.0806 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4560 batch_limit:   8677 Loss:   0.0761 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4561 batch_limit:   8677 Loss:   0.0806 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4562 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4563 batch_limit:   8677 Loss:   0.0806 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4564 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4565 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4566 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4567 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4568 batch_limit:   8677 Loss:   0.0762 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4569 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4570 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4571 batch_limit:   8677 Loss:   0.0808 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4572 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4573 batch_limit:   8677 Loss:   0.0808 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4574 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4575 batch_limit:   8677 Loss:   0.0809 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4576 batch_limit:   8677 Loss:   0.0763 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4577 batch_limit:   8677 Loss:   0.0809 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4578 batch_limit:   8677 Loss:   0.0764 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4579 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4580 batch_limit:   8677 Loss:   0.0764 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4581 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4582 batch_limit:   8677 Loss:   0.0764 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4583 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4584 batch_limit:   8677 Loss:   0.0765 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4585 batch_limit:   8677 Loss:   0.0811 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4586 batch_limit:   8677 Loss:   0.0765 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4587 batch_limit:   8677 Loss:   0.0811 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4588 batch_limit:   8677 Loss:   0.0765 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4589 batch_limit:   8677 Loss:   0.0812 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4590 batch_limit:   8677 Loss:   0.0766 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4591 batch_limit:   8677 Loss:   0.0812 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4592 batch_limit:   8677 Loss:   0.0766 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4593 batch_limit:   8677 Loss:   0.0813 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4594 batch_limit:   8677 Loss:   0.0766 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4595 batch_limit:   8677 Loss:   0.0813 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4596 batch_limit:   8677 Loss:   0.0767 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4597 batch_limit:   8677 Loss:   0.0814 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4598 batch_limit:   8677 Loss:   0.0767 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4599 batch_limit:   8677 Loss:   0.0814 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4600 batch_limit:   8677 Loss:   0.0767 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4601 batch_limit:   8677 Loss:   0.0815 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4602 batch_limit:   8677 Loss:   0.0768 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4603 batch_limit:   8677 Loss:   0.0815 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4604 batch_limit:   8677 Loss:   0.0768 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4605 batch_limit:   8677 Loss:   0.0816 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4606 batch_limit:   8677 Loss:   0.0769 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4607 batch_limit:   8677 Loss:   0.0816 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4608 batch_limit:   8677 Loss:   0.0769 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4609 batch_limit:   8677 Loss:   0.0817 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4610 batch_limit:   8677 Loss:   0.0769 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4611 batch_limit:   8677 Loss:   0.0818 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4612 batch_limit:   8677 Loss:   0.0770 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4613 batch_limit:   8677 Loss:   0.0818 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4614 batch_limit:   8677 Loss:   0.0770 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4615 batch_limit:   8677 Loss:   0.0819 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4616 batch_limit:   8677 Loss:   0.0771 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4617 batch_limit:   8677 Loss:   0.0819 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4618 batch_limit:   8677 Loss:   0.0771 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4619 batch_limit:   8677 Loss:   0.0820 Training Acc:   100.00  6771.09    99.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4620 batch_limit:   8677 Loss:   0.0772 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4621 batch_limit:   8677 Loss:   0.0820 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4622 batch_limit:   8677 Loss:   0.0772 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4623 batch_limit:   8677 Loss:   0.0821 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4624 batch_limit:   8677 Loss:   0.0773 Training Acc:   100.00  6769.53    99.55%\n",
      "Epoch   4625 batch_limit:   8677 Loss:   0.0822 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4626 batch_limit:   8677 Loss:   0.0773 Training Acc:   100.00  6770.31    99.56%\n",
      "Epoch   4627 batch_limit:   8677 Loss:   0.0822 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4628 batch_limit:   8677 Loss:   0.0774 Training Acc:   100.00  6770.31    99.56%\n",
      "Epoch   4629 batch_limit:   8677 Loss:   0.0823 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4630 batch_limit:   8677 Loss:   0.0774 Training Acc:   100.00  6770.31    99.56%\n",
      "Epoch   4631 batch_limit:   8677 Loss:   0.0824 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4632 batch_limit:   8677 Loss:   0.0775 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4633 batch_limit:   8677 Loss:   0.0824 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4634 batch_limit:   8677 Loss:   0.0775 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4635 batch_limit:   8677 Loss:   0.0825 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4636 batch_limit:   8677 Loss:   0.0776 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4637 batch_limit:   8677 Loss:   0.0826 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4638 batch_limit:   8677 Loss:   0.0777 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4639 batch_limit:   8677 Loss:   0.0826 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4640 batch_limit:   8677 Loss:   0.0777 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4641 batch_limit:   8677 Loss:   0.0827 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4642 batch_limit:   8677 Loss:   0.0778 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4643 batch_limit:   8677 Loss:   0.0828 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4644 batch_limit:   8677 Loss:   0.0778 Training Acc:   100.00  6771.09    99.57%\n",
      "Epoch   4645 batch_limit:   8677 Loss:   0.0828 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4646 batch_limit:   8677 Loss:   0.0779 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4647 batch_limit:   8677 Loss:   0.0829 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4648 batch_limit:   8677 Loss:   0.0780 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4649 batch_limit:   8677 Loss:   0.0830 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4650 batch_limit:   8677 Loss:   0.0780 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4651 batch_limit:   8677 Loss:   0.0830 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4652 batch_limit:   8677 Loss:   0.0781 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4653 batch_limit:   8677 Loss:   0.0831 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4654 batch_limit:   8677 Loss:   0.0781 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4655 batch_limit:   8677 Loss:   0.0832 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4656 batch_limit:   8677 Loss:   0.0782 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4657 batch_limit:   8677 Loss:   0.0832 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4658 batch_limit:   8677 Loss:   0.0783 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4659 batch_limit:   8677 Loss:   0.0833 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4660 batch_limit:   8677 Loss:   0.0783 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4661 batch_limit:   8677 Loss:   0.0834 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4662 batch_limit:   8677 Loss:   0.0784 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4663 batch_limit:   8677 Loss:   0.0835 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4664 batch_limit:   8677 Loss:   0.0785 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4665 batch_limit:   8677 Loss:   0.0835 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4666 batch_limit:   8677 Loss:   0.0785 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4667 batch_limit:   8677 Loss:   0.0836 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4668 batch_limit:   8677 Loss:   0.0786 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4669 batch_limit:   8677 Loss:   0.0837 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4670 batch_limit:   8677 Loss:   0.0787 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4671 batch_limit:   8677 Loss:   0.0838 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4672 batch_limit:   8677 Loss:   0.0787 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4673 batch_limit:   8677 Loss:   0.0838 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4674 batch_limit:   8677 Loss:   0.0788 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4675 batch_limit:   8677 Loss:   0.0839 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4676 batch_limit:   8677 Loss:   0.0789 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4677 batch_limit:   8677 Loss:   0.0840 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4678 batch_limit:   8677 Loss:   0.0789 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4679 batch_limit:   8677 Loss:   0.0841 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4680 batch_limit:   8677 Loss:   0.0790 Training Acc:   100.00  6771.88    99.59%\n",
      "Epoch   4681 batch_limit:   8677 Loss:   0.0842 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4682 batch_limit:   8677 Loss:   0.0791 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4683 batch_limit:   8677 Loss:   0.0842 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4684 batch_limit:   8677 Loss:   0.0791 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4685 batch_limit:   8677 Loss:   0.0843 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4686 batch_limit:   8677 Loss:   0.0792 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4687 batch_limit:   8677 Loss:   0.0844 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4688 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4689 batch_limit:   8677 Loss:   0.0845 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4690 batch_limit:   8677 Loss:   0.0793 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4691 batch_limit:   8677 Loss:   0.0846 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4692 batch_limit:   8677 Loss:   0.0794 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4693 batch_limit:   8677 Loss:   0.0846 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4694 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4695 batch_limit:   8677 Loss:   0.0847 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4696 batch_limit:   8677 Loss:   0.0795 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4697 batch_limit:   8677 Loss:   0.0848 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4698 batch_limit:   8677 Loss:   0.0796 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4699 batch_limit:   8677 Loss:   0.0849 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4700 batch_limit:   8677 Loss:   0.0797 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4701 batch_limit:   8677 Loss:   0.0850 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4702 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4703 batch_limit:   8677 Loss:   0.0850 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4704 batch_limit:   8677 Loss:   0.0798 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4705 batch_limit:   8677 Loss:   0.0851 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4706 batch_limit:   8677 Loss:   0.0799 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4707 batch_limit:   8677 Loss:   0.0852 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4708 batch_limit:   8677 Loss:   0.0800 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4709 batch_limit:   8677 Loss:   0.0853 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4710 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4711 batch_limit:   8677 Loss:   0.0854 Training Acc:   100.00  6773.44    99.61%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4712 batch_limit:   8677 Loss:   0.0801 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4713 batch_limit:   8677 Loss:   0.0855 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4714 batch_limit:   8677 Loss:   0.0802 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4715 batch_limit:   8677 Loss:   0.0855 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4716 batch_limit:   8677 Loss:   0.0803 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4717 batch_limit:   8677 Loss:   0.0856 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4718 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4719 batch_limit:   8677 Loss:   0.0857 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4720 batch_limit:   8677 Loss:   0.0804 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4721 batch_limit:   8677 Loss:   0.0858 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4722 batch_limit:   8677 Loss:   0.0805 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4723 batch_limit:   8677 Loss:   0.0859 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4724 batch_limit:   8677 Loss:   0.0806 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4725 batch_limit:   8677 Loss:   0.0860 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4726 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4727 batch_limit:   8677 Loss:   0.0860 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4728 batch_limit:   8677 Loss:   0.0807 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4729 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4730 batch_limit:   8677 Loss:   0.0808 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4731 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4732 batch_limit:   8677 Loss:   0.0809 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4733 batch_limit:   8677 Loss:   0.0863 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4734 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4735 batch_limit:   8677 Loss:   0.0864 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4736 batch_limit:   8677 Loss:   0.0810 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4737 batch_limit:   8677 Loss:   0.0865 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4738 batch_limit:   8677 Loss:   0.0811 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4739 batch_limit:   8677 Loss:   0.0866 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4740 batch_limit:   8677 Loss:   0.0812 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4741 batch_limit:   8677 Loss:   0.0866 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4742 batch_limit:   8677 Loss:   0.0813 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4743 batch_limit:   8677 Loss:   0.0867 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4744 batch_limit:   8677 Loss:   0.0813 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4745 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4746 batch_limit:   8677 Loss:   0.0814 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4747 batch_limit:   8677 Loss:   0.0869 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4748 batch_limit:   8677 Loss:   0.0815 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4749 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4750 batch_limit:   8677 Loss:   0.0816 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4751 batch_limit:   8677 Loss:   0.0871 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4752 batch_limit:   8677 Loss:   0.0817 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4753 batch_limit:   8677 Loss:   0.0872 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4754 batch_limit:   8677 Loss:   0.0817 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4755 batch_limit:   8677 Loss:   0.0873 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4756 batch_limit:   8677 Loss:   0.0818 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4757 batch_limit:   8677 Loss:   0.0873 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4758 batch_limit:   8677 Loss:   0.0819 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4759 batch_limit:   8677 Loss:   0.0874 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4760 batch_limit:   8677 Loss:   0.0820 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4761 batch_limit:   8677 Loss:   0.0875 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4762 batch_limit:   8677 Loss:   0.0821 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4763 batch_limit:   8677 Loss:   0.0876 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4764 batch_limit:   8677 Loss:   0.0821 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4765 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4766 batch_limit:   8677 Loss:   0.0822 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4767 batch_limit:   8677 Loss:   0.0878 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4768 batch_limit:   8677 Loss:   0.0823 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4769 batch_limit:   8677 Loss:   0.0879 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4770 batch_limit:   8677 Loss:   0.0824 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4771 batch_limit:   8677 Loss:   0.0880 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4772 batch_limit:   8677 Loss:   0.0825 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4773 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4774 batch_limit:   8677 Loss:   0.0825 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4775 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4776 batch_limit:   8677 Loss:   0.0826 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4777 batch_limit:   8677 Loss:   0.0882 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4778 batch_limit:   8677 Loss:   0.0827 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4779 batch_limit:   8677 Loss:   0.0883 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4780 batch_limit:   8677 Loss:   0.0828 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4781 batch_limit:   8677 Loss:   0.0884 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4782 batch_limit:   8677 Loss:   0.0829 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4783 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4784 batch_limit:   8677 Loss:   0.0830 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4785 batch_limit:   8677 Loss:   0.0886 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4786 batch_limit:   8677 Loss:   0.0830 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4787 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4788 batch_limit:   8677 Loss:   0.0831 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4789 batch_limit:   8677 Loss:   0.0888 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4790 batch_limit:   8677 Loss:   0.0832 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4791 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4792 batch_limit:   8677 Loss:   0.0833 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4793 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4794 batch_limit:   8677 Loss:   0.0834 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4795 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4796 batch_limit:   8677 Loss:   0.0835 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4797 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4798 batch_limit:   8677 Loss:   0.0835 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4799 batch_limit:   8677 Loss:   0.0892 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4800 batch_limit:   8677 Loss:   0.0836 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4801 batch_limit:   8677 Loss:   0.0893 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4802 batch_limit:   8677 Loss:   0.0837 Training Acc:   100.00  6772.66    99.60%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4803 batch_limit:   8677 Loss:   0.0894 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4804 batch_limit:   8677 Loss:   0.0838 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4805 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4806 batch_limit:   8677 Loss:   0.0839 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4807 batch_limit:   8677 Loss:   0.0896 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4808 batch_limit:   8677 Loss:   0.0840 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4809 batch_limit:   8677 Loss:   0.0897 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4810 batch_limit:   8677 Loss:   0.0841 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4811 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4812 batch_limit:   8677 Loss:   0.0841 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4813 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4814 batch_limit:   8677 Loss:   0.0842 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4815 batch_limit:   8677 Loss:   0.0899 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4816 batch_limit:   8677 Loss:   0.0843 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4817 batch_limit:   8677 Loss:   0.0900 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4818 batch_limit:   8677 Loss:   0.0844 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4819 batch_limit:   8677 Loss:   0.0901 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4820 batch_limit:   8677 Loss:   0.0845 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4821 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4822 batch_limit:   8677 Loss:   0.0846 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4823 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4824 batch_limit:   8677 Loss:   0.0847 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4825 batch_limit:   8677 Loss:   0.0904 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4826 batch_limit:   8677 Loss:   0.0847 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4827 batch_limit:   8677 Loss:   0.0905 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4828 batch_limit:   8677 Loss:   0.0848 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4829 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4830 batch_limit:   8677 Loss:   0.0849 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4831 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4832 batch_limit:   8677 Loss:   0.0850 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4833 batch_limit:   8677 Loss:   0.0907 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4834 batch_limit:   8677 Loss:   0.0851 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4835 batch_limit:   8677 Loss:   0.0908 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4836 batch_limit:   8677 Loss:   0.0852 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4837 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4838 batch_limit:   8677 Loss:   0.0853 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4839 batch_limit:   8677 Loss:   0.0910 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4840 batch_limit:   8677 Loss:   0.0854 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4841 batch_limit:   8677 Loss:   0.0911 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4842 batch_limit:   8677 Loss:   0.0855 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4843 batch_limit:   8677 Loss:   0.0912 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4844 batch_limit:   8677 Loss:   0.0855 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4845 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4846 batch_limit:   8677 Loss:   0.0856 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4847 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4848 batch_limit:   8677 Loss:   0.0857 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4849 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4850 batch_limit:   8677 Loss:   0.0858 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4851 batch_limit:   8677 Loss:   0.0915 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4852 batch_limit:   8677 Loss:   0.0859 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4853 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4854 batch_limit:   8677 Loss:   0.0860 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4855 batch_limit:   8677 Loss:   0.0917 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4856 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4857 batch_limit:   8677 Loss:   0.0918 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4858 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4859 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4860 batch_limit:   8677 Loss:   0.0863 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4861 batch_limit:   8677 Loss:   0.0920 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4862 batch_limit:   8677 Loss:   0.0864 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4863 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4864 batch_limit:   8677 Loss:   0.0865 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4865 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4866 batch_limit:   8677 Loss:   0.0865 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4867 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4868 batch_limit:   8677 Loss:   0.0866 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4869 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4870 batch_limit:   8677 Loss:   0.0867 Training Acc:   100.00  6772.66    99.60%\n",
      "Epoch   4871 batch_limit:   8677 Loss:   0.0924 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4872 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4873 batch_limit:   8677 Loss:   0.0925 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4874 batch_limit:   8677 Loss:   0.0869 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4875 batch_limit:   8677 Loss:   0.0926 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4876 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4877 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4878 batch_limit:   8677 Loss:   0.0871 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4879 batch_limit:   8677 Loss:   0.0928 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4880 batch_limit:   8677 Loss:   0.0872 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4881 batch_limit:   8677 Loss:   0.0928 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4882 batch_limit:   8677 Loss:   0.0873 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4883 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4884 batch_limit:   8677 Loss:   0.0874 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4885 batch_limit:   8677 Loss:   0.0930 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4886 batch_limit:   8677 Loss:   0.0875 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4887 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4888 batch_limit:   8677 Loss:   0.0876 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4889 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4890 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4891 batch_limit:   8677 Loss:   0.0933 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4892 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4893 batch_limit:   8677 Loss:   0.0934 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4894 batch_limit:   8677 Loss:   0.0878 Training Acc:   100.00  6773.44    99.61%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4895 batch_limit:   8677 Loss:   0.0934 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4896 batch_limit:   8677 Loss:   0.0879 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4897 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4898 batch_limit:   8677 Loss:   0.0880 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4899 batch_limit:   8677 Loss:   0.0936 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4900 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4901 batch_limit:   8677 Loss:   0.0937 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4902 batch_limit:   8677 Loss:   0.0882 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4903 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4904 batch_limit:   8677 Loss:   0.0883 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4905 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4906 batch_limit:   8677 Loss:   0.0884 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4907 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4908 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4909 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4910 batch_limit:   8677 Loss:   0.0886 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4911 batch_limit:   8677 Loss:   0.0941 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4912 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4913 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4914 batch_limit:   8677 Loss:   0.0888 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4915 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4916 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4917 batch_limit:   8677 Loss:   0.0944 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4918 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4919 batch_limit:   8677 Loss:   0.0944 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4920 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4921 batch_limit:   8677 Loss:   0.0945 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4922 batch_limit:   8677 Loss:   0.0892 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4923 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4924 batch_limit:   8677 Loss:   0.0893 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4925 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4926 batch_limit:   8677 Loss:   0.0894 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4927 batch_limit:   8677 Loss:   0.0948 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4928 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4929 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4930 batch_limit:   8677 Loss:   0.0896 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4931 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4932 batch_limit:   8677 Loss:   0.0897 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4933 batch_limit:   8677 Loss:   0.0950 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4934 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4935 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4936 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4937 batch_limit:   8677 Loss:   0.0952 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4938 batch_limit:   8677 Loss:   0.0899 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4939 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4940 batch_limit:   8677 Loss:   0.0900 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4941 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4942 batch_limit:   8677 Loss:   0.0901 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4943 batch_limit:   8677 Loss:   0.0954 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4944 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4945 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4946 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6773.44    99.61%\n",
      "Epoch   4947 batch_limit:   8677 Loss:   0.0956 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4948 batch_limit:   8677 Loss:   0.0904 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4949 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4950 batch_limit:   8677 Loss:   0.0905 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4951 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4952 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4953 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4954 batch_limit:   8677 Loss:   0.0907 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4955 batch_limit:   8677 Loss:   0.0959 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4956 batch_limit:   8677 Loss:   0.0908 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4957 batch_limit:   8677 Loss:   0.0960 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4958 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4959 batch_limit:   8677 Loss:   0.0960 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4960 batch_limit:   8677 Loss:   0.0910 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4961 batch_limit:   8677 Loss:   0.0961 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4962 batch_limit:   8677 Loss:   0.0911 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4963 batch_limit:   8677 Loss:   0.0962 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4964 batch_limit:   8677 Loss:   0.0912 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4965 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4966 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4967 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4968 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4969 batch_limit:   8677 Loss:   0.0964 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4970 batch_limit:   8677 Loss:   0.0915 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4971 batch_limit:   8677 Loss:   0.0965 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4972 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4973 batch_limit:   8677 Loss:   0.0966 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4974 batch_limit:   8677 Loss:   0.0917 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4975 batch_limit:   8677 Loss:   0.0966 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4976 batch_limit:   8677 Loss:   0.0918 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4977 batch_limit:   8677 Loss:   0.0967 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4978 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4979 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4980 batch_limit:   8677 Loss:   0.0920 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4981 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4982 batch_limit:   8677 Loss:   0.0920 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4983 batch_limit:   8677 Loss:   0.0969 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4984 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4985 batch_limit:   8677 Loss:   0.0970 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4986 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6774.22    99.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4987 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4988 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4989 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4990 batch_limit:   8677 Loss:   0.0924 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4991 batch_limit:   8677 Loss:   0.0972 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4992 batch_limit:   8677 Loss:   0.0925 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4993 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4994 batch_limit:   8677 Loss:   0.0926 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4995 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4996 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4997 batch_limit:   8677 Loss:   0.0974 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4998 batch_limit:   8677 Loss:   0.0928 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   4999 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5000 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5001 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5002 batch_limit:   8677 Loss:   0.0930 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5003 batch_limit:   8677 Loss:   0.0976 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5004 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5005 batch_limit:   8677 Loss:   0.0977 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5006 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5007 batch_limit:   8677 Loss:   0.0977 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5008 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5009 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5010 batch_limit:   8677 Loss:   0.0933 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5011 batch_limit:   8677 Loss:   0.0979 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5012 batch_limit:   8677 Loss:   0.0934 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5013 batch_limit:   8677 Loss:   0.0979 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5014 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5015 batch_limit:   8677 Loss:   0.0980 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5016 batch_limit:   8677 Loss:   0.0936 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5017 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5018 batch_limit:   8677 Loss:   0.0937 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5019 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5020 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5021 batch_limit:   8677 Loss:   0.0982 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5022 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5023 batch_limit:   8677 Loss:   0.0983 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5024 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5025 batch_limit:   8677 Loss:   0.0983 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5026 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5027 batch_limit:   8677 Loss:   0.0984 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5028 batch_limit:   8677 Loss:   0.0941 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5029 batch_limit:   8677 Loss:   0.0984 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5030 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5031 batch_limit:   8677 Loss:   0.0985 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5032 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5033 batch_limit:   8677 Loss:   0.0986 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5034 batch_limit:   8677 Loss:   0.0944 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5035 batch_limit:   8677 Loss:   0.0986 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5036 batch_limit:   8677 Loss:   0.0945 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5037 batch_limit:   8677 Loss:   0.0987 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5038 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5039 batch_limit:   8677 Loss:   0.0987 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5040 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5041 batch_limit:   8677 Loss:   0.0988 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5042 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5043 batch_limit:   8677 Loss:   0.0989 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5044 batch_limit:   8677 Loss:   0.0948 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5045 batch_limit:   8677 Loss:   0.0989 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5046 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5047 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5048 batch_limit:   8677 Loss:   0.0950 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5049 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5050 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5051 batch_limit:   8677 Loss:   0.0991 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5052 batch_limit:   8677 Loss:   0.0952 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5053 batch_limit:   8677 Loss:   0.0992 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5054 batch_limit:   8677 Loss:   0.0952 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5055 batch_limit:   8677 Loss:   0.0992 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5056 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5057 batch_limit:   8677 Loss:   0.0993 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5058 batch_limit:   8677 Loss:   0.0954 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5059 batch_limit:   8677 Loss:   0.0993 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5060 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5061 batch_limit:   8677 Loss:   0.0994 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5062 batch_limit:   8677 Loss:   0.0956 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5063 batch_limit:   8677 Loss:   0.0994 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5064 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5065 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5066 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5067 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5068 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5069 batch_limit:   8677 Loss:   0.0996 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5070 batch_limit:   8677 Loss:   0.0959 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5071 batch_limit:   8677 Loss:   0.0997 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5072 batch_limit:   8677 Loss:   0.0960 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5073 batch_limit:   8677 Loss:   0.0997 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5074 batch_limit:   8677 Loss:   0.0961 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5075 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6774.22    99.62%\n",
      "Epoch   5076 batch_limit:   8677 Loss:   0.0961 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5077 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5078 batch_limit:   8677 Loss:   0.0962 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5079 batch_limit:   8677 Loss:   0.0999 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5080 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5081 batch_limit:   8677 Loss:   0.0999 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5082 batch_limit:   8677 Loss:   0.0964 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5083 batch_limit:   8677 Loss:   0.1000 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5084 batch_limit:   8677 Loss:   0.0965 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5085 batch_limit:   8677 Loss:   0.1000 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5086 batch_limit:   8677 Loss:   0.0965 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5087 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5088 batch_limit:   8677 Loss:   0.0966 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5089 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5090 batch_limit:   8677 Loss:   0.0967 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5091 batch_limit:   8677 Loss:   0.1002 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5092 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5093 batch_limit:   8677 Loss:   0.1002 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5094 batch_limit:   8677 Loss:   0.0969 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5095 batch_limit:   8677 Loss:   0.1003 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5096 batch_limit:   8677 Loss:   0.0969 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5097 batch_limit:   8677 Loss:   0.1003 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5098 batch_limit:   8677 Loss:   0.0970 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5099 batch_limit:   8677 Loss:   0.1004 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5100 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5101 batch_limit:   8677 Loss:   0.1004 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5102 batch_limit:   8677 Loss:   0.0972 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5103 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5104 batch_limit:   8677 Loss:   0.0972 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5105 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5106 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5107 batch_limit:   8677 Loss:   0.1006 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5108 batch_limit:   8677 Loss:   0.0974 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5109 batch_limit:   8677 Loss:   0.1006 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5110 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5111 batch_limit:   8677 Loss:   0.1007 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5112 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5113 batch_limit:   8677 Loss:   0.1007 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5114 batch_limit:   8677 Loss:   0.0976 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5115 batch_limit:   8677 Loss:   0.1008 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5116 batch_limit:   8677 Loss:   0.0977 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5117 batch_limit:   8677 Loss:   0.1008 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5118 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5119 batch_limit:   8677 Loss:   0.1009 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5120 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5121 batch_limit:   8677 Loss:   0.1009 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5122 batch_limit:   8677 Loss:   0.0979 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5123 batch_limit:   8677 Loss:   0.1010 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5124 batch_limit:   8677 Loss:   0.0980 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5125 batch_limit:   8677 Loss:   0.1010 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5126 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5127 batch_limit:   8677 Loss:   0.1011 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5128 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5129 batch_limit:   8677 Loss:   0.1011 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5130 batch_limit:   8677 Loss:   0.0982 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5131 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5132 batch_limit:   8677 Loss:   0.0983 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5133 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5134 batch_limit:   8677 Loss:   0.0984 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5135 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5136 batch_limit:   8677 Loss:   0.0984 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5137 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5138 batch_limit:   8677 Loss:   0.0985 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5139 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5140 batch_limit:   8677 Loss:   0.0986 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5141 batch_limit:   8677 Loss:   0.1014 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5142 batch_limit:   8677 Loss:   0.0987 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5143 batch_limit:   8677 Loss:   0.1014 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5144 batch_limit:   8677 Loss:   0.0987 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5145 batch_limit:   8677 Loss:   0.1015 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5146 batch_limit:   8677 Loss:   0.0988 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5147 batch_limit:   8677 Loss:   0.1015 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5148 batch_limit:   8677 Loss:   0.0989 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5149 batch_limit:   8677 Loss:   0.1015 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5150 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5151 batch_limit:   8677 Loss:   0.1016 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5152 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5153 batch_limit:   8677 Loss:   0.1016 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5154 batch_limit:   8677 Loss:   0.0991 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5155 batch_limit:   8677 Loss:   0.1017 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5156 batch_limit:   8677 Loss:   0.0992 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5157 batch_limit:   8677 Loss:   0.1017 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5158 batch_limit:   8677 Loss:   0.0992 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5159 batch_limit:   8677 Loss:   0.1018 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5160 batch_limit:   8677 Loss:   0.0993 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5161 batch_limit:   8677 Loss:   0.1018 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5162 batch_limit:   8677 Loss:   0.0994 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5163 batch_limit:   8677 Loss:   0.1018 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5164 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5165 batch_limit:   8677 Loss:   0.1019 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5166 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5167 batch_limit:   8677 Loss:   0.1019 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5168 batch_limit:   8677 Loss:   0.0996 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5169 batch_limit:   8677 Loss:   0.1019 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5170 batch_limit:   8677 Loss:   0.0997 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5171 batch_limit:   8677 Loss:   0.1020 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5172 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5173 batch_limit:   8677 Loss:   0.1020 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5174 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5175 batch_limit:   8677 Loss:   0.1020 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5176 batch_limit:   8677 Loss:   0.0999 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5177 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5178 batch_limit:   8677 Loss:   0.1000 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5179 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5180 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5181 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5182 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5183 batch_limit:   8677 Loss:   0.1022 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5184 batch_limit:   8677 Loss:   0.1002 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5185 batch_limit:   8677 Loss:   0.1022 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5186 batch_limit:   8677 Loss:   0.1003 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5187 batch_limit:   8677 Loss:   0.1022 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5188 batch_limit:   8677 Loss:   0.1004 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5189 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5190 batch_limit:   8677 Loss:   0.1004 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5191 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5192 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5193 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5194 batch_limit:   8677 Loss:   0.1006 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5195 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5196 batch_limit:   8677 Loss:   0.1007 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5197 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5198 batch_limit:   8677 Loss:   0.1008 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5199 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5200 batch_limit:   8677 Loss:   0.1008 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5201 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5202 batch_limit:   8677 Loss:   0.1009 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5203 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5204 batch_limit:   8677 Loss:   0.1010 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5205 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5206 batch_limit:   8677 Loss:   0.1011 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5207 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5208 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5209 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5210 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5211 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5212 batch_limit:   8677 Loss:   0.1014 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5213 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5214 batch_limit:   8677 Loss:   0.1014 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5215 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5216 batch_limit:   8677 Loss:   0.1015 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5217 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5218 batch_limit:   8677 Loss:   0.1016 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5219 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5220 batch_limit:   8677 Loss:   0.1017 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5221 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5222 batch_limit:   8677 Loss:   0.1019 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5223 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5224 batch_limit:   8677 Loss:   0.1020 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5225 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5226 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5227 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5228 batch_limit:   8677 Loss:   0.1022 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5229 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5230 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5231 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5232 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5233 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5234 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5235 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5236 batch_limit:   8677 Loss:   0.1026 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5237 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5238 batch_limit:   8677 Loss:   0.1026 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5239 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5240 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5241 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5242 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5243 batch_limit:   8677 Loss:   0.1026 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5244 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5245 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5246 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5247 batch_limit:   8677 Loss:   0.1028 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5248 batch_limit:   8677 Loss:   0.1028 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5249 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5250 batch_limit:   8677 Loss:   0.1028 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5251 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5252 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5253 batch_limit:   8677 Loss:   0.1030 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5254 batch_limit:   8677 Loss:   0.1030 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5255 batch_limit:   8677 Loss:   0.1030 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5256 batch_limit:   8677 Loss:   0.1031 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5257 batch_limit:   8677 Loss:   0.1031 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5258 batch_limit:   8677 Loss:   0.1031 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5259 batch_limit:   8677 Loss:   0.1032 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5260 batch_limit:   8677 Loss:   0.1032 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5261 batch_limit:   8677 Loss:   0.1032 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5262 batch_limit:   8677 Loss:   0.1033 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5263 batch_limit:   8677 Loss:   0.1033 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5264 batch_limit:   8677 Loss:   0.1033 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5265 batch_limit:   8677 Loss:   0.1034 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5266 batch_limit:   8677 Loss:   0.1034 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5267 batch_limit:   8677 Loss:   0.1034 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5268 batch_limit:   8677 Loss:   0.1035 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5269 batch_limit:   8677 Loss:   0.1035 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5270 batch_limit:   8677 Loss:   0.1035 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5271 batch_limit:   8677 Loss:   0.1036 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5272 batch_limit:   8677 Loss:   0.1036 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5273 batch_limit:   8677 Loss:   0.1036 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5274 batch_limit:   8677 Loss:   0.1037 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5275 batch_limit:   8677 Loss:   0.1037 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5276 batch_limit:   8677 Loss:   0.1037 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5277 batch_limit:   8677 Loss:   0.1038 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5278 batch_limit:   8677 Loss:   0.1038 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5279 batch_limit:   8677 Loss:   0.1038 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5280 batch_limit:   8677 Loss:   0.1039 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5281 batch_limit:   8677 Loss:   0.1039 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5282 batch_limit:   8677 Loss:   0.1039 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5283 batch_limit:   8677 Loss:   0.1039 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5284 batch_limit:   8677 Loss:   0.1040 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5285 batch_limit:   8677 Loss:   0.1040 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5286 batch_limit:   8677 Loss:   0.1040 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5287 batch_limit:   8677 Loss:   0.1041 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5288 batch_limit:   8677 Loss:   0.1041 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5289 batch_limit:   8677 Loss:   0.1041 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5290 batch_limit:   8677 Loss:   0.1042 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5291 batch_limit:   8677 Loss:   0.1042 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5292 batch_limit:   8677 Loss:   0.1042 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5293 batch_limit:   8677 Loss:   0.1042 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5294 batch_limit:   8677 Loss:   0.1043 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5295 batch_limit:   8677 Loss:   0.1043 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5296 batch_limit:   8677 Loss:   0.1043 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5297 batch_limit:   8677 Loss:   0.1044 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5298 batch_limit:   8677 Loss:   0.1044 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5299 batch_limit:   8677 Loss:   0.1044 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5300 batch_limit:   8677 Loss:   0.1044 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5301 batch_limit:   8677 Loss:   0.1045 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5302 batch_limit:   8677 Loss:   0.1045 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5303 batch_limit:   8677 Loss:   0.1045 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5304 batch_limit:   8677 Loss:   0.1046 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5305 batch_limit:   8677 Loss:   0.1046 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5306 batch_limit:   8677 Loss:   0.1046 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5307 batch_limit:   8677 Loss:   0.1046 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5308 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5309 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5310 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5311 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5312 batch_limit:   8677 Loss:   0.1048 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5313 batch_limit:   8677 Loss:   0.1048 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5314 batch_limit:   8677 Loss:   0.1048 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5315 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5316 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5317 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5318 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5319 batch_limit:   8677 Loss:   0.1050 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5320 batch_limit:   8677 Loss:   0.1050 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5321 batch_limit:   8677 Loss:   0.1050 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5322 batch_limit:   8677 Loss:   0.1050 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5323 batch_limit:   8677 Loss:   0.1051 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5324 batch_limit:   8677 Loss:   0.1051 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5325 batch_limit:   8677 Loss:   0.1051 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5326 batch_limit:   8677 Loss:   0.1051 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5327 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5328 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5329 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5330 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5331 batch_limit:   8677 Loss:   0.1053 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5332 batch_limit:   8677 Loss:   0.1053 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5333 batch_limit:   8677 Loss:   0.1053 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5334 batch_limit:   8677 Loss:   0.1054 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5335 batch_limit:   8677 Loss:   0.1054 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5336 batch_limit:   8677 Loss:   0.1054 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5337 batch_limit:   8677 Loss:   0.1054 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5338 batch_limit:   8677 Loss:   0.1055 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5339 batch_limit:   8677 Loss:   0.1055 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5340 batch_limit:   8677 Loss:   0.1055 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5341 batch_limit:   8677 Loss:   0.1055 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5342 batch_limit:   8677 Loss:   0.1056 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5343 batch_limit:   8677 Loss:   0.1056 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5344 batch_limit:   8677 Loss:   0.1056 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5345 batch_limit:   8677 Loss:   0.1056 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5346 batch_limit:   8677 Loss:   0.1057 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5347 batch_limit:   8677 Loss:   0.1057 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5348 batch_limit:   8677 Loss:   0.1057 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5349 batch_limit:   8677 Loss:   0.1057 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5350 batch_limit:   8677 Loss:   0.1057 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5351 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5352 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5353 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5354 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5355 batch_limit:   8677 Loss:   0.1059 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5356 batch_limit:   8677 Loss:   0.1059 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5357 batch_limit:   8677 Loss:   0.1059 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5358 batch_limit:   8677 Loss:   0.1059 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5359 batch_limit:   8677 Loss:   0.1060 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5360 batch_limit:   8677 Loss:   0.1060 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5361 batch_limit:   8677 Loss:   0.1060 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5362 batch_limit:   8677 Loss:   0.1060 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5363 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5364 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5365 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5366 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5367 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5368 batch_limit:   8677 Loss:   0.1062 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5369 batch_limit:   8677 Loss:   0.1062 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5370 batch_limit:   8677 Loss:   0.1062 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5371 batch_limit:   8677 Loss:   0.1062 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5372 batch_limit:   8677 Loss:   0.1063 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5373 batch_limit:   8677 Loss:   0.1063 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5374 batch_limit:   8677 Loss:   0.1063 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5375 batch_limit:   8677 Loss:   0.1063 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5376 batch_limit:   8677 Loss:   0.1064 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5377 batch_limit:   8677 Loss:   0.1064 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5378 batch_limit:   8677 Loss:   0.1064 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5379 batch_limit:   8677 Loss:   0.1064 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5380 batch_limit:   8677 Loss:   0.1064 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5381 batch_limit:   8677 Loss:   0.1065 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5382 batch_limit:   8677 Loss:   0.1065 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5383 batch_limit:   8677 Loss:   0.1065 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5384 batch_limit:   8677 Loss:   0.1065 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5385 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5386 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5387 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5388 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5389 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5390 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5391 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5392 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5393 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5394 batch_limit:   8677 Loss:   0.1068 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5395 batch_limit:   8677 Loss:   0.1068 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5396 batch_limit:   8677 Loss:   0.1068 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5397 batch_limit:   8677 Loss:   0.1068 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5398 batch_limit:   8677 Loss:   0.1068 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5399 batch_limit:   8677 Loss:   0.1069 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5400 batch_limit:   8677 Loss:   0.1069 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5401 batch_limit:   8677 Loss:   0.1069 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5402 batch_limit:   8677 Loss:   0.1069 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5403 batch_limit:   8677 Loss:   0.1069 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5404 batch_limit:   8677 Loss:   0.1070 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5405 batch_limit:   8677 Loss:   0.1070 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5406 batch_limit:   8677 Loss:   0.1070 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5407 batch_limit:   8677 Loss:   0.1070 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5408 batch_limit:   8677 Loss:   0.1071 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5409 batch_limit:   8677 Loss:   0.1071 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5410 batch_limit:   8677 Loss:   0.1071 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5411 batch_limit:   8677 Loss:   0.1071 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5412 batch_limit:   8677 Loss:   0.1071 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5413 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5414 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5415 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5416 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5417 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5418 batch_limit:   8677 Loss:   0.1073 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5419 batch_limit:   8677 Loss:   0.1073 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5420 batch_limit:   8677 Loss:   0.1073 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5421 batch_limit:   8677 Loss:   0.1073 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5422 batch_limit:   8677 Loss:   0.1073 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5423 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5424 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5425 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5426 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5427 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5428 batch_limit:   8677 Loss:   0.1075 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5429 batch_limit:   8677 Loss:   0.1075 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5430 batch_limit:   8677 Loss:   0.1075 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5431 batch_limit:   8677 Loss:   0.1075 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5432 batch_limit:   8677 Loss:   0.1075 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5433 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5434 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5435 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5436 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5437 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5438 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5439 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5440 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5441 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5442 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5443 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5444 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5445 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5446 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5447 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5448 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5449 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5450 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5451 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5452 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5453 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5454 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5455 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5456 batch_limit:   8677 Loss:   0.1080 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5457 batch_limit:   8677 Loss:   0.1080 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5458 batch_limit:   8677 Loss:   0.1080 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5459 batch_limit:   8677 Loss:   0.1080 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5460 batch_limit:   8677 Loss:   0.1080 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5461 batch_limit:   8677 Loss:   0.1080 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5462 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5463 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5464 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5465 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5466 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5467 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5468 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5469 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5470 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5471 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5472 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5473 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5474 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5475 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5476 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5477 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5478 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5479 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5480 batch_limit:   8677 Loss:   0.1084 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5481 batch_limit:   8677 Loss:   0.1084 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5482 batch_limit:   8677 Loss:   0.1084 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5483 batch_limit:   8677 Loss:   0.1084 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5484 batch_limit:   8677 Loss:   0.1084 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5485 batch_limit:   8677 Loss:   0.1084 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5486 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5487 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5488 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5489 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5490 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5491 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5492 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5493 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5494 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5495 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5496 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5497 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5498 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5499 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5500 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5501 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5502 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5503 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5504 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5505 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5506 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5507 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5508 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5509 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5510 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5511 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5512 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5513 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5514 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5515 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5516 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5517 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5518 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5519 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5520 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5521 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5522 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5523 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5524 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5525 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5526 batch_limit:   8677 Loss:   0.1091 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5527 batch_limit:   8677 Loss:   0.1091 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5528 batch_limit:   8677 Loss:   0.1091 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5529 batch_limit:   8677 Loss:   0.1091 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5530 batch_limit:   8677 Loss:   0.1091 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5531 batch_limit:   8677 Loss:   0.1091 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5532 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5533 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5534 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5535 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5536 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5537 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5538 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5539 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5540 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5541 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5542 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5543 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5544 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5545 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5546 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5547 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5548 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5549 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5550 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5551 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5552 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5553 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5554 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5555 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5556 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5557 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5558 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5559 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5560 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5561 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5562 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5563 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5564 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5565 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5566 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5567 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5568 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5569 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5570 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5571 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5572 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5573 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5574 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5575 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5576 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5577 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5578 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5579 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5580 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5581 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5582 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5583 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5584 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5585 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5586 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5587 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5588 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5589 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5590 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5591 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5592 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5593 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5594 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5595 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5596 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5597 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5598 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5599 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5600 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5601 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5602 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5603 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5604 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5605 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5606 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5607 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5608 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5609 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5610 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5611 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5612 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5613 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5614 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5615 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5616 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5617 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5618 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5619 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5620 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5621 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5622 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5623 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5624 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5625 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5626 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5627 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5628 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5629 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5630 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5631 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5632 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5633 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5634 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5635 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5636 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5637 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5638 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5639 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5640 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5641 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5642 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5643 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5644 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5645 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5646 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5647 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5648 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5649 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5650 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5651 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5652 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5653 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5654 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5655 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5656 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5657 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5658 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5659 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5660 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5661 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5662 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5663 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5664 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5665 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5666 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5667 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5668 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5669 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5670 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5671 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5672 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5673 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5674 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5675 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5676 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5677 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5678 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5679 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5680 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5681 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5682 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5683 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5684 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5685 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5686 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5687 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5688 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5689 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5690 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5691 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5692 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5693 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5694 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5695 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5696 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5697 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5698 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5699 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5700 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5701 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5702 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5703 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5704 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5705 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5706 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5707 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5708 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5709 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5710 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5711 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5712 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5713 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5714 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5715 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5716 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5717 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5718 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5719 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5720 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5721 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5722 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5723 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5724 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5725 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5726 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5727 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5728 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5729 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5730 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5731 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5732 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5733 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5734 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5735 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5736 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5737 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5738 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5739 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5740 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5741 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5742 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5743 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5744 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5745 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5746 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5747 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5748 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5749 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5750 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5751 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5752 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5753 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5754 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5755 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5756 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5757 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5758 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5759 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5760 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5761 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5762 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5763 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5764 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5765 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5766 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5767 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5768 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5769 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5770 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5771 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5772 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5773 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5774 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5775 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5776 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5777 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5778 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5779 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5780 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5781 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5782 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5783 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5784 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5785 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5786 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5787 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5788 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5789 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5790 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5791 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5792 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5793 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5794 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5795 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5796 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5797 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5798 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5799 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5800 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5801 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5802 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5803 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5804 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5805 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5806 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5807 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5808 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5809 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5810 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5811 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5812 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5813 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5814 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5815 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5816 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5817 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5818 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5819 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5820 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5821 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5822 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5823 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5824 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5825 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5826 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5827 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5828 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5829 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5830 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5831 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5832 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5833 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5834 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5835 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5836 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5837 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5838 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5839 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5840 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5841 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5842 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5843 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5844 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5845 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5846 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5847 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5848 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5849 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5850 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5851 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5852 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5853 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5854 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5855 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5856 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5857 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5858 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5859 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5860 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5861 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5862 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5863 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5864 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5865 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5866 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5867 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5868 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5869 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5870 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5871 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5872 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5873 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5874 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5875 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5876 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5877 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5878 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5879 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5880 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5881 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5882 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5883 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5884 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5885 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5886 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5887 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5888 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5889 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5890 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5891 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5892 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5893 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5894 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5895 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5896 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5897 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5898 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5899 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5900 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5901 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5902 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5903 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5904 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5905 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5906 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5907 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5908 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5909 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5910 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5911 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5912 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5913 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5914 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5915 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5916 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5917 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5918 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5919 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5920 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5921 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5922 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5923 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5924 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5925 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5926 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5927 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5928 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5929 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5930 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5931 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5932 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5933 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5934 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5935 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5936 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5937 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5938 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5939 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5940 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5941 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5942 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5943 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5944 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5945 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5946 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5947 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5948 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5949 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5950 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5951 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5952 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5953 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5954 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5955 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5956 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5957 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5958 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5959 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5960 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5961 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5962 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5963 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5964 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5965 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5966 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5967 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5968 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5969 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5970 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5971 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5972 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5973 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5974 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5975 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5976 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5977 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5978 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5979 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5980 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5981 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5982 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5983 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5984 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5985 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5986 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5987 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5988 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5989 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5990 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5991 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5992 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5993 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5994 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5995 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5996 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5997 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5998 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   5999 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6000 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6001 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6002 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6003 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6004 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6005 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6006 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6007 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6008 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6009 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6010 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6011 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6012 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6013 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6014 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6015 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6016 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6017 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6018 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6019 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6020 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6021 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6022 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6023 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6024 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6025 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6026 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6027 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6028 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6029 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6030 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6031 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6032 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6033 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6034 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6035 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6036 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6037 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6038 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6039 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6040 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6041 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6042 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6043 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6044 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6045 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6046 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6047 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6048 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6049 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6050 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6051 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6052 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6053 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6054 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6055 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6056 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6057 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6058 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6059 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6060 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6061 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6062 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6063 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6064 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6065 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6066 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6067 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6068 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6069 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6070 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6071 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6072 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6073 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6074 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6075 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6076 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6077 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6078 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6079 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6080 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6081 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6082 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6083 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6084 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6085 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6086 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6087 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6088 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6089 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6090 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6091 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6092 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6093 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6094 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6095 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6096 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6097 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6098 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6099 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6100 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6101 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6102 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6103 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6104 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6105 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6106 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6107 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6108 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6109 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6110 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6111 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6112 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6113 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6114 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6115 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6116 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6117 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6118 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6119 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6120 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6121 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6122 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6123 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6124 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6125 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6126 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6127 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6128 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6129 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6130 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6131 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6132 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6133 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6134 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6135 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6136 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6137 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6138 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6139 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6140 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6141 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6142 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6143 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6144 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6145 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6146 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6147 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6148 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6149 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6150 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6151 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6152 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6153 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6154 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6155 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6156 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6157 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6158 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6159 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6160 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6161 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6162 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6163 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6164 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6165 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6166 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6167 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6168 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6169 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6170 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6171 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6172 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6173 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6174 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6175 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6176 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6177 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6178 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6179 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6180 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6181 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6182 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6183 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6184 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6185 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6186 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6187 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6188 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6189 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6190 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6191 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6192 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6193 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6194 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6195 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6196 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6197 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6198 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6199 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6200 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6201 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6202 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6203 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6204 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6205 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6206 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6207 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6208 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6209 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6210 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6211 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6212 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6213 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6214 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6215 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6216 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6217 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6218 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6219 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6220 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6221 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6222 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6223 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6224 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6225 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6226 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6227 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6228 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6229 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6230 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6231 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6232 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6233 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6234 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6235 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6236 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6237 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6238 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6239 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6240 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6241 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6242 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6243 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6244 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6245 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6246 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6247 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6248 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6249 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6250 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6251 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6252 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6253 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6254 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6255 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6256 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6257 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6258 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6259 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6260 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6261 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6262 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6263 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6264 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6265 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6266 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6267 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6268 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6269 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6270 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6271 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6272 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6273 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6274 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6275 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6276 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6277 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6278 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6279 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6280 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6281 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6282 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6283 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6284 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6285 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6286 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6287 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6288 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6289 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6290 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6291 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6292 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6293 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6294 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6295 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6296 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6297 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6298 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6299 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6300 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6301 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6302 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6303 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6304 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6305 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6306 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6307 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6308 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6309 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6310 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6311 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6312 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6313 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6314 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6315 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6316 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6317 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6318 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6319 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6320 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6321 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6322 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6323 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6324 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6325 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6326 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6327 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6328 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6329 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6330 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6331 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6332 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6333 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6334 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6335 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6336 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6337 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6338 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6339 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6340 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6341 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6342 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6343 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6344 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6345 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6346 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6347 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6348 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6349 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6350 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6351 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6352 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6353 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6354 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6355 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6356 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6357 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6358 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6359 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6360 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6361 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6362 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6363 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6364 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6365 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6366 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6367 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6368 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6369 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6370 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6371 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6372 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6373 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6374 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6375 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6376 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6377 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6378 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6379 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6380 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6381 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6382 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6383 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6384 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6385 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6386 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6387 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6388 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6389 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6390 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6391 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6392 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6393 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6394 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6395 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6396 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6397 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6398 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6399 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6400 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6401 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6402 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6403 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6404 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6405 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6406 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6407 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6408 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6409 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6410 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6411 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6412 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6413 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6414 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6415 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6416 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6417 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6418 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6419 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6420 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6421 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6422 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6423 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6424 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6425 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6426 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6427 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6428 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6429 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6430 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6431 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6432 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6433 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6434 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6435 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6436 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6437 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6438 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6439 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6440 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6441 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6442 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6443 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6444 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6445 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6446 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6447 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6448 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6449 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6450 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6451 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6452 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6453 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6454 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6455 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6456 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6457 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6458 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6459 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6460 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6461 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6462 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6463 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6464 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6465 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6466 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6467 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6468 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6469 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6470 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6471 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6472 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6473 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6474 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6475 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6476 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6477 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6478 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6479 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6480 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6481 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6482 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6483 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6484 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6485 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6486 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6487 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6488 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6489 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6490 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6491 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6492 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6493 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6494 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6495 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6496 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6497 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6498 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6499 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6500 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6501 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6502 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6503 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6504 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6505 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6506 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6507 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6508 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6509 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6510 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6511 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6512 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6513 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6514 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6515 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6516 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6517 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6518 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6519 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6520 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6521 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6522 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6523 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6524 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6525 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6526 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6527 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6528 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6529 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6530 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6531 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6532 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6533 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6534 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6535 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6536 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6537 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6538 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6539 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6540 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6541 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6542 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6543 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6544 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6545 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6546 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6547 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6548 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6549 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6550 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6551 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6552 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6553 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6554 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6555 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6556 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6557 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6558 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6559 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6560 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6561 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6562 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6563 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6564 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6565 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6566 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6567 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6568 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6569 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6570 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6571 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6572 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6573 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6574 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6575 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6576 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6577 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6578 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6579 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6580 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6581 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6582 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6583 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6584 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6585 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6586 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6587 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6588 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6589 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6590 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6591 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6592 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6593 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6594 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6595 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6596 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6597 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6598 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6599 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6600 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6601 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6602 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6603 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6604 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6605 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6606 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6607 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6608 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6609 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6610 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6611 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6612 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6613 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6614 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6615 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6616 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6617 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6618 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6619 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6620 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6621 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6622 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6623 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6624 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6625 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6626 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6627 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6628 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6629 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6630 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6631 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6632 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6633 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6634 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6635 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6636 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6637 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6638 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6639 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6640 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6641 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6642 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6643 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6644 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6645 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6646 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6647 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6648 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6649 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6650 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6651 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6652 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6653 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6654 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6655 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6656 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6657 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6658 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6659 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6660 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6661 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6662 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6663 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6664 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6665 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6666 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6667 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6668 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6669 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6670 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6671 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6672 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6673 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6674 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6675 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6676 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6677 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6678 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6679 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6680 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6681 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6682 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6683 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6684 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6685 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6686 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6687 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6688 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6689 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6690 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6691 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6692 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6693 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6694 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6695 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6696 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6697 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6698 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6699 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6700 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6701 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6702 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6703 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6704 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6705 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6706 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6707 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6708 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6709 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6710 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6711 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6712 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6713 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6714 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6715 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6716 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6717 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6718 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6719 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6720 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6721 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6722 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6723 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6724 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6725 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6726 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6727 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6728 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6729 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6730 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6731 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6732 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6733 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6734 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6735 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6736 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6737 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6738 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6739 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6740 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6741 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6742 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6743 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6744 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6745 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6746 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6747 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6748 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6749 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6750 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6751 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6752 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6753 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6754 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6755 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6756 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6757 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6758 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6759 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6760 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6761 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6762 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6763 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6764 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6765 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6766 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6767 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6768 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6769 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6770 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6771 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6772 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6773 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6774 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6775 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6776 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6777 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6778 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6779 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6780 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6781 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6782 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6783 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6784 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6785 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6786 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6787 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6788 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6789 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6790 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6791 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6792 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6793 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6794 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6795 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6796 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6797 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6798 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6799 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6800 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6801 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6802 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6803 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6804 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6805 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6806 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6807 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6808 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6809 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6810 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6811 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6812 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6813 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6814 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6815 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6816 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6817 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6818 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6819 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6820 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6821 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6822 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6823 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6824 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6825 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6826 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6827 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6828 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6829 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6830 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6831 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6832 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6833 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6834 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6835 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6836 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6837 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6838 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6839 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6840 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6841 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6842 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6843 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6844 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6845 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6846 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6847 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6848 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6849 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6850 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6851 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6852 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6853 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6854 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6855 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6856 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6857 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6858 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6859 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6860 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6861 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6862 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6863 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6864 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6865 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6866 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6867 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6868 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6869 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6870 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6871 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6872 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6873 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6874 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6875 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6876 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6877 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6878 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6879 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6880 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6881 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6882 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6883 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6884 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6885 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6886 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6887 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6888 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6889 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6890 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6891 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6892 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6893 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6894 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6895 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6896 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6897 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6898 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6899 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6900 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6901 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6902 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6903 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6904 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6905 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6906 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6907 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6908 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6909 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6910 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6911 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6912 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6913 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6914 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6915 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6916 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6917 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6918 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6919 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6920 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6921 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6922 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6923 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6924 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6925 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6926 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6927 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6928 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6929 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6930 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6931 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6932 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6933 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6934 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6935 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6936 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6937 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6938 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6939 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6940 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6941 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6942 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6943 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6944 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6945 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6946 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6947 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6948 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6949 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6950 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6951 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6952 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6953 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6954 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6955 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6956 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6957 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6958 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6959 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6960 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6961 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6962 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6963 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6964 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6965 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6966 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6967 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6968 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6969 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6970 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6971 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6972 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6973 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6974 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6975 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6976 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6977 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6978 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6979 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6980 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6981 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6982 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6983 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6984 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6985 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6986 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6987 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6988 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6989 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6990 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6991 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6992 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6993 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6994 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6995 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6996 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6997 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6998 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   6999 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7000 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7001 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7002 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7003 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7004 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7005 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7006 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7007 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7008 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7009 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7010 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7011 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7012 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7013 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7014 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7015 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7016 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7017 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7018 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7019 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7020 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7021 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7022 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7023 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7024 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7025 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7026 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7027 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7028 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7029 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7030 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7031 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7032 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7033 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7034 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7035 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7036 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7037 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7038 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7039 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7040 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7041 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7042 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7043 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7044 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7045 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7046 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7047 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7048 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7049 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7050 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7051 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7052 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7053 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7054 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7055 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7056 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7057 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7058 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7059 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7060 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7061 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7062 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7063 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7064 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7065 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7066 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7067 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7068 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7069 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7070 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7071 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7072 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7073 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7074 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7075 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7076 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7077 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7078 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7079 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7080 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7081 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7082 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7083 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7084 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7085 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7086 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7087 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7088 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7089 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7090 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7091 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7092 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7093 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7094 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7095 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7096 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7097 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7098 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7099 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7100 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7101 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7102 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7103 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7104 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7105 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7106 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7107 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7108 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7109 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7110 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7111 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7112 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7113 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7114 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7115 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7116 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7117 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7118 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7119 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7120 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7121 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7122 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7123 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7124 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7125 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7126 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7127 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7128 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7129 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7130 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7131 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7132 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7133 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7134 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7135 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7136 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7137 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7138 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7139 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7140 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7141 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7142 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7143 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7144 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7145 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7146 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7147 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7148 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7149 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7150 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7151 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7152 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7153 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7154 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7155 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7156 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7157 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7158 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7159 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7160 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7161 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7162 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7163 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7164 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7165 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7166 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7167 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7168 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7169 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7170 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7171 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7172 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7173 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7174 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7175 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7176 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7177 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7178 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7179 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7180 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7181 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7182 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7183 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7184 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7185 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7186 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7187 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7188 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7189 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7190 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7191 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7192 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7193 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7194 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7195 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7196 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7197 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7198 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7199 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7200 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7201 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7202 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7203 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.78    99.64%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7204 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7205 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7206 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7207 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7208 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7209 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7210 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7211 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7212 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7213 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7214 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7215 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7216 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7217 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7218 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7219 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7220 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7221 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7222 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7223 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7224 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7225 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7226 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7227 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7228 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7229 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7230 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7231 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7232 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7233 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7234 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7235 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7236 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7237 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7238 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7239 batch_limit:   8677 Loss:   0.1171 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7240 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7241 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7242 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7243 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7244 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7245 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7246 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7247 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7248 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7249 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7250 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7251 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7252 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7253 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7254 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7255 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7256 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7257 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7258 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7259 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7260 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7261 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7262 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7263 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7264 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7265 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7266 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7267 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7268 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7269 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7270 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7271 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7272 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7273 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7274 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7275 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7276 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7277 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7278 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7279 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7280 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7281 batch_limit:   8677 Loss:   0.1168 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7282 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7283 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7284 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7285 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7286 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7287 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7288 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7289 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7290 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7291 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7292 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7293 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7294 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7295 batch_limit:   8677 Loss:   0.1167 Training Acc:    99.01  6774.79    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7296 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7297 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7298 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7299 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7300 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7301 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7302 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7303 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7304 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7305 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7306 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7307 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7308 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7309 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7310 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7311 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7312 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7313 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7314 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7315 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7316 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7317 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7318 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7319 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7320 batch_limit:   8677 Loss:   0.1171 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7321 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7322 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7323 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7324 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7325 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7326 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7327 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7328 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7329 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7330 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7331 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7332 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7333 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7334 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7335 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7336 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7337 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7338 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7339 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7340 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7341 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7342 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7343 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7344 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7345 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7346 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7347 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7348 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7349 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7350 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7351 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7352 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7353 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7354 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7355 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7356 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7357 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7358 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7359 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7360 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7361 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7362 batch_limit:   8677 Loss:   0.1168 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7363 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7364 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7365 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7366 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7367 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7368 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7369 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7370 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7371 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7372 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7373 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7374 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7375 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7376 batch_limit:   8677 Loss:   0.1166 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7377 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7378 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7379 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7380 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7381 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7382 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7383 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7384 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7385 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7386 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7387 batch_limit:   8677 Loss:   0.1171 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7388 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7389 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7390 batch_limit:   8677 Loss:   0.1165 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7391 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7392 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7393 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7394 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7395 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7396 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7397 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7398 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7399 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7400 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7401 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7402 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7403 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7404 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7405 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7406 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7407 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7408 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7409 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7410 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7411 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7412 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7413 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7414 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7415 batch_limit:   8677 Loss:   0.1168 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7416 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7417 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7418 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7419 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7420 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7421 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7422 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7423 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7424 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7425 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7426 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7427 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7428 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7429 batch_limit:   8677 Loss:   0.1167 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7430 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7431 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7432 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7433 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7434 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7435 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7436 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7437 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7438 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7439 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7440 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7441 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7442 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7443 batch_limit:   8677 Loss:   0.1166 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7444 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7445 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7446 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7447 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7448 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7449 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7450 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7451 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7452 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7453 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7454 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7455 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7456 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7457 batch_limit:   8677 Loss:   0.1165 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7458 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7459 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7460 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7461 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7462 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7463 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7464 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7465 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7466 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7467 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7468 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7469 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7470 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7471 batch_limit:   8677 Loss:   0.1164 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7472 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7473 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7474 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7475 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7476 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7477 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7478 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7479 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7480 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6775.00    99.63%\n",
      "Epoch   7481 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.01    99.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7482 batch_limit:   8677 Loss:   0.1168 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7483 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7484 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7485 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7486 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7487 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7488 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7489 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7490 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7491 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7492 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7493 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7494 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7495 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7496 batch_limit:   8677 Loss:   0.1167 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7497 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7498 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7499 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7500 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7501 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7502 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7503 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7504 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7505 batch_limit:   8677 Loss:   0.1166 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7506 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7507 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7508 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7509 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7510 batch_limit:   8677 Loss:   0.1166 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7511 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7512 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7513 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7514 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7515 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7516 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7517 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7518 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7519 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7520 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7521 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7522 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7523 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7524 batch_limit:   8677 Loss:   0.1165 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7525 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7526 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7527 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   7528 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7529 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7530 batch_limit:   8677 Loss:   0.1167 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7531 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7532 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7533 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7534 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7535 batch_limit:   8677 Loss:   0.1168 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7536 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7537 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7538 batch_limit:   8677 Loss:   0.1165 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7539 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7540 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7541 batch_limit:   8677 Loss:   0.1166 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7542 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7543 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7544 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7545 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7546 batch_limit:   8677 Loss:   0.1171 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7547 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7548 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7549 batch_limit:   8677 Loss:   0.1167 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7550 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7551 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7552 batch_limit:   8677 Loss:   0.1166 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7553 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7554 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7555 batch_limit:   8677 Loss:   0.1168 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7556 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7557 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7558 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7559 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7560 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7561 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7562 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7563 batch_limit:   8677 Loss:   0.1167 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7564 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7565 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7566 batch_limit:   8677 Loss:   0.1167 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7567 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7568 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7569 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7570 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7571 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7572 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7573 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7574 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7575 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7576 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7577 batch_limit:   8677 Loss:   0.1167 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7578 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7579 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7580 batch_limit:   8677 Loss:   0.1168 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7581 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7582 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7583 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7584 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7585 batch_limit:   8677 Loss:   0.1171 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7586 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7587 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7588 batch_limit:   8677 Loss:   0.1168 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7589 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7590 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7591 batch_limit:   8677 Loss:   0.1168 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7592 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7593 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7594 batch_limit:   8677 Loss:   0.1171 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7595 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7596 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7597 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7598 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7599 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7600 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7601 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7602 batch_limit:   8677 Loss:   0.1168 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7603 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7604 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7605 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7606 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7607 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7608 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7609 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7610 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7611 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7612 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7613 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7614 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7615 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7616 batch_limit:   8677 Loss:   0.1169 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7617 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7618 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7619 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7620 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7621 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7622 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7623 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7624 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7625 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7626 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7627 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7628 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7629 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7630 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7631 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7632 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7633 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7634 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7635 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7636 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7637 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7638 batch_limit:   8677 Loss:   0.1171 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7639 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7640 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7641 batch_limit:   8677 Loss:   0.1170 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7642 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7643 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7644 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7645 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7646 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7647 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7648 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7649 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7650 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7651 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7652 batch_limit:   8677 Loss:   0.1171 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7653 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7654 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7655 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7656 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7657 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7658 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7659 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7660 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7661 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7662 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7663 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7664 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7665 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7666 batch_limit:   8677 Loss:   0.1171 Training Acc:    99.01  6774.79    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7667 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7668 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7669 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7670 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7671 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7672 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7673 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7674 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7675 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7676 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7677 batch_limit:   8677 Loss:   0.1172 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7678 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7679 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7680 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7681 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7682 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7683 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7684 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7685 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7686 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7687 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7688 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7689 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7690 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7691 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7692 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7693 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7694 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7695 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7696 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7697 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7698 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7699 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7700 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7701 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7702 batch_limit:   8677 Loss:   0.1173 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7703 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7704 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7705 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7706 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7707 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7708 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7709 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7710 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7711 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7712 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7713 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7714 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7715 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7716 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7717 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7718 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7719 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7720 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7721 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7722 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7723 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7724 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7725 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7726 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7727 batch_limit:   8677 Loss:   0.1174 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7728 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7729 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7730 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7731 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7732 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7733 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7734 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7735 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7736 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7737 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7738 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7739 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7740 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7741 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7742 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7743 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7744 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7745 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7746 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7747 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7748 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7749 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7750 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7751 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7752 batch_limit:   8677 Loss:   0.1175 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7753 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7754 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7755 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7756 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7757 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7758 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7759 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7760 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7761 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7762 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7763 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7764 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.01    99.62%\n",
      "Epoch   7765 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7766 batch_limit:   8677 Loss:   0.1176 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7767 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7768 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7769 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7770 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7771 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7772 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7773 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7774 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7775 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7776 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7777 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7778 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7779 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7780 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7781 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7782 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7783 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7784 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7785 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7786 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7787 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7788 batch_limit:   8677 Loss:   0.1177 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7789 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7790 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7791 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7792 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7793 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7794 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7795 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7796 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7797 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7798 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7799 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7800 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7801 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7802 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7803 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7804 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7805 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7806 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7807 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7808 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7809 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7810 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7811 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7812 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7813 batch_limit:   8677 Loss:   0.1178 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7814 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7815 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7816 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7817 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7818 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7819 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7820 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7821 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7822 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7823 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7824 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7825 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7826 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7827 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7828 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7829 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7830 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7831 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7832 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7833 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7834 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7835 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7836 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7837 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7838 batch_limit:   8677 Loss:   0.1179 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7839 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7840 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7841 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7842 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7843 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7844 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7845 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7846 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7847 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7848 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7849 batch_limit:   8677 Loss:   0.1180 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7850 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7851 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7852 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7853 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7854 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7855 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7856 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7857 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7858 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7859 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7860 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7861 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7862 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7863 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7864 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7865 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7866 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7867 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7868 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7869 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7870 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7871 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7872 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7873 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7874 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7875 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7876 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7877 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7878 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7879 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7880 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7881 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7882 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7883 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7884 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7885 batch_limit:   8677 Loss:   0.1181 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7886 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7887 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7888 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7889 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7890 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7891 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7892 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7893 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7894 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7895 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7896 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7897 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7898 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7899 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7900 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7901 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7902 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7903 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7904 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7905 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7906 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7907 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7908 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7909 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7910 batch_limit:   8677 Loss:   0.1182 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7911 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7912 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7913 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7914 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7915 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7916 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7917 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7918 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7919 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7920 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7921 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7922 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7923 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7924 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7925 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7926 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7927 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7928 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7929 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7930 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7931 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7932 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7933 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7934 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7935 batch_limit:   8677 Loss:   0.1183 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7936 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7937 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7938 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7939 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7940 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7941 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7942 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7943 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7944 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7945 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7946 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7947 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7948 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7949 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7950 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7951 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7952 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7953 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7954 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7955 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7956 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7957 batch_limit:   8677 Loss:   0.1184 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7958 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7959 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7960 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7961 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7962 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7963 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7964 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7965 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7966 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7967 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7968 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7969 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7970 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7971 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7972 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7973 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7974 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7975 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7976 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7977 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7978 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7979 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7980 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7981 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7982 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7983 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7984 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7985 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7986 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7987 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7988 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7989 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7990 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7991 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7992 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7993 batch_limit:   8677 Loss:   0.1185 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7994 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7995 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7996 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7997 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7998 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   7999 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8000 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8001 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8002 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8003 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8004 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8005 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8006 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8007 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8008 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8009 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8010 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8011 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8012 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8013 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8014 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8015 batch_limit:   8677 Loss:   0.1186 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8016 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8017 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8018 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8019 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8020 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8021 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8022 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8023 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8024 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8025 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8026 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8027 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8028 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8029 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8030 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8031 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8032 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8033 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8034 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8035 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8036 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8037 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8038 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8039 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8040 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8041 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8042 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8043 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8044 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8045 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8046 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8047 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8048 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8049 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8050 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8051 batch_limit:   8677 Loss:   0.1187 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8052 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8053 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8054 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8055 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8056 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8057 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8058 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8059 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8060 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8061 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8062 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8063 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8064 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8065 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8066 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8067 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8068 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8069 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8070 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8071 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8072 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8073 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8074 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8075 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8076 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8077 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8078 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8079 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8080 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8081 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8082 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8083 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8084 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8085 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8086 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8087 batch_limit:   8677 Loss:   0.1188 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8088 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8089 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8090 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8091 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8092 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8093 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8094 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8095 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8096 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8097 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8098 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8099 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8100 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8101 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8102 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8103 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8104 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8105 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8106 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8107 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8108 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8109 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8110 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8111 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8112 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8113 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8114 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8115 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8116 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8117 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8118 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8119 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8120 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8121 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8122 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8123 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8124 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8125 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8126 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8127 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8128 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8129 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8130 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8131 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8132 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8133 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8134 batch_limit:   8677 Loss:   0.1189 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8135 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8136 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8137 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8138 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8139 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8140 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8141 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8142 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8143 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8144 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8145 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8146 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8147 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8148 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8149 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8150 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8151 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8152 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8153 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8154 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8155 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8156 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8157 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8158 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8159 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8160 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8161 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8162 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8163 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8164 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8165 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8166 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8167 batch_limit:   8677 Loss:   0.1190 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8168 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8169 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8170 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8171 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8172 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8173 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8174 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8175 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8176 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8177 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8178 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8179 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8180 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8181 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8182 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8183 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8184 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8185 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8186 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8187 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8188 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8189 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8190 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8191 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8192 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8193 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8194 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8195 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8196 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8197 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8198 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8199 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8200 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8201 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8202 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8203 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8204 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8205 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8206 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8207 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8208 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8209 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8210 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8211 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8212 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8213 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8214 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8215 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8216 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8217 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8218 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8219 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8220 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6774.79    99.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8221 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8222 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8223 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8224 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8225 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8226 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8227 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8228 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8229 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8230 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8231 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8232 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8233 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8234 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8235 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8236 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8237 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8238 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8239 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8240 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8241 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8242 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8243 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8244 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8245 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8246 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8247 batch_limit:   8677 Loss:   0.1191 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8248 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8249 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8250 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8251 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8252 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8253 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8254 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8255 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8256 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8257 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8258 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8259 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8260 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8261 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8262 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8263 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8264 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8265 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8266 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8267 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8268 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8269 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8270 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8271 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8272 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8273 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8274 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8275 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8276 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8277 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8278 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8279 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8280 batch_limit:   8677 Loss:   0.1192 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8281 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8282 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8283 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8284 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8285 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8286 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8287 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8288 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8289 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8290 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8291 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8292 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8293 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8294 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8295 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8296 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8297 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8298 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8299 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8300 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8301 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8302 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8303 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8304 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8305 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8306 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8307 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8308 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8309 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8310 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8311 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8312 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8313 batch_limit:   8677 Loss:   0.1193 Training Acc:    99.01  6775.57    99.64%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8314 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8315 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8316 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8317 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8318 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8319 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8320 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8321 batch_limit:   8677 Loss:   0.1200 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8322 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8323 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8324 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8325 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8326 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8327 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8328 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8329 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8330 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8331 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8332 batch_limit:   8677 Loss:   0.1201 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8333 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8334 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8335 batch_limit:   8677 Loss:   0.1194 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8336 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8337 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8338 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8339 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8340 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8341 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8342 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8343 batch_limit:   8677 Loss:   0.1202 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8344 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8345 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8346 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8347 batch_limit:   8677 Loss:   0.1201 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8348 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8349 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8350 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8351 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8352 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8353 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8354 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8355 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8356 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8357 batch_limit:   8677 Loss:   0.1195 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8358 batch_limit:   8677 Loss:   0.1200 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8359 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8360 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8361 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8362 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8363 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8364 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8365 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8366 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8367 batch_limit:   8677 Loss:   0.1217 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8368 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8369 batch_limit:   8677 Loss:   0.1199 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8370 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8371 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8372 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8373 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8374 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8375 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8376 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8377 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8378 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8379 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8380 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8381 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8382 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8383 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8384 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8385 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8386 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8387 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8388 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8389 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8390 batch_limit:   8677 Loss:   0.1196 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8391 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8392 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8393 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8394 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8395 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8396 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8397 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8398 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8399 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8400 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8401 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8402 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8403 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8404 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8405 batch_limit:   8677 Loss:   0.1203 Training Acc:   100.00  6776.56    99.66%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8406 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8407 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8408 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8409 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8410 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8411 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8412 batch_limit:   8677 Loss:   0.1197 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8413 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8414 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8415 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8416 batch_limit:   8677 Loss:   0.1202 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8417 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8418 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8419 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8420 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8421 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8422 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8423 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8424 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8425 batch_limit:   8677 Loss:   0.1216 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8426 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8427 batch_limit:   8677 Loss:   0.1202 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8428 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8429 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8430 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8431 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8432 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8433 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8434 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8435 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8436 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8437 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8438 batch_limit:   8677 Loss:   0.1201 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8439 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8440 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8441 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8442 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8443 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8444 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8445 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8446 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8447 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8448 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8449 batch_limit:   8677 Loss:   0.1200 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8450 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8451 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8452 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8453 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8454 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8455 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8456 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8457 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8458 batch_limit:   8677 Loss:   0.1215 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8459 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8460 batch_limit:   8677 Loss:   0.1199 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8461 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8462 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8463 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8464 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8465 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8466 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8467 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8468 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8469 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8470 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8471 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8472 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8473 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8474 batch_limit:   8677 Loss:   0.1205 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8475 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8476 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8477 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8478 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8479 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8480 batch_limit:   8677 Loss:   0.1214 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8481 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8482 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8483 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8484 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8485 batch_limit:   8677 Loss:   0.1204 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8486 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8487 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8488 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8489 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8490 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8491 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8492 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8493 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8494 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8495 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8496 batch_limit:   8677 Loss:   0.1203 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8497 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8498 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8499 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8500 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8501 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8502 batch_limit:   8677 Loss:   0.1213 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8503 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8504 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8505 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8506 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8507 batch_limit:   8677 Loss:   0.1202 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8508 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8509 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8510 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8511 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8512 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8513 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8514 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8515 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8516 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8517 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8518 batch_limit:   8677 Loss:   0.1202 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8519 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8520 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8521 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8522 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8523 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8524 batch_limit:   8677 Loss:   0.1212 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8525 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8526 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8527 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8528 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8529 batch_limit:   8677 Loss:   0.1201 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8530 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8531 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8532 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8533 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8534 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8535 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8536 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8537 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8538 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8539 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8540 batch_limit:   8677 Loss:   0.1200 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8541 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8542 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8543 batch_limit:   8677 Loss:   0.1206 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8544 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8545 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8546 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8547 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8548 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8549 batch_limit:   8677 Loss:   0.1211 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8550 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8551 batch_limit:   8677 Loss:   0.1199 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8552 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8553 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8554 batch_limit:   8677 Loss:   0.1206 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8555 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8556 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8557 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8558 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8559 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6775.78    99.64%\n",
      "Epoch   8560 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8561 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8562 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8563 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8564 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8565 batch_limit:   8677 Loss:   0.1205 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8566 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8567 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8568 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8569 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8570 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8571 batch_limit:   8677 Loss:   0.1210 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8572 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8573 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8574 batch_limit:   8677 Loss:   0.1206 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8575 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8576 batch_limit:   8677 Loss:   0.1204 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8577 batch_limit:   8677 Loss:   0.1199 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8578 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8579 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8580 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8581 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8582 batch_limit:   8677 Loss:   0.1209 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8583 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8584 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8585 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8586 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8587 batch_limit:   8677 Loss:   0.1203 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8588 batch_limit:   8677 Loss:   0.1198 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8589 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6777.34    99.67%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8590 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8591 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8592 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8593 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6774.79    99.63%\n",
      "Epoch   8594 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8595 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8596 batch_limit:   8677 Loss:   0.1205 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8597 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8598 batch_limit:   8677 Loss:   0.1202 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8599 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8600 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8601 batch_limit:   8677 Loss:   0.1207 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8602 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8603 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8604 batch_limit:   8677 Loss:   0.1208 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8605 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8606 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8607 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8608 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8609 batch_limit:   8677 Loss:   0.1201 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8610 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8611 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8612 batch_limit:   8677 Loss:   0.1206 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8613 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8614 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8615 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8616 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8617 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8618 batch_limit:   8677 Loss:   0.1204 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8619 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8620 batch_limit:   8677 Loss:   0.1200 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8621 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8622 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8623 batch_limit:   8677 Loss:   0.1205 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8624 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8625 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8626 batch_limit:   8677 Loss:   0.1207 Training Acc:    99.01  6775.57    99.64%\n",
      "Epoch   8627 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8628 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8629 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8630 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8631 batch_limit:   8677 Loss:   0.1199 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8632 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8633 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8634 batch_limit:   8677 Loss:   0.1205 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8635 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8636 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8637 batch_limit:   8677 Loss:   0.1206 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8638 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8639 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8640 batch_limit:   8677 Loss:   0.1203 Training Acc:    99.01  6776.35    99.65%\n",
      "Epoch   8641 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8642 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8643 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8644 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8645 batch_limit:   8677 Loss:   0.1204 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8646 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8647 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8648 batch_limit:   8677 Loss:   0.1205 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8649 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8650 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8651 batch_limit:   8677 Loss:   0.1202 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8652 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8653 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8654 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8655 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8656 batch_limit:   8677 Loss:   0.1203 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8657 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8658 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8659 batch_limit:   8677 Loss:   0.1204 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8660 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8661 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8662 batch_limit:   8677 Loss:   0.1201 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8663 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8664 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8665 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8666 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8667 batch_limit:   8677 Loss:   0.1202 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8668 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8669 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8670 batch_limit:   8677 Loss:   0.1204 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8671 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8672 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8673 batch_limit:   8677 Loss:   0.1200 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8674 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8675 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8676 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8677 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8678 batch_limit:   8677 Loss:   0.1201 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8679 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8680 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8681 batch_limit:   8677 Loss:   0.1203 Training Acc:   100.00  6777.34    99.67%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8682 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8683 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8684 batch_limit:   8677 Loss:   0.1200 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8685 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8686 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8687 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8688 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8689 batch_limit:   8677 Loss:   0.1200 Training Acc:   100.00  6776.56    99.66%\n",
      "Epoch   8690 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8691 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8692 batch_limit:   8677 Loss:   0.1202 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8693 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8694 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8695 batch_limit:   8677 Loss:   0.1199 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8696 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8697 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8698 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8699 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8700 batch_limit:   8677 Loss:   0.1199 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8701 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8702 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8703 batch_limit:   8677 Loss:   0.1201 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8704 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8705 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8706 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8707 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8708 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8709 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8710 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8711 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8712 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8713 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8714 batch_limit:   8677 Loss:   0.1200 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8715 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8716 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8717 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8718 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8719 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8720 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8721 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8722 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8723 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8724 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8725 batch_limit:   8677 Loss:   0.1199 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8726 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8727 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8728 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8729 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8730 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8731 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8732 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8733 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8734 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8735 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8736 batch_limit:   8677 Loss:   0.1198 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8737 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8738 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8739 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8740 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8741 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8742 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8743 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8744 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8745 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8746 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8747 batch_limit:   8677 Loss:   0.1197 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8748 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8749 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8750 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8751 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8752 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8753 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8754 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8755 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8756 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8757 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8758 batch_limit:   8677 Loss:   0.1196 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8759 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8760 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8761 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8762 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8763 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8764 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8765 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8766 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8767 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8768 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8769 batch_limit:   8677 Loss:   0.1195 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8770 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8771 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8772 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8773 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6777.34    99.67%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8774 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8775 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8776 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8777 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8778 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8779 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8780 batch_limit:   8677 Loss:   0.1194 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8781 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8782 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8783 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8784 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8785 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8786 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8787 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8788 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8789 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8790 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8791 batch_limit:   8677 Loss:   0.1193 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8792 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8793 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8794 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8795 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8796 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8797 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8798 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8799 batch_limit:   8677 Loss:   0.1190 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8800 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8801 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8802 batch_limit:   8677 Loss:   0.1192 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8803 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8804 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8805 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8806 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8807 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8808 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8809 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8810 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8811 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8812 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8813 batch_limit:   8677 Loss:   0.1191 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8814 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8815 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8816 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8817 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8818 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8819 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8820 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8821 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8822 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8823 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8824 batch_limit:   8677 Loss:   0.1189 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8825 batch_limit:   8677 Loss:   0.1164 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8826 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8827 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8828 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8829 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8830 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8831 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8832 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8833 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8834 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8835 batch_limit:   8677 Loss:   0.1188 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8836 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8837 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8838 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8839 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8840 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8841 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8842 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8843 batch_limit:   8677 Loss:   0.1186 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8844 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8845 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8846 batch_limit:   8677 Loss:   0.1187 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8847 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8848 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8849 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8850 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8851 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8852 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8853 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8854 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8855 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8856 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8857 batch_limit:   8677 Loss:   0.1185 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8858 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8859 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8860 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8861 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8862 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8863 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8864 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8865 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6778.91    99.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8866 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8867 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8868 batch_limit:   8677 Loss:   0.1184 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8869 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8870 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8871 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8872 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8873 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8874 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8875 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8876 batch_limit:   8677 Loss:   0.1182 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8877 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8878 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8879 batch_limit:   8677 Loss:   0.1183 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8880 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8881 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8882 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8883 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8884 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8885 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8886 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8887 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8888 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8889 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8890 batch_limit:   8677 Loss:   0.1181 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8891 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8892 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8893 batch_limit:   8677 Loss:   0.1177 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8894 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8895 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8896 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8897 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8898 batch_limit:   8677 Loss:   0.1179 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8899 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8900 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8901 batch_limit:   8677 Loss:   0.1180 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8902 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8903 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8904 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8905 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8906 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8907 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8908 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8909 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8910 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8911 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8912 batch_limit:   8677 Loss:   0.1178 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8913 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8914 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8915 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8916 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8917 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8918 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8919 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8920 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8921 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8922 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8923 batch_limit:   8677 Loss:   0.1176 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8924 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8925 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8926 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8927 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8928 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8929 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6777.34    99.67%\n",
      "Epoch   8930 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8931 batch_limit:   8677 Loss:   0.1175 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8932 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8933 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8934 batch_limit:   8677 Loss:   0.1174 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8935 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8936 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8937 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8938 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8939 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8940 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8941 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8942 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8943 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8944 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8945 batch_limit:   8677 Loss:   0.1173 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8946 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8947 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8948 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8949 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8950 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8951 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8952 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8953 batch_limit:   8677 Loss:   0.1172 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8954 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8955 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8956 batch_limit:   8677 Loss:   0.1171 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8957 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6779.69    99.70%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8958 batch_limit:   8677 Loss:   0.1159 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8959 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8960 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8961 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8962 batch_limit:   8677 Loss:   0.1157 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8963 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8964 batch_limit:   8677 Loss:   0.1170 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8965 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8966 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8967 batch_limit:   8677 Loss:   0.1169 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8968 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8969 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8970 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8971 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8972 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8973 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8974 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8975 batch_limit:   8677 Loss:   0.1168 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8976 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8977 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8978 batch_limit:   8677 Loss:   0.1167 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8979 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8980 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8981 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8982 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8983 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8984 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8985 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8986 batch_limit:   8677 Loss:   0.1166 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8987 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8988 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8989 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8990 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8991 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8992 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   8993 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8994 batch_limit:   8677 Loss:   0.1161 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8995 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8996 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   8997 batch_limit:   8677 Loss:   0.1165 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   8998 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   8999 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   9000 batch_limit:   8677 Loss:   0.1162 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   9001 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9002 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9003 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9004 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9005 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9006 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9007 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9008 batch_limit:   8677 Loss:   0.1163 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9009 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9010 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9011 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   9012 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9013 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9014 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9015 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9016 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9017 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9018 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9019 batch_limit:   8677 Loss:   0.1160 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9020 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9021 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9022 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   9023 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9024 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9025 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9026 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9027 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9028 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9029 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9030 batch_limit:   8677 Loss:   0.1158 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9031 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9032 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9033 batch_limit:   8677 Loss:   0.1155 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   9034 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9035 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9036 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9037 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9038 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9039 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9040 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9041 batch_limit:   8677 Loss:   0.1156 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9042 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9043 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9044 batch_limit:   8677 Loss:   0.1153 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   9045 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9046 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9047 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9048 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9049 batch_limit:   8677 Loss:   0.1152 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9050 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6779.69    99.70%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9051 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9052 batch_limit:   8677 Loss:   0.1154 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9053 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9054 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9055 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6778.12    99.68%\n",
      "Epoch   9056 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9057 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9058 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9059 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9060 batch_limit:   8677 Loss:   0.1150 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9061 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9062 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9063 batch_limit:   8677 Loss:   0.1151 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9064 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9065 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9066 batch_limit:   8677 Loss:   0.1147 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9067 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9068 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9069 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9070 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9071 batch_limit:   8677 Loss:   0.1148 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9072 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9073 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9074 batch_limit:   8677 Loss:   0.1149 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9075 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9076 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9077 batch_limit:   8677 Loss:   0.1144 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9078 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9079 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9080 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9081 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9082 batch_limit:   8677 Loss:   0.1145 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9083 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9084 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9085 batch_limit:   8677 Loss:   0.1146 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9086 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9087 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9088 batch_limit:   8677 Loss:   0.1142 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9089 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9090 batch_limit:   8677 Loss:   0.1137 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9091 batch_limit:   8677 Loss:   0.1134 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9092 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9093 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9094 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9095 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9096 batch_limit:   8677 Loss:   0.1143 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9097 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9098 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9099 batch_limit:   8677 Loss:   0.1139 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9100 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9101 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9102 batch_limit:   8677 Loss:   0.1131 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9103 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9104 batch_limit:   8677 Loss:   0.1140 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9105 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9106 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9107 batch_limit:   8677 Loss:   0.1141 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9108 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9109 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9110 batch_limit:   8677 Loss:   0.1136 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9111 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9112 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9113 batch_limit:   8677 Loss:   0.1128 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9114 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9115 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9116 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9117 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9118 batch_limit:   8677 Loss:   0.1138 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9119 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9120 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9121 batch_limit:   8677 Loss:   0.1133 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9122 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9123 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9124 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9125 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9126 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6778.91    99.69%\n",
      "Epoch   9127 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9128 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9129 batch_limit:   8677 Loss:   0.1135 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9130 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9131 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9132 batch_limit:   8677 Loss:   0.1130 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9133 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9134 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9135 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9136 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9137 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9138 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9139 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9140 batch_limit:   8677 Loss:   0.1132 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9141 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9142 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9143 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6780.47    99.71%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9144 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9145 batch_limit:   8677 Loss:   0.1125 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9146 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9147 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9148 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9149 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9150 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9151 batch_limit:   8677 Loss:   0.1129 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9152 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9153 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9154 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9155 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9156 batch_limit:   8677 Loss:   0.1122 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9157 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9158 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9159 batch_limit:   8677 Loss:   0.1127 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9160 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9161 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9162 batch_limit:   8677 Loss:   0.1126 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9163 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9164 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9165 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9166 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9167 batch_limit:   8677 Loss:   0.1119 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9168 batch_limit:   8677 Loss:   0.1112 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9169 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9170 batch_limit:   8677 Loss:   0.1124 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9171 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9172 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9173 batch_limit:   8677 Loss:   0.1123 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9174 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9175 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9176 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9177 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9178 batch_limit:   8677 Loss:   0.1116 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9179 batch_limit:   8677 Loss:   0.1109 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9180 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9181 batch_limit:   8677 Loss:   0.1121 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9182 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9183 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9184 batch_limit:   8677 Loss:   0.1120 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9185 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9186 batch_limit:   8677 Loss:   0.1106 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9187 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9188 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9189 batch_limit:   8677 Loss:   0.1113 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9190 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9191 batch_limit:   8677 Loss:   0.1091 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9192 batch_limit:   8677 Loss:   0.1118 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9193 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9194 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9195 batch_limit:   8677 Loss:   0.1117 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9196 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9197 batch_limit:   8677 Loss:   0.1103 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9198 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9199 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9200 batch_limit:   8677 Loss:   0.1110 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9201 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9202 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9203 batch_limit:   8677 Loss:   0.1115 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9204 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9205 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9206 batch_limit:   8677 Loss:   0.1114 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9207 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9208 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9209 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9210 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9211 batch_limit:   8677 Loss:   0.1107 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9212 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9213 batch_limit:   8677 Loss:   0.1084 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9214 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9215 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9216 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9217 batch_limit:   8677 Loss:   0.1111 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9218 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9219 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9220 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9221 batch_limit:   8677 Loss:   0.1080 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9222 batch_limit:   8677 Loss:   0.1104 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9223 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9224 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9225 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9226 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9227 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9228 batch_limit:   8677 Loss:   0.1108 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9229 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9230 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9231 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9232 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9233 batch_limit:   8677 Loss:   0.1100 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9234 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9235 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6780.47    99.71%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9236 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9237 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9238 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9239 batch_limit:   8677 Loss:   0.1105 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9240 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9241 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9242 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9243 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9244 batch_limit:   8677 Loss:   0.1097 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9245 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9246 batch_limit:   8677 Loss:   0.1075 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9247 batch_limit:   8677 Loss:   0.1102 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9248 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9249 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9250 batch_limit:   8677 Loss:   0.1101 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9251 batch_limit:   8677 Loss:   0.1073 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9252 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9253 batch_limit:   8677 Loss:   0.1096 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9254 batch_limit:   8677 Loss:   0.1071 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9255 batch_limit:   8677 Loss:   0.1094 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9256 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9257 batch_limit:   8677 Loss:   0.1071 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9258 batch_limit:   8677 Loss:   0.1099 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9259 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9260 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9261 batch_limit:   8677 Loss:   0.1098 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9262 batch_limit:   8677 Loss:   0.1070 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9263 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9264 batch_limit:   8677 Loss:   0.1093 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9265 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9266 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9267 batch_limit:   8677 Loss:   0.1084 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9268 batch_limit:   8677 Loss:   0.1068 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9269 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9270 batch_limit:   8677 Loss:   0.1075 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9271 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9272 batch_limit:   8677 Loss:   0.1095 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9273 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9274 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9275 batch_limit:   8677 Loss:   0.1090 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9276 batch_limit:   8677 Loss:   0.1064 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9277 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9278 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9279 batch_limit:   8677 Loss:   0.1065 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9280 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9281 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9282 batch_limit:   8677 Loss:   0.1069 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9283 batch_limit:   8677 Loss:   0.1092 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9284 batch_limit:   8677 Loss:   0.1064 Training Acc:   100.00  6779.69    99.70%\n",
      "Epoch   9285 batch_limit:   8677 Loss:   0.1076 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9286 batch_limit:   8677 Loss:   0.1087 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9287 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9288 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9289 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9290 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9291 batch_limit:   8677 Loss:   0.1088 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9292 batch_limit:   8677 Loss:   0.1070 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9293 batch_limit:   8677 Loss:   0.1065 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9294 batch_limit:   8677 Loss:   0.1089 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9295 batch_limit:   8677 Loss:   0.1062 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9296 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9297 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9298 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9299 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9300 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9301 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9302 batch_limit:   8677 Loss:   0.1085 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9303 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9304 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9305 batch_limit:   8677 Loss:   0.1086 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9306 batch_limit:   8677 Loss:   0.1059 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9307 batch_limit:   8677 Loss:   0.1068 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9308 batch_limit:   8677 Loss:   0.1082 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9309 batch_limit:   8677 Loss:   0.1055 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9310 batch_limit:   8677 Loss:   0.1075 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9311 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9312 batch_limit:   8677 Loss:   0.1055 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9313 batch_limit:   8677 Loss:   0.1081 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9314 batch_limit:   8677 Loss:   0.1065 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9315 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9316 batch_limit:   8677 Loss:   0.1083 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9317 batch_limit:   8677 Loss:   0.1056 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9318 batch_limit:   8677 Loss:   0.1064 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9319 batch_limit:   8677 Loss:   0.1079 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9320 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9321 batch_limit:   8677 Loss:   0.1071 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9322 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9323 batch_limit:   8677 Loss:   0.1051 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9324 batch_limit:   8677 Loss:   0.1078 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9325 batch_limit:   8677 Loss:   0.1063 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9326 batch_limit:   8677 Loss:   0.1054 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9327 batch_limit:   8677 Loss:   0.1080 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9328 batch_limit:   8677 Loss:   0.1054 Training Acc:   100.00  6780.47    99.71%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9329 batch_limit:   8677 Loss:   0.1060 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9330 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9331 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9332 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9333 batch_limit:   8677 Loss:   0.1069 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9334 batch_limit:   8677 Loss:   0.1048 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9335 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9336 batch_limit:   8677 Loss:   0.1060 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9337 batch_limit:   8677 Loss:   0.1050 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9338 batch_limit:   8677 Loss:   0.1077 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9339 batch_limit:   8677 Loss:   0.1051 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9340 batch_limit:   8677 Loss:   0.1056 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9341 batch_limit:   8677 Loss:   0.1074 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9342 batch_limit:   8677 Loss:   0.1046 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9343 batch_limit:   8677 Loss:   0.1063 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9344 batch_limit:   8677 Loss:   0.1067 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9345 batch_limit:   8677 Loss:   0.1045 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9346 batch_limit:   8677 Loss:   0.1070 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9347 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9348 batch_limit:   8677 Loss:   0.1046 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9349 batch_limit:   8677 Loss:   0.1073 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9350 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9351 batch_limit:   8677 Loss:   0.1051 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9352 batch_limit:   8677 Loss:   0.1072 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9353 batch_limit:   8677 Loss:   0.1043 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9354 batch_limit:   8677 Loss:   0.1059 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9355 batch_limit:   8677 Loss:   0.1065 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9356 batch_limit:   8677 Loss:   0.1041 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9357 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9358 batch_limit:   8677 Loss:   0.1057 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9359 batch_limit:   8677 Loss:   0.1043 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9360 batch_limit:   8677 Loss:   0.1070 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9361 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9362 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9363 batch_limit:   8677 Loss:   0.1069 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9364 batch_limit:   8677 Loss:   0.1041 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9365 batch_limit:   8677 Loss:   0.1054 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9366 batch_limit:   8677 Loss:   0.1063 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9367 batch_limit:   8677 Loss:   0.1038 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9368 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9369 batch_limit:   8677 Loss:   0.1055 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9370 batch_limit:   8677 Loss:   0.1039 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9371 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9372 batch_limit:   8677 Loss:   0.1046 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9373 batch_limit:   8677 Loss:   0.1043 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9374 batch_limit:   8677 Loss:   0.1066 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9375 batch_limit:   8677 Loss:   0.1038 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9376 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9377 batch_limit:   8677 Loss:   0.1061 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9378 batch_limit:   8677 Loss:   0.1035 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9379 batch_limit:   8677 Loss:   0.1057 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9380 batch_limit:   8677 Loss:   0.1053 Training Acc:   100.00  6780.47    99.71%\n",
      "Epoch   9381 batch_limit:   8677 Loss:   0.1035 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9382 batch_limit:   8677 Loss:   0.1062 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9383 batch_limit:   8677 Loss:   0.1044 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9384 batch_limit:   8677 Loss:   0.1038 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9385 batch_limit:   8677 Loss:   0.1063 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9386 batch_limit:   8677 Loss:   0.1036 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9387 batch_limit:   8677 Loss:   0.1044 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9388 batch_limit:   8677 Loss:   0.1059 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9389 batch_limit:   8677 Loss:   0.1032 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9390 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9391 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9392 batch_limit:   8677 Loss:   0.1031 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9393 batch_limit:   8677 Loss:   0.1058 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9394 batch_limit:   8677 Loss:   0.1043 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9395 batch_limit:   8677 Loss:   0.1034 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9396 batch_limit:   8677 Loss:   0.1060 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9397 batch_limit:   8677 Loss:   0.1034 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9398 batch_limit:   8677 Loss:   0.1039 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9399 batch_limit:   8677 Loss:   0.1057 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9400 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9401 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9402 batch_limit:   8677 Loss:   0.1050 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9403 batch_limit:   8677 Loss:   0.1028 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9404 batch_limit:   8677 Loss:   0.1053 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9405 batch_limit:   8677 Loss:   0.1042 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9406 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9407 batch_limit:   8677 Loss:   0.1056 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9408 batch_limit:   8677 Loss:   0.1033 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9409 batch_limit:   8677 Loss:   0.1034 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9410 batch_limit:   8677 Loss:   0.1055 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9411 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9412 batch_limit:   8677 Loss:   0.1041 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9413 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9414 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9415 batch_limit:   8677 Loss:   0.1048 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9416 batch_limit:   8677 Loss:   0.1041 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9417 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9418 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9419 batch_limit:   8677 Loss:   0.1032 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9420 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9421 batch_limit:   8677 Loss:   0.1052 Training Acc:   100.00  6781.25    99.72%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9422 batch_limit:   8677 Loss:   0.1024 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9423 batch_limit:   8677 Loss:   0.1035 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9424 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9425 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9426 batch_limit:   8677 Loss:   0.1042 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9427 batch_limit:   8677 Loss:   0.1040 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9428 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9429 batch_limit:   8677 Loss:   0.1047 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9430 batch_limit:   8677 Loss:   0.1031 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9431 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9432 batch_limit:   8677 Loss:   0.1049 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9433 batch_limit:   8677 Loss:   0.1023 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9434 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9435 batch_limit:   8677 Loss:   0.1045 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9436 batch_limit:   8677 Loss:   0.1018 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9437 batch_limit:   8677 Loss:   0.1036 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9438 batch_limit:   8677 Loss:   0.1039 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9439 batch_limit:   8677 Loss:   0.1017 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9440 batch_limit:   8677 Loss:   0.1042 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9441 batch_limit:   8677 Loss:   0.1030 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9442 batch_limit:   8677 Loss:   0.1018 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9443 batch_limit:   8677 Loss:   0.1045 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9444 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9445 batch_limit:   8677 Loss:   0.1022 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9446 batch_limit:   8677 Loss:   0.1043 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9447 batch_limit:   8677 Loss:   0.1015 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9448 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9449 batch_limit:   8677 Loss:   0.1038 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9450 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9451 batch_limit:   8677 Loss:   0.1036 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9452 batch_limit:   8677 Loss:   0.1030 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9453 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9454 batch_limit:   8677 Loss:   0.1040 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9455 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9456 batch_limit:   8677 Loss:   0.1016 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9457 batch_limit:   8677 Loss:   0.1041 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9458 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9459 batch_limit:   8677 Loss:   0.1022 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9460 batch_limit:   8677 Loss:   0.1036 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9461 batch_limit:   8677 Loss:   0.1009 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9462 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9463 batch_limit:   8677 Loss:   0.1029 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9464 batch_limit:   8677 Loss:   0.1008 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9465 batch_limit:   8677 Loss:   0.1034 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9466 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9467 batch_limit:   8677 Loss:   0.1010 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9468 batch_limit:   8677 Loss:   0.1037 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9469 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9470 batch_limit:   8677 Loss:   0.1014 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9471 batch_limit:   8677 Loss:   0.1034 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9472 batch_limit:   8677 Loss:   0.1007 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9473 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9474 batch_limit:   8677 Loss:   0.1028 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9475 batch_limit:   8677 Loss:   0.1004 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9476 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9477 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9478 batch_limit:   8677 Loss:   0.1004 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9479 batch_limit:   8677 Loss:   0.1031 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9480 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9481 batch_limit:   8677 Loss:   0.1007 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9482 batch_limit:   8677 Loss:   0.1032 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9483 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9484 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9485 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9486 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9487 batch_limit:   8677 Loss:   0.1019 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9488 batch_limit:   8677 Loss:   0.1020 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9489 batch_limit:   8677 Loss:   0.0999 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9490 batch_limit:   8677 Loss:   0.1025 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9491 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9492 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9493 batch_limit:   8677 Loss:   0.1027 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9494 batch_limit:   8677 Loss:   0.1004 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9495 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9496 batch_limit:   8677 Loss:   0.1026 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9497 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9498 batch_limit:   8677 Loss:   0.1011 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9499 batch_limit:   8677 Loss:   0.1020 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9500 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9501 batch_limit:   8677 Loss:   0.1017 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9502 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9503 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9504 batch_limit:   8677 Loss:   0.1021 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9505 batch_limit:   8677 Loss:   0.1004 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9506 batch_limit:   8677 Loss:   0.0997 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9507 batch_limit:   8677 Loss:   0.1022 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9508 batch_limit:   8677 Loss:   0.0996 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9509 batch_limit:   8677 Loss:   0.1002 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9510 batch_limit:   8677 Loss:   0.1019 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9511 batch_limit:   8677 Loss:   0.0992 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9512 batch_limit:   8677 Loss:   0.1008 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9513 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9514 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6782.03    99.74%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9515 batch_limit:   8677 Loss:   0.1013 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9516 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9517 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9518 batch_limit:   8677 Loss:   0.1017 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9519 batch_limit:   8677 Loss:   0.0996 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9520 batch_limit:   8677 Loss:   0.0993 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9521 batch_limit:   8677 Loss:   0.1017 Training Acc:   100.00  6781.25    99.72%\n",
      "Epoch   9522 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9523 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9524 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9525 batch_limit:   8677 Loss:   0.0986 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9526 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9527 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9528 batch_limit:   8677 Loss:   0.0985 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9529 batch_limit:   8677 Loss:   0.1010 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9530 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9531 batch_limit:   8677 Loss:   0.0986 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9532 batch_limit:   8677 Loss:   0.1012 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9533 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9534 batch_limit:   8677 Loss:   0.0989 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9535 batch_limit:   8677 Loss:   0.1011 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9536 batch_limit:   8677 Loss:   0.0984 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9537 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9538 batch_limit:   8677 Loss:   0.1006 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9539 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9540 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9541 batch_limit:   8677 Loss:   0.0999 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9542 batch_limit:   8677 Loss:   0.0980 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9543 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9544 batch_limit:   8677 Loss:   0.0991 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9545 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9546 batch_limit:   8677 Loss:   0.1007 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9547 batch_limit:   8677 Loss:   0.0983 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9548 batch_limit:   8677 Loss:   0.0985 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9549 batch_limit:   8677 Loss:   0.1005 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9550 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9551 batch_limit:   8677 Loss:   0.0991 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9552 batch_limit:   8677 Loss:   0.1000 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9553 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9554 batch_limit:   8677 Loss:   0.0997 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9555 batch_limit:   8677 Loss:   0.0993 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9556 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9557 batch_limit:   8677 Loss:   0.1001 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9558 batch_limit:   8677 Loss:   0.0985 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9559 batch_limit:   8677 Loss:   0.0977 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9560 batch_limit:   8677 Loss:   0.1002 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9561 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9562 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9563 batch_limit:   8677 Loss:   0.1000 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9564 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9565 batch_limit:   8677 Loss:   0.0987 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9566 batch_limit:   8677 Loss:   0.0994 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9567 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9568 batch_limit:   8677 Loss:   0.0992 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9569 batch_limit:   8677 Loss:   0.0987 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9570 batch_limit:   8677 Loss:   0.0970 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9571 batch_limit:   8677 Loss:   0.0996 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9572 batch_limit:   8677 Loss:   0.0979 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9573 batch_limit:   8677 Loss:   0.0972 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9574 batch_limit:   8677 Loss:   0.0998 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9575 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9576 batch_limit:   8677 Loss:   0.0976 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9577 batch_limit:   8677 Loss:   0.0995 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9578 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9579 batch_limit:   8677 Loss:   0.0982 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9580 batch_limit:   8677 Loss:   0.0989 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9581 batch_limit:   8677 Loss:   0.0966 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9582 batch_limit:   8677 Loss:   0.0988 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9583 batch_limit:   8677 Loss:   0.0982 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9584 batch_limit:   8677 Loss:   0.0966 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9585 batch_limit:   8677 Loss:   0.0992 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9586 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9587 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9588 batch_limit:   8677 Loss:   0.0993 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9589 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9590 batch_limit:   8677 Loss:   0.0972 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9591 batch_limit:   8677 Loss:   0.0990 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9592 batch_limit:   8677 Loss:   0.0964 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9593 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9594 batch_limit:   8677 Loss:   0.0985 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9595 batch_limit:   8677 Loss:   0.0962 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9596 batch_limit:   8677 Loss:   0.0983 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9597 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9598 batch_limit:   8677 Loss:   0.0961 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9599 batch_limit:   8677 Loss:   0.0987 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9600 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9601 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9602 batch_limit:   8677 Loss:   0.0989 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9603 batch_limit:   8677 Loss:   0.0965 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9604 batch_limit:   8677 Loss:   0.0967 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9605 batch_limit:   8677 Loss:   0.0986 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9606 batch_limit:   8677 Loss:   0.0960 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9607 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6783.59    99.76%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9608 batch_limit:   8677 Loss:   0.0982 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9609 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9610 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9611 batch_limit:   8677 Loss:   0.0975 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9612 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9613 batch_limit:   8677 Loss:   0.0983 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9614 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9615 batch_limit:   8677 Loss:   0.0959 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9616 batch_limit:   8677 Loss:   0.0985 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9617 batch_limit:   8677 Loss:   0.0962 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9618 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9619 batch_limit:   8677 Loss:   0.0983 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9620 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9621 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9622 batch_limit:   8677 Loss:   0.0979 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9623 batch_limit:   8677 Loss:   0.0954 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9624 batch_limit:   8677 Loss:   0.0974 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9625 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9626 batch_limit:   8677 Loss:   0.0954 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9627 batch_limit:   8677 Loss:   0.0978 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9628 batch_limit:   8677 Loss:   0.0966 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9629 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9630 batch_limit:   8677 Loss:   0.0981 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9631 batch_limit:   8677 Loss:   0.0959 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9632 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9633 batch_limit:   8677 Loss:   0.0980 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9634 batch_limit:   8677 Loss:   0.0954 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9635 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9636 batch_limit:   8677 Loss:   0.0976 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9637 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9638 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9639 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6782.03    99.74%\n",
      "Epoch   9640 batch_limit:   8677 Loss:   0.0950 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9641 batch_limit:   8677 Loss:   0.0974 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9642 batch_limit:   8677 Loss:   0.0965 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9643 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9644 batch_limit:   8677 Loss:   0.0977 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9645 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9646 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9647 batch_limit:   8677 Loss:   0.0977 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9648 batch_limit:   8677 Loss:   0.0952 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9649 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9650 batch_limit:   8677 Loss:   0.0974 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9651 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9652 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9653 batch_limit:   8677 Loss:   0.0970 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9654 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9655 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9656 batch_limit:   8677 Loss:   0.0964 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9657 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9658 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9659 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9660 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9661 batch_limit:   8677 Loss:   0.0974 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9662 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9663 batch_limit:   8677 Loss:   0.0952 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9664 batch_limit:   8677 Loss:   0.0973 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9665 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9666 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9667 batch_limit:   8677 Loss:   0.0969 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9668 batch_limit:   8677 Loss:   0.0945 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9669 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9670 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9671 batch_limit:   8677 Loss:   0.0944 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9672 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9673 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9674 batch_limit:   8677 Loss:   0.0944 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9675 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9676 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9677 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9678 batch_limit:   8677 Loss:   0.0971 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9679 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9680 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9681 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9682 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9683 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9684 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9685 batch_limit:   8677 Loss:   0.0941 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9686 batch_limit:   8677 Loss:   0.0962 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9687 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9688 batch_limit:   8677 Loss:   0.0941 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9689 batch_limit:   8677 Loss:   0.0966 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9690 batch_limit:   8677 Loss:   0.0952 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9691 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9692 batch_limit:   8677 Loss:   0.0968 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9693 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9694 batch_limit:   8677 Loss:   0.0945 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9695 batch_limit:   8677 Loss:   0.0967 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9696 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9697 batch_limit:   8677 Loss:   0.0950 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9698 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9699 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6783.59    99.76%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9700 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9701 batch_limit:   8677 Loss:   0.0959 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9702 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9703 batch_limit:   8677 Loss:   0.0960 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9704 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9705 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9706 batch_limit:   8677 Loss:   0.0964 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9707 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9708 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9709 batch_limit:   8677 Loss:   0.0965 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9710 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9711 batch_limit:   8677 Loss:   0.0944 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9712 batch_limit:   8677 Loss:   0.0963 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9713 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9714 batch_limit:   8677 Loss:   0.0948 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9715 batch_limit:   8677 Loss:   0.0959 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9716 batch_limit:   8677 Loss:   0.0936 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9717 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9718 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9719 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9720 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9721 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9722 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9723 batch_limit:   8677 Loss:   0.0961 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9724 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9725 batch_limit:   8677 Loss:   0.0937 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9726 batch_limit:   8677 Loss:   0.0962 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9727 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6782.81    99.75%\n",
      "Epoch   9728 batch_limit:   8677 Loss:   0.0941 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9729 batch_limit:   8677 Loss:   0.0960 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9730 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9731 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9732 batch_limit:   8677 Loss:   0.0956 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9733 batch_limit:   8677 Loss:   0.0933 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9734 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9735 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9736 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9737 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9738 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9739 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9740 batch_limit:   8677 Loss:   0.0958 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9741 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9742 batch_limit:   8677 Loss:   0.0934 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9743 batch_limit:   8677 Loss:   0.0959 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9744 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9745 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9746 batch_limit:   8677 Loss:   0.0957 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9747 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9748 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9749 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9750 batch_limit:   8677 Loss:   0.0930 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9751 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9752 batch_limit:   8677 Loss:   0.0948 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9753 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9754 batch_limit:   8677 Loss:   0.0952 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9755 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9756 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9757 batch_limit:   8677 Loss:   0.0955 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9758 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9759 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9760 batch_limit:   8677 Loss:   0.0956 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9761 batch_limit:   8677 Loss:   0.0933 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9762 batch_limit:   8677 Loss:   0.0934 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9763 batch_limit:   8677 Loss:   0.0954 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9764 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9765 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9766 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9767 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9768 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9769 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9770 batch_limit:   8677 Loss:   0.0926 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9771 batch_limit:   8677 Loss:   0.0948 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9772 batch_limit:   8677 Loss:   0.0941 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9773 batch_limit:   8677 Loss:   0.0926 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9774 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9775 batch_limit:   8677 Loss:   0.0936 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9776 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9777 batch_limit:   8677 Loss:   0.0953 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9778 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6783.59    99.76%\n",
      "Epoch   9779 batch_limit:   8677 Loss:   0.0930 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9780 batch_limit:   8677 Loss:   0.0951 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9781 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9782 batch_limit:   8677 Loss:   0.0934 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9783 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9784 batch_limit:   8677 Loss:   0.0925 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9785 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9786 batch_limit:   8677 Loss:   0.0945 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9787 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9788 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9789 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9790 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9791 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9792 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6784.38    99.77%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9793 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9794 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9795 batch_limit:   8677 Loss:   0.0930 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9796 batch_limit:   8677 Loss:   0.0926 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9797 batch_limit:   8677 Loss:   0.0949 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9798 batch_limit:   8677 Loss:   0.0926 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9799 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9800 batch_limit:   8677 Loss:   0.0947 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9801 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9802 batch_limit:   8677 Loss:   0.0933 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9803 batch_limit:   8677 Loss:   0.0944 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9804 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9805 batch_limit:   8677 Loss:   0.0937 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9806 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9807 batch_limit:   8677 Loss:   0.0920 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9808 batch_limit:   8677 Loss:   0.0941 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9809 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9810 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9811 batch_limit:   8677 Loss:   0.0945 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9812 batch_limit:   8677 Loss:   0.0930 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9813 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9814 batch_limit:   8677 Loss:   0.0946 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9815 batch_limit:   8677 Loss:   0.0925 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9816 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9817 batch_limit:   8677 Loss:   0.0945 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9818 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9819 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9820 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9821 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9822 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9823 batch_limit:   8677 Loss:   0.0940 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9824 batch_limit:   8677 Loss:   0.0918 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9825 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9826 batch_limit:   8677 Loss:   0.0936 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9827 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9828 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9829 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9830 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9831 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9832 batch_limit:   8677 Loss:   0.0926 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9833 batch_limit:   8677 Loss:   0.0918 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9834 batch_limit:   8677 Loss:   0.0943 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9835 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9836 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9837 batch_limit:   8677 Loss:   0.0942 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9838 batch_limit:   8677 Loss:   0.0918 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9839 batch_limit:   8677 Loss:   0.0924 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9840 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9841 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9842 batch_limit:   8677 Loss:   0.0928 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9843 batch_limit:   8677 Loss:   0.0936 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9844 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9845 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9846 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9847 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9848 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9849 batch_limit:   8677 Loss:   0.0928 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9850 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9851 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9852 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9853 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9854 batch_limit:   8677 Loss:   0.0939 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9855 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9856 batch_limit:   8677 Loss:   0.0917 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9857 batch_limit:   8677 Loss:   0.0938 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9858 batch_limit:   8677 Loss:   0.0915 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9859 batch_limit:   8677 Loss:   0.0920 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9860 batch_limit:   8677 Loss:   0.0936 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9861 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9862 batch_limit:   8677 Loss:   0.0924 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9863 batch_limit:   8677 Loss:   0.0933 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9864 batch_limit:   8677 Loss:   0.0911 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9865 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9866 batch_limit:   8677 Loss:   0.0930 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9867 batch_limit:   8677 Loss:   0.0910 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9868 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9869 batch_limit:   8677 Loss:   0.0926 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9870 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9871 batch_limit:   8677 Loss:   0.0934 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9872 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9873 batch_limit:   8677 Loss:   0.0910 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9874 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9875 batch_limit:   8677 Loss:   0.0917 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9876 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9877 batch_limit:   8677 Loss:   0.0935 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9878 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9879 batch_limit:   8677 Loss:   0.0915 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9880 batch_limit:   8677 Loss:   0.0934 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9881 batch_limit:   8677 Loss:   0.0911 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9882 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9883 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9884 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9885 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6786.72    99.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9886 batch_limit:   8677 Loss:   0.0928 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9887 batch_limit:   8677 Loss:   0.0907 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9888 batch_limit:   8677 Loss:   0.0925 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9889 batch_limit:   8677 Loss:   0.0925 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9890 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9891 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9892 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9893 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9894 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9895 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9896 batch_limit:   8677 Loss:   0.0907 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9897 batch_limit:   8677 Loss:   0.0932 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9898 batch_limit:   8677 Loss:   0.0912 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9899 batch_limit:   8677 Loss:   0.0910 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9900 batch_limit:   8677 Loss:   0.0931 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9901 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9902 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9903 batch_limit:   8677 Loss:   0.0929 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9904 batch_limit:   8677 Loss:   0.0907 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9905 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9906 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9907 batch_limit:   8677 Loss:   0.0905 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9908 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9909 batch_limit:   8677 Loss:   0.0924 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9910 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9911 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9912 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9913 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9914 batch_limit:   8677 Loss:   0.0925 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9915 batch_limit:   8677 Loss:   0.0917 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9916 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9917 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9918 batch_limit:   8677 Loss:   0.0912 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9919 batch_limit:   8677 Loss:   0.0904 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9920 batch_limit:   8677 Loss:   0.0928 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9921 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9922 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9923 batch_limit:   8677 Loss:   0.0927 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9924 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9925 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9926 batch_limit:   8677 Loss:   0.0925 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9927 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9928 batch_limit:   8677 Loss:   0.0911 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9929 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9930 batch_limit:   8677 Loss:   0.0901 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9931 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9932 batch_limit:   8677 Loss:   0.0921 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9933 batch_limit:   8677 Loss:   0.0900 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9934 batch_limit:   8677 Loss:   0.0917 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9935 batch_limit:   8677 Loss:   0.0918 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9936 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9937 batch_limit:   8677 Loss:   0.0920 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9938 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9939 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9940 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9941 batch_limit:   8677 Loss:   0.0910 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9942 batch_limit:   8677 Loss:   0.0899 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9943 batch_limit:   8677 Loss:   0.0924 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9944 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9945 batch_limit:   8677 Loss:   0.0901 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9946 batch_limit:   8677 Loss:   0.0923 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9947 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9948 batch_limit:   8677 Loss:   0.0904 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9949 batch_limit:   8677 Loss:   0.0922 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9950 batch_limit:   8677 Loss:   0.0901 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9951 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9952 batch_limit:   8677 Loss:   0.0920 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch   9953 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9954 batch_limit:   8677 Loss:   0.0908 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9955 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9956 batch_limit:   8677 Loss:   0.0897 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9957 batch_limit:   8677 Loss:   0.0911 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9958 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9959 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9960 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9961 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9962 batch_limit:   8677 Loss:   0.0894 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9963 batch_limit:   8677 Loss:   0.0917 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9964 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9965 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9966 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9967 batch_limit:   8677 Loss:   0.0905 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9968 batch_limit:   8677 Loss:   0.0896 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch   9969 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9970 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9971 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9972 batch_limit:   8677 Loss:   0.0919 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9973 batch_limit:   8677 Loss:   0.0899 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9974 batch_limit:   8677 Loss:   0.0900 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9975 batch_limit:   8677 Loss:   0.0918 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch   9976 batch_limit:   8677 Loss:   0.0897 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9977 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6785.94    99.79%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9978 batch_limit:   8677 Loss:   0.0916 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch   9979 batch_limit:   8677 Loss:   0.0894 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9980 batch_limit:   8677 Loss:   0.0904 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9981 batch_limit:   8677 Loss:   0.0915 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9982 batch_limit:   8677 Loss:   0.0893 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9983 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9984 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9985 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9986 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9987 batch_limit:   8677 Loss:   0.0910 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9988 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9989 batch_limit:   8677 Loss:   0.0911 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9990 batch_limit:   8677 Loss:   0.0906 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9991 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9992 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9993 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9994 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9995 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9996 batch_limit:   8677 Loss:   0.0899 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch   9997 batch_limit:   8677 Loss:   0.0893 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch   9998 batch_limit:   8677 Loss:   0.0914 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch   9999 batch_limit:   8677 Loss:   0.0896 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10000 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10001 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10002 batch_limit:   8677 Loss:   0.0894 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10003 batch_limit:   8677 Loss:   0.0896 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10004 batch_limit:   8677 Loss:   0.0913 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10005 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10006 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10007 batch_limit:   8677 Loss:   0.0912 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10008 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10009 batch_limit:   8677 Loss:   0.0899 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10010 batch_limit:   8677 Loss:   0.0910 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10011 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10012 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10013 batch_limit:   8677 Loss:   0.0908 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10014 batch_limit:   8677 Loss:   0.0886 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10015 batch_limit:   8677 Loss:   0.0904 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10016 batch_limit:   8677 Loss:   0.0905 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10017 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10018 batch_limit:   8677 Loss:   0.0907 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10019 batch_limit:   8677 Loss:   0.0901 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10020 batch_limit:   8677 Loss:   0.0886 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10021 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10022 batch_limit:   8677 Loss:   0.0897 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10023 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10024 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10025 batch_limit:   8677 Loss:   0.0894 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10026 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10027 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10028 batch_limit:   8677 Loss:   0.0892 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10029 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10030 batch_limit:   8677 Loss:   0.0909 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10031 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10032 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10033 batch_limit:   8677 Loss:   0.0908 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10034 batch_limit:   8677 Loss:   0.0888 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10035 batch_limit:   8677 Loss:   0.0892 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10036 batch_limit:   8677 Loss:   0.0908 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10037 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10038 batch_limit:   8677 Loss:   0.0893 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10039 batch_limit:   8677 Loss:   0.0907 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10040 batch_limit:   8677 Loss:   0.0883 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10041 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10042 batch_limit:   8677 Loss:   0.0905 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10043 batch_limit:   8677 Loss:   0.0882 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10044 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10045 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10046 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10047 batch_limit:   8677 Loss:   0.0900 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10048 batch_limit:   8677 Loss:   0.0899 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10049 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10050 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10051 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10052 batch_limit:   8677 Loss:   0.0882 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10053 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10054 batch_limit:   8677 Loss:   0.0892 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10055 batch_limit:   8677 Loss:   0.0883 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10056 batch_limit:   8677 Loss:   0.0904 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10057 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10058 batch_limit:   8677 Loss:   0.0884 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10059 batch_limit:   8677 Loss:   0.0904 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10060 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10061 batch_limit:   8677 Loss:   0.0884 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10062 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10063 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10064 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10065 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10066 batch_limit:   8677 Loss:   0.0883 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10067 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10068 batch_limit:   8677 Loss:   0.0903 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10069 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10070 batch_limit:   8677 Loss:   0.0886 Training Acc:   100.00  6785.94    99.79%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10071 batch_limit:   8677 Loss:   0.0902 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10072 batch_limit:   8677 Loss:   0.0879 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10073 batch_limit:   8677 Loss:   0.0888 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10074 batch_limit:   8677 Loss:   0.0901 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10075 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10076 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10077 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10078 batch_limit:   8677 Loss:   0.0876 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10079 batch_limit:   8677 Loss:   0.0893 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10080 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10081 batch_limit:   8677 Loss:   0.0876 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10082 batch_limit:   8677 Loss:   0.0895 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10083 batch_limit:   8677 Loss:   0.0892 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10084 batch_limit:   8677 Loss:   0.0876 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10085 batch_limit:   8677 Loss:   0.0896 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10086 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10087 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10088 batch_limit:   8677 Loss:   0.0897 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10089 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10090 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10091 batch_limit:   8677 Loss:   0.0897 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10092 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10093 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10094 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10095 batch_limit:   8677 Loss:   0.0883 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10096 batch_limit:   8677 Loss:   0.0878 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10097 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10098 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10099 batch_limit:   8677 Loss:   0.0878 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10100 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10101 batch_limit:   8677 Loss:   0.0879 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10102 batch_limit:   8677 Loss:   0.0878 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10103 batch_limit:   8677 Loss:   0.0898 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10104 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10105 batch_limit:   8677 Loss:   0.0879 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10106 batch_limit:   8677 Loss:   0.0897 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10107 batch_limit:   8677 Loss:   0.0875 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10108 batch_limit:   8677 Loss:   0.0880 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10109 batch_limit:   8677 Loss:   0.0896 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10110 batch_limit:   8677 Loss:   0.0873 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10111 batch_limit:   8677 Loss:   0.0882 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10112 batch_limit:   8677 Loss:   0.0894 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10113 batch_limit:   8677 Loss:   0.0872 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10114 batch_limit:   8677 Loss:   0.0884 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10115 batch_limit:   8677 Loss:   0.0892 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10116 batch_limit:   8677 Loss:   0.0871 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10117 batch_limit:   8677 Loss:   0.0886 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10118 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10119 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10120 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10121 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10122 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10123 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10124 batch_limit:   8677 Loss:   0.0885 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10125 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10126 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10127 batch_limit:   8677 Loss:   0.0882 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10128 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10129 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10130 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10131 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10132 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10133 batch_limit:   8677 Loss:   0.0879 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10134 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10135 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10136 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10137 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10138 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10139 batch_limit:   8677 Loss:   0.0875 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10140 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10141 batch_limit:   8677 Loss:   0.0891 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10142 batch_limit:   8677 Loss:   0.0873 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10143 batch_limit:   8677 Loss:   0.0871 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10144 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10145 batch_limit:   8677 Loss:   0.0872 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10146 batch_limit:   8677 Loss:   0.0871 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10147 batch_limit:   8677 Loss:   0.0890 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10148 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10149 batch_limit:   8677 Loss:   0.0872 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10150 batch_limit:   8677 Loss:   0.0889 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10151 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10152 batch_limit:   8677 Loss:   0.0873 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10153 batch_limit:   8677 Loss:   0.0888 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10154 batch_limit:   8677 Loss:   0.0867 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10155 batch_limit:   8677 Loss:   0.0874 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10156 batch_limit:   8677 Loss:   0.0887 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10157 batch_limit:   8677 Loss:   0.0865 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10158 batch_limit:   8677 Loss:   0.0875 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10159 batch_limit:   8677 Loss:   0.0886 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10160 batch_limit:   8677 Loss:   0.0864 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10161 batch_limit:   8677 Loss:   0.0877 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10162 batch_limit:   8677 Loss:   0.0884 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10163 batch_limit:   8677 Loss:   0.0863 Training Acc:   100.00  6785.16    99.78%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10164 batch_limit:   8677 Loss:   0.0878 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10165 batch_limit:   8677 Loss:   0.0882 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10166 batch_limit:   8677 Loss:   0.0863 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10167 batch_limit:   8677 Loss:   0.0879 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10168 batch_limit:   8677 Loss:   0.0880 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10169 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10170 batch_limit:   8677 Loss:   0.0880 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10171 batch_limit:   8677 Loss:   0.0878 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10172 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10173 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10174 batch_limit:   8677 Loss:   0.0876 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10175 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10176 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10177 batch_limit:   8677 Loss:   0.0874 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10178 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10179 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10180 batch_limit:   8677 Loss:   0.0873 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10181 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10182 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10183 batch_limit:   8677 Loss:   0.0871 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10184 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10185 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10186 batch_limit:   8677 Loss:   0.0870 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10187 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10188 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10189 batch_limit:   8677 Loss:   0.0869 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10190 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10191 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10192 batch_limit:   8677 Loss:   0.0867 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10193 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10194 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10195 batch_limit:   8677 Loss:   0.0866 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10196 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10197 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10198 batch_limit:   8677 Loss:   0.0864 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10199 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10200 batch_limit:   8677 Loss:   0.0881 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10201 batch_limit:   8677 Loss:   0.0863 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10202 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10203 batch_limit:   8677 Loss:   0.0880 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10204 batch_limit:   8677 Loss:   0.0861 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10205 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10206 batch_limit:   8677 Loss:   0.0880 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10207 batch_limit:   8677 Loss:   0.0860 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10208 batch_limit:   8677 Loss:   0.0862 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10209 batch_limit:   8677 Loss:   0.0879 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10210 batch_limit:   8677 Loss:   0.0859 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10211 batch_limit:   8677 Loss:   0.0863 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10212 batch_limit:   8677 Loss:   0.0878 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10213 batch_limit:   8677 Loss:   0.0857 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10214 batch_limit:   8677 Loss:   0.0863 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10215 batch_limit:   8677 Loss:   0.0877 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10216 batch_limit:   8677 Loss:   0.0856 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10217 batch_limit:   8677 Loss:   0.0864 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10218 batch_limit:   8677 Loss:   0.0876 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10219 batch_limit:   8677 Loss:   0.0855 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10220 batch_limit:   8677 Loss:   0.0865 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10221 batch_limit:   8677 Loss:   0.0875 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10222 batch_limit:   8677 Loss:   0.0854 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10223 batch_limit:   8677 Loss:   0.0866 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10224 batch_limit:   8677 Loss:   0.0874 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10225 batch_limit:   8677 Loss:   0.0853 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10226 batch_limit:   8677 Loss:   0.0866 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10227 batch_limit:   8677 Loss:   0.0872 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10228 batch_limit:   8677 Loss:   0.0852 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10229 batch_limit:   8677 Loss:   0.0867 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10230 batch_limit:   8677 Loss:   0.0871 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10231 batch_limit:   8677 Loss:   0.0852 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch  10232 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10233 batch_limit:   8677 Loss:   0.0869 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10234 batch_limit:   8677 Loss:   0.0852 Training Acc:   100.00  6784.38    99.77%\n",
      "Epoch  10235 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10236 batch_limit:   8677 Loss:   0.0867 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10237 batch_limit:   8677 Loss:   0.0852 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10238 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10239 batch_limit:   8677 Loss:   0.0867 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10240 batch_limit:   8677 Loss:   0.0851 Training Acc:   100.00  6785.16    99.78%\n",
      "Epoch  10241 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10242 batch_limit:   8677 Loss:   0.0865 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10243 batch_limit:   8677 Loss:   0.0851 Training Acc:   100.00  6785.94    99.79%\n",
      "Epoch  10244 batch_limit:   8677 Loss:   0.0869 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10245 batch_limit:   8677 Loss:   0.0865 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10246 batch_limit:   8677 Loss:   0.0850 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10247 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6789.06    99.84%\n",
      "Epoch  10248 batch_limit:   8677 Loss:   0.0863 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10249 batch_limit:   8677 Loss:   0.0850 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10250 batch_limit:   8677 Loss:   0.0869 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10251 batch_limit:   8677 Loss:   0.0863 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10252 batch_limit:   8677 Loss:   0.0850 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10253 batch_limit:   8677 Loss:   0.0868 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10254 batch_limit:   8677 Loss:   0.0862 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10255 batch_limit:   8677 Loss:   0.0849 Training Acc:   100.00  6786.72    99.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10256 batch_limit:   8677 Loss:   0.0869 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10257 batch_limit:   8677 Loss:   0.0860 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10258 batch_limit:   8677 Loss:   0.0851 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10259 batch_limit:   8677 Loss:   0.0867 Training Acc:   100.00  6789.84    99.85%\n",
      "Epoch  10260 batch_limit:   8677 Loss:   0.0863 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10261 batch_limit:   8677 Loss:   0.0848 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10262 batch_limit:   8677 Loss:   0.0867 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10263 batch_limit:   8677 Loss:   0.0860 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10264 batch_limit:   8677 Loss:   0.0848 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10265 batch_limit:   8677 Loss:   0.0870 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10266 batch_limit:   8677 Loss:   0.0860 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10267 batch_limit:   8677 Loss:   0.0848 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10268 batch_limit:   8677 Loss:   0.0866 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10269 batch_limit:   8677 Loss:   0.0856 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10270 batch_limit:   8677 Loss:   0.0847 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10271 batch_limit:   8677 Loss:   0.0868 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10272 batch_limit:   8677 Loss:   0.0856 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10273 batch_limit:   8677 Loss:   0.0848 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10274 batch_limit:   8677 Loss:   0.0865 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10275 batch_limit:   8677 Loss:   0.0856 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10276 batch_limit:   8677 Loss:   0.0846 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10277 batch_limit:   8677 Loss:   0.0865 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10278 batch_limit:   8677 Loss:   0.0854 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10279 batch_limit:   8677 Loss:   0.0846 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10280 batch_limit:   8677 Loss:   0.0866 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10281 batch_limit:   8677 Loss:   0.0854 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10282 batch_limit:   8677 Loss:   0.0845 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10283 batch_limit:   8677 Loss:   0.0864 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10284 batch_limit:   8677 Loss:   0.0852 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10285 batch_limit:   8677 Loss:   0.0845 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10286 batch_limit:   8677 Loss:   0.0866 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10287 batch_limit:   8677 Loss:   0.0852 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10288 batch_limit:   8677 Loss:   0.0846 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10289 batch_limit:   8677 Loss:   0.0863 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10290 batch_limit:   8677 Loss:   0.0852 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10291 batch_limit:   8677 Loss:   0.0844 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10292 batch_limit:   8677 Loss:   0.0863 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10293 batch_limit:   8677 Loss:   0.0850 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10294 batch_limit:   8677 Loss:   0.0844 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10295 batch_limit:   8677 Loss:   0.0864 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10296 batch_limit:   8677 Loss:   0.0851 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10297 batch_limit:   8677 Loss:   0.0843 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10298 batch_limit:   8677 Loss:   0.0862 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10299 batch_limit:   8677 Loss:   0.0849 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10300 batch_limit:   8677 Loss:   0.0843 Training Acc:   100.00  6787.50    99.82%\n",
      "Epoch  10301 batch_limit:   8677 Loss:   0.0865 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10302 batch_limit:   8677 Loss:   0.0848 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10303 batch_limit:   8677 Loss:   0.0844 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10304 batch_limit:   8677 Loss:   0.0861 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10305 batch_limit:   8677 Loss:   0.0847 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10306 batch_limit:   8677 Loss:   0.0843 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10307 batch_limit:   8677 Loss:   0.0862 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10308 batch_limit:   8677 Loss:   0.0847 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10309 batch_limit:   8677 Loss:   0.0843 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10310 batch_limit:   8677 Loss:   0.0860 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10311 batch_limit:   8677 Loss:   0.0848 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10312 batch_limit:   8677 Loss:   0.0841 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10313 batch_limit:   8677 Loss:   0.0860 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10314 batch_limit:   8677 Loss:   0.0846 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10315 batch_limit:   8677 Loss:   0.0841 Training Acc:   100.00  6788.28    99.83%\n",
      "Epoch  10316 batch_limit:   8677 Loss:   0.0863 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10317 batch_limit:   8677 Loss:   0.0845 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10318 batch_limit:   8677 Loss:   0.0841 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10319 batch_limit:   8677 Loss:   0.0859 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10320 batch_limit:   8677 Loss:   0.0843 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10321 batch_limit:   8677 Loss:   0.0841 Training Acc:   100.00  6786.72    99.80%\n",
      "Epoch  10322 batch_limit:   8677 Loss:   0.0861 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10323 batch_limit:   8677 Loss:   0.0843 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10324 batch_limit:   8677 Loss:   0.0841 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10325 batch_limit:   8677 Loss:   0.0858 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10326 batch_limit:   8677 Loss:   0.0843 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10327 batch_limit:   8677 Loss:   0.0839 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10328 batch_limit:   8677 Loss:   0.0857 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10329 batch_limit:   8677 Loss:   0.0842 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10330 batch_limit:   8677 Loss:   0.0839 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10331 batch_limit:   8677 Loss:   0.0860 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10332 batch_limit:   8677 Loss:   0.0842 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10333 batch_limit:   8677 Loss:   0.0839 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10334 batch_limit:   8677 Loss:   0.0856 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10335 batch_limit:   8677 Loss:   0.0840 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10336 batch_limit:   8677 Loss:   0.0839 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10337 batch_limit:   8677 Loss:   0.0859 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10338 batch_limit:   8677 Loss:   0.0840 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10339 batch_limit:   8677 Loss:   0.0839 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10340 batch_limit:   8677 Loss:   0.0855 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10341 batch_limit:   8677 Loss:   0.0840 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10342 batch_limit:   8677 Loss:   0.0838 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10343 batch_limit:   8677 Loss:   0.0855 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10344 batch_limit:   8677 Loss:   0.0839 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10345 batch_limit:   8677 Loss:   0.0837 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10346 batch_limit:   8677 Loss:   0.0856 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10347 batch_limit:   8677 Loss:   0.0839 Training Acc:    99.01  6785.73    99.79%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10348 batch_limit:   8677 Loss:   0.0836 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10349 batch_limit:   8677 Loss:   0.0854 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10350 batch_limit:   8677 Loss:   0.0837 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10351 batch_limit:   8677 Loss:   0.0836 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10352 batch_limit:   8677 Loss:   0.0857 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10353 batch_limit:   8677 Loss:   0.0837 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10354 batch_limit:   8677 Loss:   0.0837 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10355 batch_limit:   8677 Loss:   0.0852 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10356 batch_limit:   8677 Loss:   0.0836 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10357 batch_limit:   8677 Loss:   0.0836 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10358 batch_limit:   8677 Loss:   0.0853 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10359 batch_limit:   8677 Loss:   0.0836 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10360 batch_limit:   8677 Loss:   0.0836 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10361 batch_limit:   8677 Loss:   0.0853 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10362 batch_limit:   8677 Loss:   0.0837 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10363 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10364 batch_limit:   8677 Loss:   0.0851 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10365 batch_limit:   8677 Loss:   0.0835 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10366 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10367 batch_limit:   8677 Loss:   0.0855 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10368 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10369 batch_limit:   8677 Loss:   0.0835 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10370 batch_limit:   8677 Loss:   0.0850 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10371 batch_limit:   8677 Loss:   0.0833 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10372 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10373 batch_limit:   8677 Loss:   0.0851 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10374 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10375 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10376 batch_limit:   8677 Loss:   0.0850 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10377 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10378 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10379 batch_limit:   8677 Loss:   0.0849 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10380 batch_limit:   8677 Loss:   0.0833 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10381 batch_limit:   8677 Loss:   0.0831 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10382 batch_limit:   8677 Loss:   0.0852 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10383 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6785.73    99.79%\n",
      "Epoch  10384 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10385 batch_limit:   8677 Loss:   0.0847 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10386 batch_limit:   8677 Loss:   0.0831 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10387 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10388 batch_limit:   8677 Loss:   0.0850 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10389 batch_limit:   8677 Loss:   0.0831 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10390 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10391 batch_limit:   8677 Loss:   0.0847 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10392 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10393 batch_limit:   8677 Loss:   0.0830 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10394 batch_limit:   8677 Loss:   0.0846 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10395 batch_limit:   8677 Loss:   0.0831 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10396 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10397 batch_limit:   8677 Loss:   0.0849 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10398 batch_limit:   8677 Loss:   0.0831 Training Acc:    99.01  6786.51    99.80%\n",
      "Epoch  10399 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10400 batch_limit:   8677 Loss:   0.0845 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10401 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10402 batch_limit:   8677 Loss:   0.0828 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10403 batch_limit:   8677 Loss:   0.0848 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10404 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10405 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10406 batch_limit:   8677 Loss:   0.0844 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10407 batch_limit:   8677 Loss:   0.0830 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10408 batch_limit:   8677 Loss:   0.0827 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10409 batch_limit:   8677 Loss:   0.0844 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10410 batch_limit:   8677 Loss:   0.0830 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10411 batch_limit:   8677 Loss:   0.0826 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10412 batch_limit:   8677 Loss:   0.0846 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10413 batch_limit:   8677 Loss:   0.0830 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10414 batch_limit:   8677 Loss:   0.0825 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10415 batch_limit:   8677 Loss:   0.0843 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10416 batch_limit:   8677 Loss:   0.0828 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10417 batch_limit:   8677 Loss:   0.0825 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10418 batch_limit:   8677 Loss:   0.0846 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10419 batch_limit:   8677 Loss:   0.0828 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10420 batch_limit:   8677 Loss:   0.0826 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10421 batch_limit:   8677 Loss:   0.0841 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10422 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10423 batch_limit:   8677 Loss:   0.0824 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10424 batch_limit:   8677 Loss:   0.0842 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10425 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10426 batch_limit:   8677 Loss:   0.0824 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10427 batch_limit:   8677 Loss:   0.0841 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10428 batch_limit:   8677 Loss:   0.0831 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10429 batch_limit:   8677 Loss:   0.0822 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10430 batch_limit:   8677 Loss:   0.0839 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10431 batch_limit:   8677 Loss:   0.0828 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10432 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10433 batch_limit:   8677 Loss:   0.0843 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10434 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10435 batch_limit:   8677 Loss:   0.0822 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10436 batch_limit:   8677 Loss:   0.0838 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10437 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10438 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10439 batch_limit:   8677 Loss:   0.0839 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10440 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6787.29    99.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10441 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10442 batch_limit:   8677 Loss:   0.0837 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10443 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10444 batch_limit:   8677 Loss:   0.0819 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10445 batch_limit:   8677 Loss:   0.0836 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10446 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10447 batch_limit:   8677 Loss:   0.0818 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10448 batch_limit:   8677 Loss:   0.0838 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10449 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10450 batch_limit:   8677 Loss:   0.0818 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10451 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10452 batch_limit:   8677 Loss:   0.0830 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10453 batch_limit:   8677 Loss:   0.0818 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10454 batch_limit:   8677 Loss:   0.0835 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10455 batch_limit:   8677 Loss:   0.0831 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10456 batch_limit:   8677 Loss:   0.0819 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10457 batch_limit:   8677 Loss:   0.0831 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10458 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10459 batch_limit:   8677 Loss:   0.0817 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10460 batch_limit:   8677 Loss:   0.0830 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10461 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10462 batch_limit:   8677 Loss:   0.0817 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10463 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10464 batch_limit:   8677 Loss:   0.0837 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10465 batch_limit:   8677 Loss:   0.0816 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10466 batch_limit:   8677 Loss:   0.0827 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10467 batch_limit:   8677 Loss:   0.0833 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10468 batch_limit:   8677 Loss:   0.0816 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10469 batch_limit:   8677 Loss:   0.0827 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10470 batch_limit:   8677 Loss:   0.0835 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10471 batch_limit:   8677 Loss:   0.0817 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10472 batch_limit:   8677 Loss:   0.0824 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10473 batch_limit:   8677 Loss:   0.0835 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10474 batch_limit:   8677 Loss:   0.0817 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10475 batch_limit:   8677 Loss:   0.0823 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10476 batch_limit:   8677 Loss:   0.0833 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10477 batch_limit:   8677 Loss:   0.0816 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10478 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10479 batch_limit:   8677 Loss:   0.0838 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10480 batch_limit:   8677 Loss:   0.0816 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10481 batch_limit:   8677 Loss:   0.0820 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10482 batch_limit:   8677 Loss:   0.0833 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10483 batch_limit:   8677 Loss:   0.0815 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10484 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10485 batch_limit:   8677 Loss:   0.0835 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10486 batch_limit:   8677 Loss:   0.0816 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10487 batch_limit:   8677 Loss:   0.0819 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10488 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10489 batch_limit:   8677 Loss:   0.0817 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10490 batch_limit:   8677 Loss:   0.0818 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10491 batch_limit:   8677 Loss:   0.0832 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10492 batch_limit:   8677 Loss:   0.0817 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10493 batch_limit:   8677 Loss:   0.0816 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10494 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10495 batch_limit:   8677 Loss:   0.0818 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10496 batch_limit:   8677 Loss:   0.0815 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10497 batch_limit:   8677 Loss:   0.0830 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10498 batch_limit:   8677 Loss:   0.0816 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10499 batch_limit:   8677 Loss:   0.0814 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10500 batch_limit:   8677 Loss:   0.0834 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10501 batch_limit:   8677 Loss:   0.0817 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10502 batch_limit:   8677 Loss:   0.0814 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10503 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10504 batch_limit:   8677 Loss:   0.0818 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10505 batch_limit:   8677 Loss:   0.0813 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10506 batch_limit:   8677 Loss:   0.0830 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10507 batch_limit:   8677 Loss:   0.0817 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10508 batch_limit:   8677 Loss:   0.0812 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10509 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10510 batch_limit:   8677 Loss:   0.0820 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10511 batch_limit:   8677 Loss:   0.0811 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10512 batch_limit:   8677 Loss:   0.0827 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10513 batch_limit:   8677 Loss:   0.0818 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10514 batch_limit:   8677 Loss:   0.0810 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10515 batch_limit:   8677 Loss:   0.0829 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10516 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10517 batch_limit:   8677 Loss:   0.0810 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10518 batch_limit:   8677 Loss:   0.0825 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10519 batch_limit:   8677 Loss:   0.0819 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10520 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10521 batch_limit:   8677 Loss:   0.0826 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10522 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10523 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10524 batch_limit:   8677 Loss:   0.0822 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10525 batch_limit:   8677 Loss:   0.0823 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10526 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10527 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10528 batch_limit:   8677 Loss:   0.0822 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10529 batch_limit:   8677 Loss:   0.0808 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10530 batch_limit:   8677 Loss:   0.0819 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10531 batch_limit:   8677 Loss:   0.0827 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10532 batch_limit:   8677 Loss:   0.0808 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10533 batch_limit:   8677 Loss:   0.0816 Training Acc:    99.01  6793.54    99.91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10534 batch_limit:   8677 Loss:   0.0824 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10535 batch_limit:   8677 Loss:   0.0808 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10536 batch_limit:   8677 Loss:   0.0816 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10537 batch_limit:   8677 Loss:   0.0827 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10538 batch_limit:   8677 Loss:   0.0808 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10539 batch_limit:   8677 Loss:   0.0813 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10540 batch_limit:   8677 Loss:   0.0825 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10541 batch_limit:   8677 Loss:   0.0810 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10542 batch_limit:   8677 Loss:   0.0811 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10543 batch_limit:   8677 Loss:   0.0825 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10544 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10545 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10546 batch_limit:   8677 Loss:   0.0827 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10547 batch_limit:   8677 Loss:   0.0811 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10548 batch_limit:   8677 Loss:   0.0808 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10549 batch_limit:   8677 Loss:   0.0823 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10550 batch_limit:   8677 Loss:   0.0810 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10551 batch_limit:   8677 Loss:   0.0807 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10552 batch_limit:   8677 Loss:   0.0826 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10553 batch_limit:   8677 Loss:   0.0811 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10554 batch_limit:   8677 Loss:   0.0807 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10555 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10556 batch_limit:   8677 Loss:   0.0811 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10557 batch_limit:   8677 Loss:   0.0806 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10558 batch_limit:   8677 Loss:   0.0823 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10559 batch_limit:   8677 Loss:   0.0811 Training Acc:    99.01  6787.29    99.81%\n",
      "Epoch  10560 batch_limit:   8677 Loss:   0.0805 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10561 batch_limit:   8677 Loss:   0.0820 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10562 batch_limit:   8677 Loss:   0.0814 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10563 batch_limit:   8677 Loss:   0.0804 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10564 batch_limit:   8677 Loss:   0.0819 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10565 batch_limit:   8677 Loss:   0.0813 Training Acc:    99.01  6788.07    99.82%\n",
      "Epoch  10566 batch_limit:   8677 Loss:   0.0803 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10567 batch_limit:   8677 Loss:   0.0819 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10568 batch_limit:   8677 Loss:   0.0816 Training Acc:    99.01  6788.85    99.84%\n",
      "Epoch  10569 batch_limit:   8677 Loss:   0.0803 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10570 batch_limit:   8677 Loss:   0.0815 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10571 batch_limit:   8677 Loss:   0.0815 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10572 batch_limit:   8677 Loss:   0.0802 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10573 batch_limit:   8677 Loss:   0.0815 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10574 batch_limit:   8677 Loss:   0.0818 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10575 batch_limit:   8677 Loss:   0.0802 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10576 batch_limit:   8677 Loss:   0.0811 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10577 batch_limit:   8677 Loss:   0.0819 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10578 batch_limit:   8677 Loss:   0.0803 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10579 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10580 batch_limit:   8677 Loss:   0.0819 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10581 batch_limit:   8677 Loss:   0.0803 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10582 batch_limit:   8677 Loss:   0.0806 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10583 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10584 batch_limit:   8677 Loss:   0.0804 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10585 batch_limit:   8677 Loss:   0.0804 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10586 batch_limit:   8677 Loss:   0.0818 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10587 batch_limit:   8677 Loss:   0.0804 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10588 batch_limit:   8677 Loss:   0.0802 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10589 batch_limit:   8677 Loss:   0.0821 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10590 batch_limit:   8677 Loss:   0.0806 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10591 batch_limit:   8677 Loss:   0.0801 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10592 batch_limit:   8677 Loss:   0.0817 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10593 batch_limit:   8677 Loss:   0.0807 Training Acc:    99.01  6789.63    99.85%\n",
      "Epoch  10594 batch_limit:   8677 Loss:   0.0800 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10595 batch_limit:   8677 Loss:   0.0817 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10596 batch_limit:   8677 Loss:   0.0807 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10597 batch_limit:   8677 Loss:   0.0799 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10598 batch_limit:   8677 Loss:   0.0815 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10599 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10600 batch_limit:   8677 Loss:   0.0799 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10601 batch_limit:   8677 Loss:   0.0814 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10602 batch_limit:   8677 Loss:   0.0808 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10603 batch_limit:   8677 Loss:   0.0798 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10604 batch_limit:   8677 Loss:   0.0813 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10605 batch_limit:   8677 Loss:   0.0810 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10606 batch_limit:   8677 Loss:   0.0798 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10607 batch_limit:   8677 Loss:   0.0810 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10608 batch_limit:   8677 Loss:   0.0810 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10609 batch_limit:   8677 Loss:   0.0798 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10610 batch_limit:   8677 Loss:   0.0808 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10611 batch_limit:   8677 Loss:   0.0813 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10612 batch_limit:   8677 Loss:   0.0798 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10613 batch_limit:   8677 Loss:   0.0804 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10614 batch_limit:   8677 Loss:   0.0813 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10615 batch_limit:   8677 Loss:   0.0799 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10616 batch_limit:   8677 Loss:   0.0802 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10617 batch_limit:   8677 Loss:   0.0814 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10618 batch_limit:   8677 Loss:   0.0800 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10619 batch_limit:   8677 Loss:   0.0799 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10620 batch_limit:   8677 Loss:   0.0815 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10621 batch_limit:   8677 Loss:   0.0802 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10622 batch_limit:   8677 Loss:   0.0796 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10623 batch_limit:   8677 Loss:   0.0812 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10624 batch_limit:   8677 Loss:   0.0803 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10625 batch_limit:   8677 Loss:   0.0795 Training Acc:    99.01  6795.10    99.93%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10626 batch_limit:   8677 Loss:   0.0812 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10627 batch_limit:   8677 Loss:   0.0805 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10628 batch_limit:   8677 Loss:   0.0794 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10629 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10630 batch_limit:   8677 Loss:   0.0805 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10631 batch_limit:   8677 Loss:   0.0794 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10632 batch_limit:   8677 Loss:   0.0808 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10633 batch_limit:   8677 Loss:   0.0806 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10634 batch_limit:   8677 Loss:   0.0794 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10635 batch_limit:   8677 Loss:   0.0805 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10636 batch_limit:   8677 Loss:   0.0808 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10637 batch_limit:   8677 Loss:   0.0794 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10638 batch_limit:   8677 Loss:   0.0803 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10639 batch_limit:   8677 Loss:   0.0808 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10640 batch_limit:   8677 Loss:   0.0794 Training Acc:    99.01  6790.42    99.86%\n",
      "Epoch  10641 batch_limit:   8677 Loss:   0.0800 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10642 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10643 batch_limit:   8677 Loss:   0.0795 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10644 batch_limit:   8677 Loss:   0.0797 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10645 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10646 batch_limit:   8677 Loss:   0.0796 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10647 batch_limit:   8677 Loss:   0.0795 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10648 batch_limit:   8677 Loss:   0.0809 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10649 batch_limit:   8677 Loss:   0.0798 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10650 batch_limit:   8677 Loss:   0.0792 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10651 batch_limit:   8677 Loss:   0.0807 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10652 batch_limit:   8677 Loss:   0.0800 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10653 batch_limit:   8677 Loss:   0.0790 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10654 batch_limit:   8677 Loss:   0.0805 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10655 batch_limit:   8677 Loss:   0.0802 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10656 batch_limit:   8677 Loss:   0.0790 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10657 batch_limit:   8677 Loss:   0.0802 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10658 batch_limit:   8677 Loss:   0.0804 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10659 batch_limit:   8677 Loss:   0.0790 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10660 batch_limit:   8677 Loss:   0.0799 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10661 batch_limit:   8677 Loss:   0.0805 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10662 batch_limit:   8677 Loss:   0.0790 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10663 batch_limit:   8677 Loss:   0.0796 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10664 batch_limit:   8677 Loss:   0.0806 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10665 batch_limit:   8677 Loss:   0.0791 Training Acc:    99.01  6791.98    99.88%\n",
      "Epoch  10666 batch_limit:   8677 Loss:   0.0793 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10667 batch_limit:   8677 Loss:   0.0806 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10668 batch_limit:   8677 Loss:   0.0793 Training Acc:    99.01  6791.20    99.87%\n",
      "Epoch  10669 batch_limit:   8677 Loss:   0.0790 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10670 batch_limit:   8677 Loss:   0.0805 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10671 batch_limit:   8677 Loss:   0.0794 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10672 batch_limit:   8677 Loss:   0.0788 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10673 batch_limit:   8677 Loss:   0.0803 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10674 batch_limit:   8677 Loss:   0.0796 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10675 batch_limit:   8677 Loss:   0.0787 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10676 batch_limit:   8677 Loss:   0.0801 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10677 batch_limit:   8677 Loss:   0.0797 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10678 batch_limit:   8677 Loss:   0.0786 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10679 batch_limit:   8677 Loss:   0.0799 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10680 batch_limit:   8677 Loss:   0.0799 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10681 batch_limit:   8677 Loss:   0.0786 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10682 batch_limit:   8677 Loss:   0.0796 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10683 batch_limit:   8677 Loss:   0.0800 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10684 batch_limit:   8677 Loss:   0.0787 Training Acc:    99.01  6792.76    99.89%\n",
      "Epoch  10685 batch_limit:   8677 Loss:   0.0792 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10686 batch_limit:   8677 Loss:   0.0801 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10687 batch_limit:   8677 Loss:   0.0788 Training Acc:    99.01  6793.54    99.91%\n",
      "Epoch  10688 batch_limit:   8677 Loss:   0.0789 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10689 batch_limit:   8677 Loss:   0.0801 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10690 batch_limit:   8677 Loss:   0.0789 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10691 batch_limit:   8677 Loss:   0.0786 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10692 batch_limit:   8677 Loss:   0.0800 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10693 batch_limit:   8677 Loss:   0.0791 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10694 batch_limit:   8677 Loss:   0.0784 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10695 batch_limit:   8677 Loss:   0.0798 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10696 batch_limit:   8677 Loss:   0.0793 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10697 batch_limit:   8677 Loss:   0.0783 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10698 batch_limit:   8677 Loss:   0.0795 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10699 batch_limit:   8677 Loss:   0.0795 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10700 batch_limit:   8677 Loss:   0.0782 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10701 batch_limit:   8677 Loss:   0.0791 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10702 batch_limit:   8677 Loss:   0.0797 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10703 batch_limit:   8677 Loss:   0.0783 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10704 batch_limit:   8677 Loss:   0.0788 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10705 batch_limit:   8677 Loss:   0.0798 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10706 batch_limit:   8677 Loss:   0.0784 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10707 batch_limit:   8677 Loss:   0.0784 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10708 batch_limit:   8677 Loss:   0.0798 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10709 batch_limit:   8677 Loss:   0.0786 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10710 batch_limit:   8677 Loss:   0.0782 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10711 batch_limit:   8677 Loss:   0.0796 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10712 batch_limit:   8677 Loss:   0.0788 Training Acc:    99.01  6794.32    99.92%\n",
      "Epoch  10713 batch_limit:   8677 Loss:   0.0780 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10714 batch_limit:   8677 Loss:   0.0793 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10715 batch_limit:   8677 Loss:   0.0790 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10716 batch_limit:   8677 Loss:   0.0779 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10717 batch_limit:   8677 Loss:   0.0790 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10718 batch_limit:   8677 Loss:   0.0792 Training Acc:    99.01  6795.10    99.93%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10719 batch_limit:   8677 Loss:   0.0779 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10720 batch_limit:   8677 Loss:   0.0786 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10721 batch_limit:   8677 Loss:   0.0794 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10722 batch_limit:   8677 Loss:   0.0780 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10723 batch_limit:   8677 Loss:   0.0782 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10724 batch_limit:   8677 Loss:   0.0794 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10725 batch_limit:   8677 Loss:   0.0782 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10726 batch_limit:   8677 Loss:   0.0779 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10727 batch_limit:   8677 Loss:   0.0793 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10728 batch_limit:   8677 Loss:   0.0784 Training Acc:    99.01  6795.10    99.93%\n",
      "Epoch  10729 batch_limit:   8677 Loss:   0.0777 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10730 batch_limit:   8677 Loss:   0.0790 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10731 batch_limit:   8677 Loss:   0.0786 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10732 batch_limit:   8677 Loss:   0.0776 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10733 batch_limit:   8677 Loss:   0.0787 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10734 batch_limit:   8677 Loss:   0.0788 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10735 batch_limit:   8677 Loss:   0.0776 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10736 batch_limit:   8677 Loss:   0.0783 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10737 batch_limit:   8677 Loss:   0.0790 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10738 batch_limit:   8677 Loss:   0.0776 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10739 batch_limit:   8677 Loss:   0.0779 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10740 batch_limit:   8677 Loss:   0.0790 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10741 batch_limit:   8677 Loss:   0.0778 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10742 batch_limit:   8677 Loss:   0.0776 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10743 batch_limit:   8677 Loss:   0.0789 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10744 batch_limit:   8677 Loss:   0.0780 Training Acc:    99.01  6795.88    99.94%\n",
      "Epoch  10745 batch_limit:   8677 Loss:   0.0774 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10746 batch_limit:   8677 Loss:   0.0785 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10747 batch_limit:   8677 Loss:   0.0783 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10748 batch_limit:   8677 Loss:   0.0772 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10749 batch_limit:   8677 Loss:   0.0782 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10750 batch_limit:   8677 Loss:   0.0785 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10751 batch_limit:   8677 Loss:   0.0772 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10752 batch_limit:   8677 Loss:   0.0777 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10753 batch_limit:   8677 Loss:   0.0787 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10754 batch_limit:   8677 Loss:   0.0774 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10755 batch_limit:   8677 Loss:   0.0774 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10756 batch_limit:   8677 Loss:   0.0786 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10757 batch_limit:   8677 Loss:   0.0776 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10758 batch_limit:   8677 Loss:   0.0771 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10759 batch_limit:   8677 Loss:   0.0783 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10760 batch_limit:   8677 Loss:   0.0779 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10761 batch_limit:   8677 Loss:   0.0770 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10762 batch_limit:   8677 Loss:   0.0779 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10763 batch_limit:   8677 Loss:   0.0781 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10764 batch_limit:   8677 Loss:   0.0769 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10765 batch_limit:   8677 Loss:   0.0775 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10766 batch_limit:   8677 Loss:   0.0783 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10767 batch_limit:   8677 Loss:   0.0770 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10768 batch_limit:   8677 Loss:   0.0771 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10769 batch_limit:   8677 Loss:   0.0783 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10770 batch_limit:   8677 Loss:   0.0772 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10771 batch_limit:   8677 Loss:   0.0768 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10772 batch_limit:   8677 Loss:   0.0780 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10773 batch_limit:   8677 Loss:   0.0775 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10774 batch_limit:   8677 Loss:   0.0767 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10775 batch_limit:   8677 Loss:   0.0776 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10776 batch_limit:   8677 Loss:   0.0778 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10777 batch_limit:   8677 Loss:   0.0766 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10778 batch_limit:   8677 Loss:   0.0772 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10779 batch_limit:   8677 Loss:   0.0780 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10780 batch_limit:   8677 Loss:   0.0767 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10781 batch_limit:   8677 Loss:   0.0767 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10782 batch_limit:   8677 Loss:   0.0780 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10783 batch_limit:   8677 Loss:   0.0769 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10784 batch_limit:   8677 Loss:   0.0765 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10785 batch_limit:   8677 Loss:   0.0777 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10786 batch_limit:   8677 Loss:   0.0772 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10787 batch_limit:   8677 Loss:   0.0764 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10788 batch_limit:   8677 Loss:   0.0772 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10789 batch_limit:   8677 Loss:   0.0775 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10790 batch_limit:   8677 Loss:   0.0763 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10791 batch_limit:   8677 Loss:   0.0768 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10792 batch_limit:   8677 Loss:   0.0777 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10793 batch_limit:   8677 Loss:   0.0764 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10794 batch_limit:   8677 Loss:   0.0764 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10795 batch_limit:   8677 Loss:   0.0776 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10796 batch_limit:   8677 Loss:   0.0766 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10797 batch_limit:   8677 Loss:   0.0762 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10798 batch_limit:   8677 Loss:   0.0773 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10799 batch_limit:   8677 Loss:   0.0769 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10800 batch_limit:   8677 Loss:   0.0760 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10801 batch_limit:   8677 Loss:   0.0768 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10802 batch_limit:   8677 Loss:   0.0772 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10803 batch_limit:   8677 Loss:   0.0760 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10804 batch_limit:   8677 Loss:   0.0763 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10805 batch_limit:   8677 Loss:   0.0774 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10806 batch_limit:   8677 Loss:   0.0762 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10807 batch_limit:   8677 Loss:   0.0760 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10808 batch_limit:   8677 Loss:   0.0772 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10809 batch_limit:   8677 Loss:   0.0764 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10810 batch_limit:   8677 Loss:   0.0758 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10811 batch_limit:   8677 Loss:   0.0768 Training Acc:    99.01  6797.45    99.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10812 batch_limit:   8677 Loss:   0.0767 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10813 batch_limit:   8677 Loss:   0.0757 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10814 batch_limit:   8677 Loss:   0.0763 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10815 batch_limit:   8677 Loss:   0.0770 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10816 batch_limit:   8677 Loss:   0.0758 Training Acc:    99.01  6796.67    99.95%\n",
      "Epoch  10817 batch_limit:   8677 Loss:   0.0758 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10818 batch_limit:   8677 Loss:   0.0771 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10819 batch_limit:   8677 Loss:   0.0760 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10820 batch_limit:   8677 Loss:   0.0756 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10821 batch_limit:   8677 Loss:   0.0767 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10822 batch_limit:   8677 Loss:   0.0762 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10823 batch_limit:   8677 Loss:   0.0755 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10824 batch_limit:   8677 Loss:   0.0763 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10825 batch_limit:   8677 Loss:   0.0765 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10826 batch_limit:   8677 Loss:   0.0754 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10827 batch_limit:   8677 Loss:   0.0757 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10828 batch_limit:   8677 Loss:   0.0768 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10829 batch_limit:   8677 Loss:   0.0755 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10830 batch_limit:   8677 Loss:   0.0753 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10831 batch_limit:   8677 Loss:   0.0766 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10832 batch_limit:   8677 Loss:   0.0758 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10833 batch_limit:   8677 Loss:   0.0752 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10834 batch_limit:   8677 Loss:   0.0762 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10835 batch_limit:   8677 Loss:   0.0760 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10836 batch_limit:   8677 Loss:   0.0751 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10837 batch_limit:   8677 Loss:   0.0757 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10838 batch_limit:   8677 Loss:   0.0763 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10839 batch_limit:   8677 Loss:   0.0752 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10840 batch_limit:   8677 Loss:   0.0752 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10841 batch_limit:   8677 Loss:   0.0764 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10842 batch_limit:   8677 Loss:   0.0753 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10843 batch_limit:   8677 Loss:   0.0749 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10844 batch_limit:   8677 Loss:   0.0762 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10845 batch_limit:   8677 Loss:   0.0756 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10846 batch_limit:   8677 Loss:   0.0749 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10847 batch_limit:   8677 Loss:   0.0757 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10848 batch_limit:   8677 Loss:   0.0759 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10849 batch_limit:   8677 Loss:   0.0748 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10850 batch_limit:   8677 Loss:   0.0751 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10851 batch_limit:   8677 Loss:   0.0761 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10852 batch_limit:   8677 Loss:   0.0749 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10853 batch_limit:   8677 Loss:   0.0747 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10854 batch_limit:   8677 Loss:   0.0760 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10855 batch_limit:   8677 Loss:   0.0751 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10856 batch_limit:   8677 Loss:   0.0746 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10857 batch_limit:   8677 Loss:   0.0756 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10858 batch_limit:   8677 Loss:   0.0754 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10859 batch_limit:   8677 Loss:   0.0745 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10860 batch_limit:   8677 Loss:   0.0750 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10861 batch_limit:   8677 Loss:   0.0757 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10862 batch_limit:   8677 Loss:   0.0746 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10863 batch_limit:   8677 Loss:   0.0746 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10864 batch_limit:   8677 Loss:   0.0758 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10865 batch_limit:   8677 Loss:   0.0747 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10866 batch_limit:   8677 Loss:   0.0743 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10867 batch_limit:   8677 Loss:   0.0755 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10868 batch_limit:   8677 Loss:   0.0750 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10869 batch_limit:   8677 Loss:   0.0742 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10870 batch_limit:   8677 Loss:   0.0749 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10871 batch_limit:   8677 Loss:   0.0753 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10872 batch_limit:   8677 Loss:   0.0743 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10873 batch_limit:   8677 Loss:   0.0744 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10874 batch_limit:   8677 Loss:   0.0755 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10875 batch_limit:   8677 Loss:   0.0744 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10876 batch_limit:   8677 Loss:   0.0740 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10877 batch_limit:   8677 Loss:   0.0753 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10878 batch_limit:   8677 Loss:   0.0746 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10879 batch_limit:   8677 Loss:   0.0739 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10880 batch_limit:   8677 Loss:   0.0748 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10881 batch_limit:   8677 Loss:   0.0749 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10882 batch_limit:   8677 Loss:   0.0739 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10883 batch_limit:   8677 Loss:   0.0742 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10884 batch_limit:   8677 Loss:   0.0752 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10885 batch_limit:   8677 Loss:   0.0741 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10886 batch_limit:   8677 Loss:   0.0738 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10887 batch_limit:   8677 Loss:   0.0751 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10888 batch_limit:   8677 Loss:   0.0742 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10889 batch_limit:   8677 Loss:   0.0736 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10890 batch_limit:   8677 Loss:   0.0746 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10891 batch_limit:   8677 Loss:   0.0745 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10892 batch_limit:   8677 Loss:   0.0736 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10893 batch_limit:   8677 Loss:   0.0741 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10894 batch_limit:   8677 Loss:   0.0749 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10895 batch_limit:   8677 Loss:   0.0737 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10896 batch_limit:   8677 Loss:   0.0736 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10897 batch_limit:   8677 Loss:   0.0749 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10898 batch_limit:   8677 Loss:   0.0738 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10899 batch_limit:   8677 Loss:   0.0733 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10900 batch_limit:   8677 Loss:   0.0745 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10901 batch_limit:   8677 Loss:   0.0741 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10902 batch_limit:   8677 Loss:   0.0733 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10903 batch_limit:   8677 Loss:   0.0740 Training Acc:    99.01  6798.23    99.97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10904 batch_limit:   8677 Loss:   0.0745 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10905 batch_limit:   8677 Loss:   0.0734 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10906 batch_limit:   8677 Loss:   0.0735 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10907 batch_limit:   8677 Loss:   0.0746 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10908 batch_limit:   8677 Loss:   0.0734 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10909 batch_limit:   8677 Loss:   0.0731 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10910 batch_limit:   8677 Loss:   0.0743 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10911 batch_limit:   8677 Loss:   0.0735 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10912 batch_limit:   8677 Loss:   0.0730 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10913 batch_limit:   8677 Loss:   0.0740 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10914 batch_limit:   8677 Loss:   0.0741 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10915 batch_limit:   8677 Loss:   0.0731 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10916 batch_limit:   8677 Loss:   0.0735 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10917 batch_limit:   8677 Loss:   0.0741 Training Acc:    99.01  6797.45    99.96%\n",
      "Epoch  10918 batch_limit:   8677 Loss:   0.0729 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10919 batch_limit:   8677 Loss:   0.0729 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10920 batch_limit:   8677 Loss:   0.0741 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10921 batch_limit:   8677 Loss:   0.0730 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10922 batch_limit:   8677 Loss:   0.0728 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10923 batch_limit:   8677 Loss:   0.0741 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10924 batch_limit:   8677 Loss:   0.0737 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10925 batch_limit:   8677 Loss:   0.0730 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10926 batch_limit:   8677 Loss:   0.0736 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10927 batch_limit:   8677 Loss:   0.0736 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10928 batch_limit:   8677 Loss:   0.0725 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10929 batch_limit:   8677 Loss:   0.0729 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10930 batch_limit:   8677 Loss:   0.0737 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10931 batch_limit:   8677 Loss:   0.0726 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10932 batch_limit:   8677 Loss:   0.0726 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10933 batch_limit:   8677 Loss:   0.0741 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10934 batch_limit:   8677 Loss:   0.0735 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10935 batch_limit:   8677 Loss:   0.0729 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10936 batch_limit:   8677 Loss:   0.0736 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10937 batch_limit:   8677 Loss:   0.0730 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10938 batch_limit:   8677 Loss:   0.0723 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10939 batch_limit:   8677 Loss:   0.0728 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10940 batch_limit:   8677 Loss:   0.0732 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10941 batch_limit:   8677 Loss:   0.0723 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10942 batch_limit:   8677 Loss:   0.0726 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10943 batch_limit:   8677 Loss:   0.0740 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10944 batch_limit:   8677 Loss:   0.0732 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10945 batch_limit:   8677 Loss:   0.0725 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10946 batch_limit:   8677 Loss:   0.0732 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10947 batch_limit:   8677 Loss:   0.0726 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10948 batch_limit:   8677 Loss:   0.0720 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10949 batch_limit:   8677 Loss:   0.0726 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10950 batch_limit:   8677 Loss:   0.0729 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10951 batch_limit:   8677 Loss:   0.0721 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10952 batch_limit:   8677 Loss:   0.0724 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10953 batch_limit:   8677 Loss:   0.0737 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10954 batch_limit:   8677 Loss:   0.0726 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10955 batch_limit:   8677 Loss:   0.0719 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10956 batch_limit:   8677 Loss:   0.0730 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10957 batch_limit:   8677 Loss:   0.0722 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10958 batch_limit:   8677 Loss:   0.0716 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10959 batch_limit:   8677 Loss:   0.0725 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10960 batch_limit:   8677 Loss:   0.0726 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10961 batch_limit:   8677 Loss:   0.0718 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10962 batch_limit:   8677 Loss:   0.0724 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10963 batch_limit:   8677 Loss:   0.0732 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10964 batch_limit:   8677 Loss:   0.0719 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10965 batch_limit:   8677 Loss:   0.0717 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10966 batch_limit:   8677 Loss:   0.0728 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10967 batch_limit:   8677 Loss:   0.0717 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10968 batch_limit:   8677 Loss:   0.0714 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10969 batch_limit:   8677 Loss:   0.0726 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10970 batch_limit:   8677 Loss:   0.0721 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10971 batch_limit:   8677 Loss:   0.0715 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10972 batch_limit:   8677 Loss:   0.0725 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10973 batch_limit:   8677 Loss:   0.0725 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10974 batch_limit:   8677 Loss:   0.0713 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10975 batch_limit:   8677 Loss:   0.0718 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10976 batch_limit:   8677 Loss:   0.0723 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10977 batch_limit:   8677 Loss:   0.0711 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10978 batch_limit:   8677 Loss:   0.0715 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10979 batch_limit:   8677 Loss:   0.0725 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10980 batch_limit:   8677 Loss:   0.0714 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10981 batch_limit:   8677 Loss:   0.0716 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10982 batch_limit:   8677 Loss:   0.0726 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10983 batch_limit:   8677 Loss:   0.0716 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10984 batch_limit:   8677 Loss:   0.0711 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10985 batch_limit:   8677 Loss:   0.0720 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10986 batch_limit:   8677 Loss:   0.0713 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10987 batch_limit:   8677 Loss:   0.0709 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10988 batch_limit:   8677 Loss:   0.0718 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10989 batch_limit:   8677 Loss:   0.0717 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10990 batch_limit:   8677 Loss:   0.0710 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10991 batch_limit:   8677 Loss:   0.0718 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10992 batch_limit:   8677 Loss:   0.0723 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10993 batch_limit:   8677 Loss:   0.0710 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10994 batch_limit:   8677 Loss:   0.0711 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10995 batch_limit:   8677 Loss:   0.0718 Training Acc:    99.01  6799.01    99.99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10996 batch_limit:   8677 Loss:   0.0707 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10997 batch_limit:   8677 Loss:   0.0707 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  10998 batch_limit:   8677 Loss:   0.0718 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  10999 batch_limit:   8677 Loss:   0.0711 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11000 batch_limit:   8677 Loss:   0.0710 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11001 batch_limit:   8677 Loss:   0.0719 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11002 batch_limit:   8677 Loss:   0.0716 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11003 batch_limit:   8677 Loss:   0.0707 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11004 batch_limit:   8677 Loss:   0.0710 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11005 batch_limit:   8677 Loss:   0.0712 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11006 batch_limit:   8677 Loss:   0.0705 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11007 batch_limit:   8677 Loss:   0.0705 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11008 batch_limit:   8677 Loss:   0.0715 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11009 batch_limit:   8677 Loss:   0.0709 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11010 batch_limit:   8677 Loss:   0.0707 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11011 batch_limit:   8677 Loss:   0.0718 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11012 batch_limit:   8677 Loss:   0.0713 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11013 batch_limit:   8677 Loss:   0.0703 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11014 batch_limit:   8677 Loss:   0.0707 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11015 batch_limit:   8677 Loss:   0.0710 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11016 batch_limit:   8677 Loss:   0.0703 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11017 batch_limit:   8677 Loss:   0.0702 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11018 batch_limit:   8677 Loss:   0.0713 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11019 batch_limit:   8677 Loss:   0.0710 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11020 batch_limit:   8677 Loss:   0.0705 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11021 batch_limit:   8677 Loss:   0.0712 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11022 batch_limit:   8677 Loss:   0.0711 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11023 batch_limit:   8677 Loss:   0.0701 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11024 batch_limit:   8677 Loss:   0.0701 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11025 batch_limit:   8677 Loss:   0.0708 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11026 batch_limit:   8677 Loss:   0.0702 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11027 batch_limit:   8677 Loss:   0.0698 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11028 batch_limit:   8677 Loss:   0.0708 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11029 batch_limit:   8677 Loss:   0.0712 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11030 batch_limit:   8677 Loss:   0.0705 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11031 batch_limit:   8677 Loss:   0.0703 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11032 batch_limit:   8677 Loss:   0.0708 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11033 batch_limit:   8677 Loss:   0.0701 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11034 batch_limit:   8677 Loss:   0.0696 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11035 batch_limit:   8677 Loss:   0.0702 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11036 batch_limit:   8677 Loss:   0.0704 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11037 batch_limit:   8677 Loss:   0.0698 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11038 batch_limit:   8677 Loss:   0.0699 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11039 batch_limit:   8677 Loss:   0.0712 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11040 batch_limit:   8677 Loss:   0.0706 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11041 batch_limit:   8677 Loss:   0.0695 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11042 batch_limit:   8677 Loss:   0.0702 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11043 batch_limit:   8677 Loss:   0.0702 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11044 batch_limit:   8677 Loss:   0.0694 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11045 batch_limit:   8677 Loss:   0.0694 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11046 batch_limit:   8677 Loss:   0.0707 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11047 batch_limit:   8677 Loss:   0.0698 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11048 batch_limit:   8677 Loss:   0.0693 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11049 batch_limit:   8677 Loss:   0.0709 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11050 batch_limit:   8677 Loss:   0.0703 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11051 batch_limit:   8677 Loss:   0.0689 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11052 batch_limit:   8677 Loss:   0.0701 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11053 batch_limit:   8677 Loss:   0.0700 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11054 batch_limit:   8677 Loss:   0.0686 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11055 batch_limit:   8677 Loss:   0.0699 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11056 batch_limit:   8677 Loss:   0.0703 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11057 batch_limit:   8677 Loss:   0.0689 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11058 batch_limit:   8677 Loss:   0.0702 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11059 batch_limit:   8677 Loss:   0.0703 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11060 batch_limit:   8677 Loss:   0.0690 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11061 batch_limit:   8677 Loss:   0.0698 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11062 batch_limit:   8677 Loss:   0.0696 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11063 batch_limit:   8677 Loss:   0.0687 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11064 batch_limit:   8677 Loss:   0.0697 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11065 batch_limit:   8677 Loss:   0.0696 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11066 batch_limit:   8677 Loss:   0.0689 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11067 batch_limit:   8677 Loss:   0.0699 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11068 batch_limit:   8677 Loss:   0.0699 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11069 batch_limit:   8677 Loss:   0.0690 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11070 batch_limit:   8677 Loss:   0.0695 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11071 batch_limit:   8677 Loss:   0.0694 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11072 batch_limit:   8677 Loss:   0.0686 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11073 batch_limit:   8677 Loss:   0.0692 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11074 batch_limit:   8677 Loss:   0.0694 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11075 batch_limit:   8677 Loss:   0.0686 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11076 batch_limit:   8677 Loss:   0.0693 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11077 batch_limit:   8677 Loss:   0.0699 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11078 batch_limit:   8677 Loss:   0.0691 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11079 batch_limit:   8677 Loss:   0.0691 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11080 batch_limit:   8677 Loss:   0.0695 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11081 batch_limit:   8677 Loss:   0.0686 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11082 batch_limit:   8677 Loss:   0.0686 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11083 batch_limit:   8677 Loss:   0.0692 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11084 batch_limit:   8677 Loss:   0.0686 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11085 batch_limit:   8677 Loss:   0.0686 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11086 batch_limit:   8677 Loss:   0.0694 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11087 batch_limit:   8677 Loss:   0.0694 Training Acc:    99.01  6799.01    99.99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11088 batch_limit:   8677 Loss:   0.0690 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11089 batch_limit:   8677 Loss:   0.0689 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11090 batch_limit:   8677 Loss:   0.0690 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11091 batch_limit:   8677 Loss:   0.0685 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11092 batch_limit:   8677 Loss:   0.0683 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11093 batch_limit:   8677 Loss:   0.0687 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11094 batch_limit:   8677 Loss:   0.0686 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11095 batch_limit:   8677 Loss:   0.0684 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11096 batch_limit:   8677 Loss:   0.0689 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11097 batch_limit:   8677 Loss:   0.0694 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11098 batch_limit:   8677 Loss:   0.0690 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11099 batch_limit:   8677 Loss:   0.0684 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11100 batch_limit:   8677 Loss:   0.0684 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11101 batch_limit:   8677 Loss:   0.0685 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11102 batch_limit:   8677 Loss:   0.0682 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11103 batch_limit:   8677 Loss:   0.0680 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11104 batch_limit:   8677 Loss:   0.0683 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11105 batch_limit:   8677 Loss:   0.0689 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11106 batch_limit:   8677 Loss:   0.0688 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11107 batch_limit:   8677 Loss:   0.0685 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11108 batch_limit:   8677 Loss:   0.0686 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11109 batch_limit:   8677 Loss:   0.0685 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11110 batch_limit:   8677 Loss:   0.0680 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11111 batch_limit:   8677 Loss:   0.0679 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11112 batch_limit:   8677 Loss:   0.0680 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11113 batch_limit:   8677 Loss:   0.0681 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11114 batch_limit:   8677 Loss:   0.0685 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11115 batch_limit:   8677 Loss:   0.0681 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11116 batch_limit:   8677 Loss:   0.0679 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11117 batch_limit:   8677 Loss:   0.0694 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11118 batch_limit:   8677 Loss:   0.0683 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11119 batch_limit:   8677 Loss:   0.0672 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11120 batch_limit:   8677 Loss:   0.0682 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11121 batch_limit:   8677 Loss:   0.0681 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11122 batch_limit:   8677 Loss:   0.0672 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11123 batch_limit:   8677 Loss:   0.0676 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11124 batch_limit:   8677 Loss:   0.0689 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11125 batch_limit:   8677 Loss:   0.0682 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11126 batch_limit:   8677 Loss:   0.0675 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11127 batch_limit:   8677 Loss:   0.0688 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11128 batch_limit:   8677 Loss:   0.0680 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11129 batch_limit:   8677 Loss:   0.0668 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11130 batch_limit:   8677 Loss:   0.0680 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11131 batch_limit:   8677 Loss:   0.0680 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11132 batch_limit:   8677 Loss:   0.0665 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11133 batch_limit:   8677 Loss:   0.0681 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11134 batch_limit:   8677 Loss:   0.0686 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11135 batch_limit:   8677 Loss:   0.0673 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11136 batch_limit:   8677 Loss:   0.0683 Training Acc:    99.01  6798.23    99.97%\n",
      "Epoch  11137 batch_limit:   8677 Loss:   0.0680 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11138 batch_limit:   8677 Loss:   0.0668 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11139 batch_limit:   8677 Loss:   0.0679 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11140 batch_limit:   8677 Loss:   0.0674 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11141 batch_limit:   8677 Loss:   0.0667 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11142 batch_limit:   8677 Loss:   0.0680 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11143 batch_limit:   8677 Loss:   0.0678 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11144 batch_limit:   8677 Loss:   0.0672 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11145 batch_limit:   8677 Loss:   0.0682 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11146 batch_limit:   8677 Loss:   0.0676 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11147 batch_limit:   8677 Loss:   0.0668 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11148 batch_limit:   8677 Loss:   0.0677 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11149 batch_limit:   8677 Loss:   0.0672 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11150 batch_limit:   8677 Loss:   0.0665 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11151 batch_limit:   8677 Loss:   0.0677 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11152 batch_limit:   8677 Loss:   0.0675 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11153 batch_limit:   8677 Loss:   0.0669 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11154 batch_limit:   8677 Loss:   0.0679 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11155 batch_limit:   8677 Loss:   0.0676 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11156 batch_limit:   8677 Loss:   0.0666 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11157 batch_limit:   8677 Loss:   0.0674 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11158 batch_limit:   8677 Loss:   0.0672 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11159 batch_limit:   8677 Loss:   0.0663 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11160 batch_limit:   8677 Loss:   0.0672 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11161 batch_limit:   8677 Loss:   0.0674 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11162 batch_limit:   8677 Loss:   0.0666 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11163 batch_limit:   8677 Loss:   0.0674 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11164 batch_limit:   8677 Loss:   0.0678 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11165 batch_limit:   8677 Loss:   0.0667 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11166 batch_limit:   8677 Loss:   0.0668 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11167 batch_limit:   8677 Loss:   0.0673 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11168 batch_limit:   8677 Loss:   0.0663 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11169 batch_limit:   8677 Loss:   0.0664 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11170 batch_limit:   8677 Loss:   0.0673 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11171 batch_limit:   8677 Loss:   0.0667 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11172 batch_limit:   8677 Loss:   0.0666 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11173 batch_limit:   8677 Loss:   0.0677 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11174 batch_limit:   8677 Loss:   0.0672 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11175 batch_limit:   8677 Loss:   0.0662 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11176 batch_limit:   8677 Loss:   0.0670 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11177 batch_limit:   8677 Loss:   0.0668 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11178 batch_limit:   8677 Loss:   0.0658 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11179 batch_limit:   8677 Loss:   0.0666 Training Acc:    99.01  6799.01    99.99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11180 batch_limit:   8677 Loss:   0.0672 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11181 batch_limit:   8677 Loss:   0.0662 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11182 batch_limit:   8677 Loss:   0.0667 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11183 batch_limit:   8677 Loss:   0.0678 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11184 batch_limit:   8677 Loss:   0.0665 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11185 batch_limit:   8677 Loss:   0.0661 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11186 batch_limit:   8677 Loss:   0.0671 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11187 batch_limit:   8677 Loss:   0.0660 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11188 batch_limit:   8677 Loss:   0.0656 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11189 batch_limit:   8677 Loss:   0.0670 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11190 batch_limit:   8677 Loss:   0.0665 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11191 batch_limit:   8677 Loss:   0.0658 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11192 batch_limit:   8677 Loss:   0.0673 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11193 batch_limit:   8677 Loss:   0.0669 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11194 batch_limit:   8677 Loss:   0.0655 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11195 batch_limit:   8677 Loss:   0.0669 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11196 batch_limit:   8677 Loss:   0.0661 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11197 batch_limit:   8677 Loss:   0.0652 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11198 batch_limit:   8677 Loss:   0.0668 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11199 batch_limit:   8677 Loss:   0.0661 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11200 batch_limit:   8677 Loss:   0.0656 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11201 batch_limit:   8677 Loss:   0.0671 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11202 batch_limit:   8677 Loss:   0.0664 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11203 batch_limit:   8677 Loss:   0.0657 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11204 batch_limit:   8677 Loss:   0.0667 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11205 batch_limit:   8677 Loss:   0.0657 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11206 batch_limit:   8677 Loss:   0.0654 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11207 batch_limit:   8677 Loss:   0.0665 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11208 batch_limit:   8677 Loss:   0.0656 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11209 batch_limit:   8677 Loss:   0.0656 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11210 batch_limit:   8677 Loss:   0.0668 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11211 batch_limit:   8677 Loss:   0.0662 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11212 batch_limit:   8677 Loss:   0.0658 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11213 batch_limit:   8677 Loss:   0.0665 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11214 batch_limit:   8677 Loss:   0.0656 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11215 batch_limit:   8677 Loss:   0.0653 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11216 batch_limit:   8677 Loss:   0.0662 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11217 batch_limit:   8677 Loss:   0.0654 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11218 batch_limit:   8677 Loss:   0.0653 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11219 batch_limit:   8677 Loss:   0.0665 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11220 batch_limit:   8677 Loss:   0.0661 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11221 batch_limit:   8677 Loss:   0.0656 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11222 batch_limit:   8677 Loss:   0.0664 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11223 batch_limit:   8677 Loss:   0.0657 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11224 batch_limit:   8677 Loss:   0.0649 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11225 batch_limit:   8677 Loss:   0.0659 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11226 batch_limit:   8677 Loss:   0.0654 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11227 batch_limit:   8677 Loss:   0.0648 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11228 batch_limit:   8677 Loss:   0.0661 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11229 batch_limit:   8677 Loss:   0.0662 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11230 batch_limit:   8677 Loss:   0.0654 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11231 batch_limit:   8677 Loss:   0.0662 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11232 batch_limit:   8677 Loss:   0.0658 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11233 batch_limit:   8677 Loss:   0.0647 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11234 batch_limit:   8677 Loss:   0.0656 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11235 batch_limit:   8677 Loss:   0.0654 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11236 batch_limit:   8677 Loss:   0.0645 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11237 batch_limit:   8677 Loss:   0.0657 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11238 batch_limit:   8677 Loss:   0.0660 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11239 batch_limit:   8677 Loss:   0.0653 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11240 batch_limit:   8677 Loss:   0.0659 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11241 batch_limit:   8677 Loss:   0.0659 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11242 batch_limit:   8677 Loss:   0.0647 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11243 batch_limit:   8677 Loss:   0.0652 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11244 batch_limit:   8677 Loss:   0.0654 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11245 batch_limit:   8677 Loss:   0.0644 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11246 batch_limit:   8677 Loss:   0.0651 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11247 batch_limit:   8677 Loss:   0.0659 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11248 batch_limit:   8677 Loss:   0.0652 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11249 batch_limit:   8677 Loss:   0.0654 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11250 batch_limit:   8677 Loss:   0.0660 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11251 batch_limit:   8677 Loss:   0.0648 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11252 batch_limit:   8677 Loss:   0.0646 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11253 batch_limit:   8677 Loss:   0.0654 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11254 batch_limit:   8677 Loss:   0.0646 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11255 batch_limit:   8677 Loss:   0.0643 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11256 batch_limit:   8677 Loss:   0.0657 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11257 batch_limit:   8677 Loss:   0.0655 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11258 batch_limit:   8677 Loss:   0.0647 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11259 batch_limit:   8677 Loss:   0.0657 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11260 batch_limit:   8677 Loss:   0.0652 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11261 batch_limit:   8677 Loss:   0.0640 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11262 batch_limit:   8677 Loss:   0.0652 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11263 batch_limit:   8677 Loss:   0.0648 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11264 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11265 batch_limit:   8677 Loss:   0.0653 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11266 batch_limit:   8677 Loss:   0.0654 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11267 batch_limit:   8677 Loss:   0.0646 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11268 batch_limit:   8677 Loss:   0.0655 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11269 batch_limit:   8677 Loss:   0.0651 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11270 batch_limit:   8677 Loss:   0.0640 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11271 batch_limit:   8677 Loss:   0.0650 Training Acc:    99.01  6799.01    99.99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11272 batch_limit:   8677 Loss:   0.0646 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11273 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11274 batch_limit:   8677 Loss:   0.0650 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11275 batch_limit:   8677 Loss:   0.0651 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11276 batch_limit:   8677 Loss:   0.0644 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11277 batch_limit:   8677 Loss:   0.0653 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11278 batch_limit:   8677 Loss:   0.0650 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11279 batch_limit:   8677 Loss:   0.0639 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11280 batch_limit:   8677 Loss:   0.0647 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11281 batch_limit:   8677 Loss:   0.0645 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11282 batch_limit:   8677 Loss:   0.0636 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11283 batch_limit:   8677 Loss:   0.0647 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11284 batch_limit:   8677 Loss:   0.0650 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11285 batch_limit:   8677 Loss:   0.0642 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11286 batch_limit:   8677 Loss:   0.0650 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11287 batch_limit:   8677 Loss:   0.0650 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11288 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11289 batch_limit:   8677 Loss:   0.0644 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11290 batch_limit:   8677 Loss:   0.0645 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11291 batch_limit:   8677 Loss:   0.0635 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11292 batch_limit:   8677 Loss:   0.0642 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11293 batch_limit:   8677 Loss:   0.0649 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11294 batch_limit:   8677 Loss:   0.0642 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11295 batch_limit:   8677 Loss:   0.0644 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11296 batch_limit:   8677 Loss:   0.0652 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11297 batch_limit:   8677 Loss:   0.0640 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11298 batch_limit:   8677 Loss:   0.0637 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11299 batch_limit:   8677 Loss:   0.0646 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11300 batch_limit:   8677 Loss:   0.0637 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11301 batch_limit:   8677 Loss:   0.0633 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11302 batch_limit:   8677 Loss:   0.0648 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11303 batch_limit:   8677 Loss:   0.0645 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11304 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11305 batch_limit:   8677 Loss:   0.0650 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11306 batch_limit:   8677 Loss:   0.0644 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11307 batch_limit:   8677 Loss:   0.0632 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11308 batch_limit:   8677 Loss:   0.0645 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11309 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11310 batch_limit:   8677 Loss:   0.0629 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11311 batch_limit:   8677 Loss:   0.0646 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11312 batch_limit:   8677 Loss:   0.0643 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11313 batch_limit:   8677 Loss:   0.0637 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11314 batch_limit:   8677 Loss:   0.0649 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11315 batch_limit:   8677 Loss:   0.0641 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11316 batch_limit:   8677 Loss:   0.0633 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11317 batch_limit:   8677 Loss:   0.0643 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11318 batch_limit:   8677 Loss:   0.0634 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11319 batch_limit:   8677 Loss:   0.0631 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11320 batch_limit:   8677 Loss:   0.0644 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11321 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11322 batch_limit:   8677 Loss:   0.0637 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11323 batch_limit:   8677 Loss:   0.0647 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11324 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11325 batch_limit:   8677 Loss:   0.0633 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11326 batch_limit:   8677 Loss:   0.0641 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11327 batch_limit:   8677 Loss:   0.0632 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11328 batch_limit:   8677 Loss:   0.0630 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11329 batch_limit:   8677 Loss:   0.0641 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11330 batch_limit:   8677 Loss:   0.0636 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11331 batch_limit:   8677 Loss:   0.0634 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11332 batch_limit:   8677 Loss:   0.0645 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11333 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11334 batch_limit:   8677 Loss:   0.0631 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11335 batch_limit:   8677 Loss:   0.0640 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11336 batch_limit:   8677 Loss:   0.0632 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11337 batch_limit:   8677 Loss:   0.0627 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11338 batch_limit:   8677 Loss:   0.0640 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11339 batch_limit:   8677 Loss:   0.0635 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11340 batch_limit:   8677 Loss:   0.0631 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11341 batch_limit:   8677 Loss:   0.0643 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11342 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11343 batch_limit:   8677 Loss:   0.0629 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11344 batch_limit:   8677 Loss:   0.0639 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11345 batch_limit:   8677 Loss:   0.0631 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11346 batch_limit:   8677 Loss:   0.0625 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11347 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11348 batch_limit:   8677 Loss:   0.0632 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11349 batch_limit:   8677 Loss:   0.0629 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11350 batch_limit:   8677 Loss:   0.0642 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11351 batch_limit:   8677 Loss:   0.0637 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11352 batch_limit:   8677 Loss:   0.0630 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11353 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11354 batch_limit:   8677 Loss:   0.0629 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11355 batch_limit:   8677 Loss:   0.0624 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11356 batch_limit:   8677 Loss:   0.0636 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11357 batch_limit:   8677 Loss:   0.0629 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11358 batch_limit:   8677 Loss:   0.0627 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11359 batch_limit:   8677 Loss:   0.0640 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11360 batch_limit:   8677 Loss:   0.0636 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11361 batch_limit:   8677 Loss:   0.0630 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11362 batch_limit:   8677 Loss:   0.0637 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11363 batch_limit:   8677 Loss:   0.0628 Training Acc:    99.01  6799.01    99.99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11364 batch_limit:   8677 Loss:   0.0624 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11365 batch_limit:   8677 Loss:   0.0634 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11366 batch_limit:   8677 Loss:   0.0626 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11367 batch_limit:   8677 Loss:   0.0625 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11368 batch_limit:   8677 Loss:   0.0638 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11369 batch_limit:   8677 Loss:   0.0634 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11370 batch_limit:   8677 Loss:   0.0630 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11371 batch_limit:   8677 Loss:   0.0637 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11372 batch_limit:   8677 Loss:   0.0627 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11373 batch_limit:   8677 Loss:   0.0623 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11374 batch_limit:   8677 Loss:   0.0633 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11375 batch_limit:   8677 Loss:   0.0624 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11376 batch_limit:   8677 Loss:   0.0622 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11377 batch_limit:   8677 Loss:   0.0637 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11378 batch_limit:   8677 Loss:   0.0632 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11379 batch_limit:   8677 Loss:   0.0629 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11380 batch_limit:   8677 Loss:   0.0636 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11381 batch_limit:   8677 Loss:   0.0626 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11382 batch_limit:   8677 Loss:   0.0621 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11383 batch_limit:   8677 Loss:   0.0631 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11384 batch_limit:   8677 Loss:   0.0622 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11385 batch_limit:   8677 Loss:   0.0620 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11386 batch_limit:   8677 Loss:   0.0635 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11387 batch_limit:   8677 Loss:   0.0630 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11388 batch_limit:   8677 Loss:   0.0627 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11389 batch_limit:   8677 Loss:   0.0635 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11390 batch_limit:   8677 Loss:   0.0625 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11391 batch_limit:   8677 Loss:   0.0620 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11392 batch_limit:   8677 Loss:   0.0630 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11393 batch_limit:   8677 Loss:   0.0621 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11394 batch_limit:   8677 Loss:   0.0619 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11395 batch_limit:   8677 Loss:   0.0633 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11396 batch_limit:   8677 Loss:   0.0628 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11397 batch_limit:   8677 Loss:   0.0625 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11398 batch_limit:   8677 Loss:   0.0634 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11399 batch_limit:   8677 Loss:   0.0623 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11400 batch_limit:   8677 Loss:   0.0619 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11401 batch_limit:   8677 Loss:   0.0629 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11402 batch_limit:   8677 Loss:   0.0619 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11403 batch_limit:   8677 Loss:   0.0618 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11404 batch_limit:   8677 Loss:   0.0631 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11405 batch_limit:   8677 Loss:   0.0626 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11406 batch_limit:   8677 Loss:   0.0624 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11407 batch_limit:   8677 Loss:   0.0632 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11408 batch_limit:   8677 Loss:   0.0622 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11409 batch_limit:   8677 Loss:   0.0618 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11410 batch_limit:   8677 Loss:   0.0627 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11411 batch_limit:   8677 Loss:   0.0618 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11412 batch_limit:   8677 Loss:   0.0616 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11413 batch_limit:   8677 Loss:   0.0630 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11414 batch_limit:   8677 Loss:   0.0625 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11415 batch_limit:   8677 Loss:   0.0622 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11416 batch_limit:   8677 Loss:   0.0631 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11417 batch_limit:   8677 Loss:   0.0621 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11418 batch_limit:   8677 Loss:   0.0616 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11419 batch_limit:   8677 Loss:   0.0626 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11420 batch_limit:   8677 Loss:   0.0616 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11421 batch_limit:   8677 Loss:   0.0615 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11422 batch_limit:   8677 Loss:   0.0628 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11423 batch_limit:   8677 Loss:   0.0623 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11424 batch_limit:   8677 Loss:   0.0621 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11425 batch_limit:   8677 Loss:   0.0630 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11426 batch_limit:   8677 Loss:   0.0620 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11427 batch_limit:   8677 Loss:   0.0615 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11428 batch_limit:   8677 Loss:   0.0625 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11429 batch_limit:   8677 Loss:   0.0615 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11430 batch_limit:   8677 Loss:   0.0613 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11431 batch_limit:   8677 Loss:   0.0627 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11432 batch_limit:   8677 Loss:   0.0621 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11433 batch_limit:   8677 Loss:   0.0620 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11434 batch_limit:   8677 Loss:   0.0629 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11435 batch_limit:   8677 Loss:   0.0619 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11436 batch_limit:   8677 Loss:   0.0614 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11437 batch_limit:   8677 Loss:   0.0624 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11438 batch_limit:   8677 Loss:   0.0613 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11439 batch_limit:   8677 Loss:   0.0612 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11440 batch_limit:   8677 Loss:   0.0625 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11441 batch_limit:   8677 Loss:   0.0620 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11442 batch_limit:   8677 Loss:   0.0619 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11443 batch_limit:   8677 Loss:   0.0628 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11444 batch_limit:   8677 Loss:   0.0618 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11445 batch_limit:   8677 Loss:   0.0613 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11446 batch_limit:   8677 Loss:   0.0622 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11447 batch_limit:   8677 Loss:   0.0612 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11448 batch_limit:   8677 Loss:   0.0611 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11449 batch_limit:   8677 Loss:   0.0624 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11450 batch_limit:   8677 Loss:   0.0618 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11451 batch_limit:   8677 Loss:   0.0617 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11452 batch_limit:   8677 Loss:   0.0627 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11453 batch_limit:   8677 Loss:   0.0617 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11454 batch_limit:   8677 Loss:   0.0612 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11455 batch_limit:   8677 Loss:   0.0621 Training Acc:    99.01  6799.01    99.99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11456 batch_limit:   8677 Loss:   0.0611 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11457 batch_limit:   8677 Loss:   0.0609 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11458 batch_limit:   8677 Loss:   0.0622 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11459 batch_limit:   8677 Loss:   0.0617 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11460 batch_limit:   8677 Loss:   0.0616 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11461 batch_limit:   8677 Loss:   0.0625 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11462 batch_limit:   8677 Loss:   0.0616 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11463 batch_limit:   8677 Loss:   0.0611 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11464 batch_limit:   8677 Loss:   0.0620 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11465 batch_limit:   8677 Loss:   0.0609 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11466 batch_limit:   8677 Loss:   0.0608 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11467 batch_limit:   8677 Loss:   0.0621 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11468 batch_limit:   8677 Loss:   0.0615 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11469 batch_limit:   8677 Loss:   0.0615 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11470 batch_limit:   8677 Loss:   0.0624 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11471 batch_limit:   8677 Loss:   0.0615 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11472 batch_limit:   8677 Loss:   0.0610 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11473 batch_limit:   8677 Loss:   0.0619 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11474 batch_limit:   8677 Loss:   0.0608 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11475 batch_limit:   8677 Loss:   0.0607 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11476 batch_limit:   8677 Loss:   0.0620 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11477 batch_limit:   8677 Loss:   0.0614 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11478 batch_limit:   8677 Loss:   0.0613 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11479 batch_limit:   8677 Loss:   0.0623 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11480 batch_limit:   8677 Loss:   0.0614 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11481 batch_limit:   8677 Loss:   0.0609 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11482 batch_limit:   8677 Loss:   0.0618 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11483 batch_limit:   8677 Loss:   0.0607 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11484 batch_limit:   8677 Loss:   0.0606 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11485 batch_limit:   8677 Loss:   0.0618 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11486 batch_limit:   8677 Loss:   0.0612 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11487 batch_limit:   8677 Loss:   0.0612 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11488 batch_limit:   8677 Loss:   0.0622 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11489 batch_limit:   8677 Loss:   0.0613 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11490 batch_limit:   8677 Loss:   0.0608 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11491 batch_limit:   8677 Loss:   0.0616 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11492 batch_limit:   8677 Loss:   0.0606 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11493 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11494 batch_limit:   8677 Loss:   0.0617 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11495 batch_limit:   8677 Loss:   0.0611 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11496 batch_limit:   8677 Loss:   0.0611 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11497 batch_limit:   8677 Loss:   0.0621 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11498 batch_limit:   8677 Loss:   0.0612 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11499 batch_limit:   8677 Loss:   0.0607 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11500 batch_limit:   8677 Loss:   0.0615 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11501 batch_limit:   8677 Loss:   0.0604 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11502 batch_limit:   8677 Loss:   0.0604 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11503 batch_limit:   8677 Loss:   0.0615 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11504 batch_limit:   8677 Loss:   0.0609 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11505 batch_limit:   8677 Loss:   0.0609 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11506 batch_limit:   8677 Loss:   0.0619 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11507 batch_limit:   8677 Loss:   0.0611 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11508 batch_limit:   8677 Loss:   0.0606 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11509 batch_limit:   8677 Loss:   0.0614 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11510 batch_limit:   8677 Loss:   0.0603 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11511 batch_limit:   8677 Loss:   0.0603 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11512 batch_limit:   8677 Loss:   0.0614 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11513 batch_limit:   8677 Loss:   0.0607 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11514 batch_limit:   8677 Loss:   0.0608 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11515 batch_limit:   8677 Loss:   0.0618 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11516 batch_limit:   8677 Loss:   0.0610 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11517 batch_limit:   8677 Loss:   0.0606 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11518 batch_limit:   8677 Loss:   0.0613 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11519 batch_limit:   8677 Loss:   0.0602 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11520 batch_limit:   8677 Loss:   0.0601 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11521 batch_limit:   8677 Loss:   0.0613 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11522 batch_limit:   8677 Loss:   0.0606 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11523 batch_limit:   8677 Loss:   0.0607 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11524 batch_limit:   8677 Loss:   0.0617 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11525 batch_limit:   8677 Loss:   0.0609 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11526 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11527 batch_limit:   8677 Loss:   0.0612 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11528 batch_limit:   8677 Loss:   0.0601 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11529 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11530 batch_limit:   8677 Loss:   0.0611 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11531 batch_limit:   8677 Loss:   0.0604 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11532 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11533 batch_limit:   8677 Loss:   0.0616 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11534 batch_limit:   8677 Loss:   0.0608 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11535 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11536 batch_limit:   8677 Loss:   0.0611 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11537 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11538 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11539 batch_limit:   8677 Loss:   0.0610 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11540 batch_limit:   8677 Loss:   0.0602 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11541 batch_limit:   8677 Loss:   0.0604 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11542 batch_limit:   8677 Loss:   0.0614 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11543 batch_limit:   8677 Loss:   0.0608 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11544 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11545 batch_limit:   8677 Loss:   0.0611 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11546 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11547 batch_limit:   8677 Loss:   0.0599 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11548 batch_limit:   8677 Loss:   0.0608 Training Acc:    99.01  6799.01    99.99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11549 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11550 batch_limit:   8677 Loss:   0.0602 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11551 batch_limit:   8677 Loss:   0.0613 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11552 batch_limit:   8677 Loss:   0.0607 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11553 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11554 batch_limit:   8677 Loss:   0.0610 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11555 batch_limit:   8677 Loss:   0.0599 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11556 batch_limit:   8677 Loss:   0.0598 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11557 batch_limit:   8677 Loss:   0.0607 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11558 batch_limit:   8677 Loss:   0.0598 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11559 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11560 batch_limit:   8677 Loss:   0.0612 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11561 batch_limit:   8677 Loss:   0.0606 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11562 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11563 batch_limit:   8677 Loss:   0.0610 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11564 batch_limit:   8677 Loss:   0.0599 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11565 batch_limit:   8677 Loss:   0.0597 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11566 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11567 batch_limit:   8677 Loss:   0.0596 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11568 batch_limit:   8677 Loss:   0.0598 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11569 batch_limit:   8677 Loss:   0.0610 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11570 batch_limit:   8677 Loss:   0.0604 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11571 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11572 batch_limit:   8677 Loss:   0.0610 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11573 batch_limit:   8677 Loss:   0.0598 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11574 batch_limit:   8677 Loss:   0.0597 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11575 batch_limit:   8677 Loss:   0.0604 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11576 batch_limit:   8677 Loss:   0.0594 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11577 batch_limit:   8677 Loss:   0.0597 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11578 batch_limit:   8677 Loss:   0.0609 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11579 batch_limit:   8677 Loss:   0.0603 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11580 batch_limit:   8677 Loss:   0.0603 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11581 batch_limit:   8677 Loss:   0.0609 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11582 batch_limit:   8677 Loss:   0.0597 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11583 batch_limit:   8677 Loss:   0.0596 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11584 batch_limit:   8677 Loss:   0.0603 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11585 batch_limit:   8677 Loss:   0.0593 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11586 batch_limit:   8677 Loss:   0.0596 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11587 batch_limit:   8677 Loss:   0.0608 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11588 batch_limit:   8677 Loss:   0.0601 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11589 batch_limit:   8677 Loss:   0.0602 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11590 batch_limit:   8677 Loss:   0.0608 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11591 batch_limit:   8677 Loss:   0.0596 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11592 batch_limit:   8677 Loss:   0.0595 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11593 batch_limit:   8677 Loss:   0.0602 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11594 batch_limit:   8677 Loss:   0.0592 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11595 batch_limit:   8677 Loss:   0.0595 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11596 batch_limit:   8677 Loss:   0.0606 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11597 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11598 batch_limit:   8677 Loss:   0.0601 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11599 batch_limit:   8677 Loss:   0.0607 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11600 batch_limit:   8677 Loss:   0.0595 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11601 batch_limit:   8677 Loss:   0.0594 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11602 batch_limit:   8677 Loss:   0.0601 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11603 batch_limit:   8677 Loss:   0.0590 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11604 batch_limit:   8677 Loss:   0.0593 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11605 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11606 batch_limit:   8677 Loss:   0.0599 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11607 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11608 batch_limit:   8677 Loss:   0.0606 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11609 batch_limit:   8677 Loss:   0.0594 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11610 batch_limit:   8677 Loss:   0.0593 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11611 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11612 batch_limit:   8677 Loss:   0.0589 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11613 batch_limit:   8677 Loss:   0.0592 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11614 batch_limit:   8677 Loss:   0.0604 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11615 batch_limit:   8677 Loss:   0.0598 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11616 batch_limit:   8677 Loss:   0.0599 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11617 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11618 batch_limit:   8677 Loss:   0.0593 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11619 batch_limit:   8677 Loss:   0.0593 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11620 batch_limit:   8677 Loss:   0.0599 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11621 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11622 batch_limit:   8677 Loss:   0.0591 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11623 batch_limit:   8677 Loss:   0.0603 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11624 batch_limit:   8677 Loss:   0.0596 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11625 batch_limit:   8677 Loss:   0.0598 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11626 batch_limit:   8677 Loss:   0.0605 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11627 batch_limit:   8677 Loss:   0.0592 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11628 batch_limit:   8677 Loss:   0.0592 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11629 batch_limit:   8677 Loss:   0.0598 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11630 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11631 batch_limit:   8677 Loss:   0.0590 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11632 batch_limit:   8677 Loss:   0.0602 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11633 batch_limit:   8677 Loss:   0.0595 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11634 batch_limit:   8677 Loss:   0.0597 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11635 batch_limit:   8677 Loss:   0.0603 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11636 batch_limit:   8677 Loss:   0.0591 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11637 batch_limit:   8677 Loss:   0.0591 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11638 batch_limit:   8677 Loss:   0.0597 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11639 batch_limit:   8677 Loss:   0.0586 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11640 batch_limit:   8677 Loss:   0.0589 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11641 batch_limit:   8677 Loss:   0.0601 Training Acc:    99.01  6799.01    99.99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11642 batch_limit:   8677 Loss:   0.0594 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11643 batch_limit:   8677 Loss:   0.0596 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11644 batch_limit:   8677 Loss:   0.0602 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11645 batch_limit:   8677 Loss:   0.0590 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11646 batch_limit:   8677 Loss:   0.0590 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11647 batch_limit:   8677 Loss:   0.0596 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11648 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11649 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11650 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11651 batch_limit:   8677 Loss:   0.0593 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11652 batch_limit:   8677 Loss:   0.0595 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11653 batch_limit:   8677 Loss:   0.0601 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11654 batch_limit:   8677 Loss:   0.0590 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11655 batch_limit:   8677 Loss:   0.0589 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11656 batch_limit:   8677 Loss:   0.0596 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11657 batch_limit:   8677 Loss:   0.0584 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11658 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11659 batch_limit:   8677 Loss:   0.0599 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11660 batch_limit:   8677 Loss:   0.0592 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11661 batch_limit:   8677 Loss:   0.0594 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11662 batch_limit:   8677 Loss:   0.0601 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11663 batch_limit:   8677 Loss:   0.0589 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11664 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11665 batch_limit:   8677 Loss:   0.0595 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11666 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11667 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11668 batch_limit:   8677 Loss:   0.0598 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11669 batch_limit:   8677 Loss:   0.0591 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11670 batch_limit:   8677 Loss:   0.0594 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11671 batch_limit:   8677 Loss:   0.0600 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11672 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11673 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11674 batch_limit:   8677 Loss:   0.0594 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11675 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11676 batch_limit:   8677 Loss:   0.0586 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11677 batch_limit:   8677 Loss:   0.0596 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11678 batch_limit:   8677 Loss:   0.0589 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11679 batch_limit:   8677 Loss:   0.0593 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11680 batch_limit:   8677 Loss:   0.0599 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11681 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11682 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11683 batch_limit:   8677 Loss:   0.0593 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11684 batch_limit:   8677 Loss:   0.0581 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11685 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11686 batch_limit:   8677 Loss:   0.0595 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11687 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11688 batch_limit:   8677 Loss:   0.0592 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11689 batch_limit:   8677 Loss:   0.0598 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11690 batch_limit:   8677 Loss:   0.0586 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11691 batch_limit:   8677 Loss:   0.0586 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11692 batch_limit:   8677 Loss:   0.0592 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11693 batch_limit:   8677 Loss:   0.0580 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11694 batch_limit:   8677 Loss:   0.0584 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11695 batch_limit:   8677 Loss:   0.0594 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11696 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11697 batch_limit:   8677 Loss:   0.0591 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11698 batch_limit:   8677 Loss:   0.0597 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11699 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11700 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11701 batch_limit:   8677 Loss:   0.0591 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11702 batch_limit:   8677 Loss:   0.0579 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11703 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11704 batch_limit:   8677 Loss:   0.0593 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11705 batch_limit:   8677 Loss:   0.0586 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11706 batch_limit:   8677 Loss:   0.0590 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11707 batch_limit:   8677 Loss:   0.0596 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11708 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11709 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11710 batch_limit:   8677 Loss:   0.0590 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11711 batch_limit:   8677 Loss:   0.0578 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11712 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11713 batch_limit:   8677 Loss:   0.0592 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11714 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11715 batch_limit:   8677 Loss:   0.0589 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11716 batch_limit:   8677 Loss:   0.0595 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11717 batch_limit:   8677 Loss:   0.0584 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11718 batch_limit:   8677 Loss:   0.0584 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11719 batch_limit:   8677 Loss:   0.0589 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11720 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11721 batch_limit:   8677 Loss:   0.0581 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11722 batch_limit:   8677 Loss:   0.0591 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11723 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11724 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11725 batch_limit:   8677 Loss:   0.0594 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11726 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11727 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11728 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11729 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11730 batch_limit:   8677 Loss:   0.0580 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11731 batch_limit:   8677 Loss:   0.0590 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11732 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11733 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11734 batch_limit:   8677 Loss:   0.0593 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11735 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11736 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11737 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11738 batch_limit:   8677 Loss:   0.0575 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11739 batch_limit:   8677 Loss:   0.0580 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11740 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11741 batch_limit:   8677 Loss:   0.0580 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11742 batch_limit:   8677 Loss:   0.0586 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11743 batch_limit:   8677 Loss:   0.0592 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11744 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11745 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11746 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11747 batch_limit:   8677 Loss:   0.0574 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11748 batch_limit:   8677 Loss:   0.0579 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11749 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11750 batch_limit:   8677 Loss:   0.0579 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11751 batch_limit:   8677 Loss:   0.0584 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11752 batch_limit:   8677 Loss:   0.0591 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11753 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11754 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11755 batch_limit:   8677 Loss:   0.0586 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11756 batch_limit:   8677 Loss:   0.0574 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11757 batch_limit:   8677 Loss:   0.0578 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11758 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11759 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11760 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11761 batch_limit:   8677 Loss:   0.0590 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11762 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11763 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11764 batch_limit:   8677 Loss:   0.0586 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11765 batch_limit:   8677 Loss:   0.0574 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11766 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11767 batch_limit:   8677 Loss:   0.0584 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11768 batch_limit:   8677 Loss:   0.0575 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11769 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11770 batch_limit:   8677 Loss:   0.0589 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11771 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11772 batch_limit:   8677 Loss:   0.0584 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11773 batch_limit:   8677 Loss:   0.0586 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11774 batch_limit:   8677 Loss:   0.0574 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11775 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11776 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11777 batch_limit:   8677 Loss:   0.0572 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11778 batch_limit:   8677 Loss:   0.0580 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11779 batch_limit:   8677 Loss:   0.0588 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11780 batch_limit:   8677 Loss:   0.0581 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11781 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11782 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11783 batch_limit:   8677 Loss:   0.0574 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11784 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11785 batch_limit:   8677 Loss:   0.0581 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11786 batch_limit:   8677 Loss:   0.0570 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11787 batch_limit:   8677 Loss:   0.0578 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11788 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11789 batch_limit:   8677 Loss:   0.0579 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11790 batch_limit:   8677 Loss:   0.0584 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11791 batch_limit:   8677 Loss:   0.0587 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11792 batch_limit:   8677 Loss:   0.0573 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11793 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11794 batch_limit:   8677 Loss:   0.0580 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11795 batch_limit:   8677 Loss:   0.0569 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11796 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11797 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11798 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11799 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11800 batch_limit:   8677 Loss:   0.0586 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11801 batch_limit:   8677 Loss:   0.0573 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11802 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11803 batch_limit:   8677 Loss:   0.0579 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11804 batch_limit:   8677 Loss:   0.0568 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11805 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11806 batch_limit:   8677 Loss:   0.0584 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11807 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11808 batch_limit:   8677 Loss:   0.0581 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11809 batch_limit:   8677 Loss:   0.0585 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11810 batch_limit:   8677 Loss:   0.0572 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11811 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11812 batch_limit:   8677 Loss:   0.0579 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11813 batch_limit:   8677 Loss:   0.0567 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11814 batch_limit:   8677 Loss:   0.0575 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11815 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11816 batch_limit:   8677 Loss:   0.0574 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11817 batch_limit:   8677 Loss:   0.0581 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11818 batch_limit:   8677 Loss:   0.0584 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11819 batch_limit:   8677 Loss:   0.0572 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11820 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11821 batch_limit:   8677 Loss:   0.0578 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11822 batch_limit:   8677 Loss:   0.0566 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11823 batch_limit:   8677 Loss:   0.0574 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11824 batch_limit:   8677 Loss:   0.0581 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11825 batch_limit:   8677 Loss:   0.0573 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11826 batch_limit:   8677 Loss:   0.0580 Training Acc:    99.01  6799.01    99.99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11827 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11828 batch_limit:   8677 Loss:   0.0572 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11829 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11830 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11831 batch_limit:   8677 Loss:   0.0565 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11832 batch_limit:   8677 Loss:   0.0573 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11833 batch_limit:   8677 Loss:   0.0579 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11834 batch_limit:   8677 Loss:   0.0571 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11835 batch_limit:   8677 Loss:   0.0579 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11836 batch_limit:   8677 Loss:   0.0583 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11837 batch_limit:   8677 Loss:   0.0572 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11838 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11839 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11840 batch_limit:   8677 Loss:   0.0564 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11841 batch_limit:   8677 Loss:   0.0572 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11842 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11843 batch_limit:   8677 Loss:   0.0570 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11844 batch_limit:   8677 Loss:   0.0579 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11845 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11846 batch_limit:   8677 Loss:   0.0572 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11847 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11848 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11849 batch_limit:   8677 Loss:   0.0564 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11850 batch_limit:   8677 Loss:   0.0572 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11851 batch_limit:   8677 Loss:   0.0575 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11852 batch_limit:   8677 Loss:   0.0567 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11853 batch_limit:   8677 Loss:   0.0578 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11854 batch_limit:   8677 Loss:   0.0580 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11855 batch_limit:   8677 Loss:   0.0573 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11856 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11857 batch_limit:   8677 Loss:   0.0575 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11858 batch_limit:   8677 Loss:   0.0564 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11859 batch_limit:   8677 Loss:   0.0572 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11860 batch_limit:   8677 Loss:   0.0573 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11861 batch_limit:   8677 Loss:   0.0565 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11862 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11863 batch_limit:   8677 Loss:   0.0579 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11864 batch_limit:   8677 Loss:   0.0573 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11865 batch_limit:   8677 Loss:   0.0580 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11866 batch_limit:   8677 Loss:   0.0575 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11867 batch_limit:   8677 Loss:   0.0564 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11868 batch_limit:   8677 Loss:   0.0573 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11869 batch_limit:   8677 Loss:   0.0570 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11870 batch_limit:   8677 Loss:   0.0562 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11871 batch_limit:   8677 Loss:   0.0577 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11872 batch_limit:   8677 Loss:   0.0576 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11873 batch_limit:   8677 Loss:   0.0572 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11874 batch_limit:   8677 Loss:   0.0582 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11875 batch_limit:   8677 Loss:   0.0575 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11876 batch_limit:   8677 Loss:   0.0565 Training Acc:    99.01  6799.01    99.99%\n",
      "Epoch  11877 batch_limit:   8677 Loss:   0.0574 Training Acc:   100.00  6800.00   100.00%\n",
      "batch_limit    128 Training Acc:   100.00   100.00   100.00%\n",
      "batch_limit    256 Training Acc:   100.00   200.00   100.00%\n",
      "batch_limit    384 Training Acc:   100.00   300.00   100.00%\n",
      "batch_limit    512 Training Acc:   100.00   400.00   100.00%\n",
      "batch_limit    640 Training Acc:   100.00   500.00   100.00%\n",
      "batch_limit    768 Training Acc:   100.00   600.00   100.00%\n",
      "batch_limit    896 Training Acc:   100.00   700.00   100.00%\n",
      "batch_limit   1024 Training Acc:   100.00   800.00   100.00%\n",
      "batch_limit   1152 Training Acc:   100.00   900.00   100.00%\n",
      "batch_limit   1280 Training Acc:   100.00  1000.00   100.00%\n",
      "batch_limit   1408 Training Acc:   100.00  1100.00   100.00%\n",
      "batch_limit   1536 Training Acc:   100.00  1200.00   100.00%\n",
      "batch_limit   1664 Training Acc:   100.00  1300.00   100.00%\n",
      "batch_limit   1792 Training Acc:   100.00  1400.00   100.00%\n",
      "batch_limit   1920 Training Acc:   100.00  1500.00   100.00%\n",
      "batch_limit   2048 Training Acc:   100.00  1600.00   100.00%\n",
      "batch_limit   2176 Training Acc:   100.00  1700.00   100.00%\n",
      "batch_limit   2304 Training Acc:   100.00  1800.00   100.00%\n",
      "batch_limit   2432 Training Acc:   100.00  1900.00   100.00%\n",
      "batch_limit   2560 Training Acc:   100.00  2000.00   100.00%\n",
      "batch_limit   2688 Training Acc:   100.00  2100.00   100.00%\n",
      "batch_limit   2816 Training Acc:   100.00  2200.00   100.00%\n",
      "batch_limit   2944 Training Acc:   100.00  2300.00   100.00%\n",
      "batch_limit   3072 Training Acc:   100.00  2400.00   100.00%\n",
      "batch_limit   3200 Training Acc:   100.00  2500.00   100.00%\n",
      "batch_limit   3328 Training Acc:   100.00  2600.00   100.00%\n",
      "batch_limit   3456 Training Acc:   100.00  2700.00   100.00%\n",
      "batch_limit   3584 Training Acc:   100.00  2800.00   100.00%\n",
      "batch_limit   3712 Training Acc:   100.00  2900.00   100.00%\n",
      "batch_limit   3840 Training Acc:   100.00  3000.00   100.00%\n",
      "batch_limit   3968 Training Acc:   100.00  3100.00   100.00%\n",
      "batch_limit   4096 Training Acc:   100.00  3200.00   100.00%\n",
      "batch_limit   4224 Training Acc:   100.00  3300.00   100.00%\n",
      "batch_limit   4352 Training Acc:   100.00  3400.00   100.00%\n",
      "batch_limit   4480 Training Acc:   100.00  3500.00   100.00%\n",
      "batch_limit   4608 Training Acc:   100.00  3600.00   100.00%\n",
      "batch_limit   4736 Training Acc:   100.00  3700.00   100.00%\n",
      "batch_limit   4864 Training Acc:   100.00  3800.00   100.00%\n",
      "batch_limit   4992 Training Acc:   100.00  3900.00   100.00%\n",
      "batch_limit   5120 Training Acc:   100.00  4000.00   100.00%\n",
      "batch_limit   5248 Training Acc:   100.00  4100.00   100.00%\n",
      "batch_limit   5376 Training Acc:   100.00  4200.00   100.00%\n",
      "batch_limit   5504 Training Acc:   100.00  4300.00   100.00%\n",
      "batch_limit   5632 Training Acc:   100.00  4400.00   100.00%\n",
      "batch_limit   5760 Training Acc:   100.00  4500.00   100.00%\n",
      "batch_limit   5888 Training Acc:   100.00  4600.00   100.00%\n",
      "batch_limit   6016 Training Acc:   100.00  4700.00   100.00%\n",
      "batch_limit   6144 Training Acc:   100.00  4800.00   100.00%\n",
      "batch_limit   6272 Training Acc:   100.00  4900.00   100.00%\n",
      "batch_limit   6400 Training Acc:   100.00  5000.00   100.00%\n",
      "batch_limit   6528 Training Acc:   100.00  5100.00   100.00%\n",
      "batch_limit   6656 Training Acc:   100.00  5200.00   100.00%\n",
      "batch_limit   6784 Training Acc:   100.00  5300.00   100.00%\n",
      "batch_limit   6912 Training Acc:   100.00  5400.00   100.00%\n",
      "batch_limit   7040 Training Acc:   100.00  5500.00   100.00%\n",
      "batch_limit   7168 Training Acc:   100.00  5600.00   100.00%\n",
      "batch_limit   7296 Training Acc:   100.00  5700.00   100.00%\n",
      "batch_limit   7424 Training Acc:   100.00  5800.00   100.00%\n",
      "batch_limit   7552 Training Acc:   100.00  5900.00   100.00%\n",
      "batch_limit   7680 Training Acc:   100.00  6000.00   100.00%\n",
      "batch_limit   7808 Training Acc:   100.00  6100.00   100.00%\n",
      "batch_limit   7936 Training Acc:   100.00  6200.00   100.00%\n",
      "batch_limit   8064 Training Acc:   100.00  6300.00   100.00%\n",
      "batch_limit   8192 Training Acc:   100.00  6400.00   100.00%\n",
      "batch_limit   8320 Training Acc:   100.00  6500.00   100.00%\n",
      "batch_limit   8448 Training Acc:   100.00  6600.00   100.00%\n",
      "batch_limit   8576 Training Acc:   100.00  6700.00   100.00%\n",
      "batch_limit   8677 Training Acc:   100.00  6800.00   100.00%\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "\n",
    "acc = 0\n",
    "acc_cum = 0\n",
    "acc_temp = 0\n",
    "epoch_idx = 0\n",
    "loss_disp_true = False\n",
    "\n",
    "with tf.Session() as session:\n",
    "# with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "#with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    while acc < 100:\n",
    "        epoch_idx += 1\n",
    "        acc_cum = 0\n",
    "        for step in range(int(num_batches)):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "        \n",
    "            offset = (step * batch_size)\n",
    "            \n",
    "            if num_samples > offset + batch_size:\n",
    "                batch_limit = offset + batch_size\n",
    "            else:\n",
    "                batch_limit = num_samples\n",
    "                loss_disp_true = True\n",
    "                       \n",
    "            # offset = np.random.randint(num_samples - batch_size , size = 1)[0]\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset_reform[offset:(batch_limit), :]\n",
    "            batch_labels = train_label_hot[offset:(batch_limit), :]\n",
    "\n",
    "            # print(offset)\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, dropout_prob : 1}\n",
    "            _, l, predictions, added_summary, logits_check = session.run(\n",
    "                [optimizer, loss, train_prediction, loss_summary, logits], feed_dict=feed_dict)\n",
    "\n",
    "            # writer.add_summary(added_summary)\n",
    "            acc_temp =  accuracy_multilabel_softmax(predictions, batch_labels)\n",
    "            acc_cum += acc_temp\n",
    "            acc = acc_cum/(step+1)\n",
    "            if loss_disp_true:\n",
    "                loss_disp_true = False\n",
    "                loss_history.append(l)\n",
    "                print(\"Epoch {0:6d} batch_limit: {1:6d} Loss: {2:8.4f} Training Acc: {3:8.2f} {4:8.2f} {5:8.2f}%\".format(epoch_idx, batch_limit, l, acc_temp, acc_cum, acc))\n",
    "\n",
    "    # Do for the test batch\n",
    "    acc = 0\n",
    "    acc_cum = 0\n",
    "    for step in range(int(num_batches)):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) #  % (train_label_hot.shape[0] - batch_size)\n",
    "        \n",
    "        if num_samples > offset + batch_size:\n",
    "            batch_limit = offset + batch_size\n",
    "        else:\n",
    "            batch_limit = num_samples\n",
    "            loss_disp_true = True\n",
    "\n",
    "        batch_data = train_dataset_reform[offset:(batch_limit), :]\n",
    "        batch_labels = train_label_hot[offset:(batch_limit), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, dropout_prob : 1}\n",
    "        test_predictions = session.run(train_prediction, feed_dict=feed_dict)\n",
    "        batch_acc = accuracy_multilabel_softmax(test_predictions, batch_labels)\n",
    "        acc_cum = acc_cum + batch_acc\n",
    "        acc = acc_cum/(step+1)\n",
    "        \n",
    "        print(\"batch_limit {0:6d} Training Acc: {1:8.2f} {2:8.2f} {3:8.2f}%\".format(batch_limit, batch_acc, acc_cum, acc))\n",
    "\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = str(hidden_size)+str(hidden_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (18.0, 8.0)\n",
    "plt.style.use('ggplot')\n",
    "plt.plot(loss_history)\n",
    "fig = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "fig.savefig(f.name[:-4]+hidden+'nd.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5384745e-01, 7.4615252e-01],\n",
       "       [2.0434979e-01, 7.9565012e-01],\n",
       "       [1.6374715e-01, 8.3625287e-01],\n",
       "       [1.1730880e-01, 8.8269120e-01],\n",
       "       [6.9220319e-02, 9.3077970e-01],\n",
       "       [3.9688896e-02, 9.6031117e-01],\n",
       "       [2.2599647e-02, 9.7740036e-01],\n",
       "       [9.8537682e-03, 9.9014622e-01],\n",
       "       [3.0065891e-03, 9.9699342e-01],\n",
       "       [8.2764920e-04, 9.9917233e-01],\n",
       "       [2.3163472e-04, 9.9976832e-01],\n",
       "       [3.7650520e-05, 9.9996233e-01],\n",
       "       [2.7037618e-06, 9.9999726e-01],\n",
       "       [2.2148234e-07, 9.9999976e-01],\n",
       "       [2.3519155e-08, 1.0000000e+00],\n",
       "       [1.4293515e-09, 1.0000000e+00],\n",
       "       [5.6932287e-10, 1.0000000e+00],\n",
       "       [6.6213697e-12, 1.0000000e+00],\n",
       "       [1.0000000e+00, 5.7688329e-08],\n",
       "       [1.0000000e+00, 1.8921666e-14],\n",
       "       [1.0000000e+00, 1.9000734e-13],\n",
       "       [1.0000000e+00, 2.6629274e-10],\n",
       "       [9.9999988e-01, 1.2320911e-07],\n",
       "       [9.9968171e-01, 3.1836570e-04],\n",
       "       [9.9072427e-01, 9.2756785e-03],\n",
       "       [9.7556782e-01, 2.4432251e-02],\n",
       "       [9.6865094e-01, 3.1349111e-02],\n",
       "       [9.6470672e-01, 3.5293359e-02],\n",
       "       [9.6299464e-01, 3.7005376e-02],\n",
       "       [9.6244144e-01, 3.7558511e-02],\n",
       "       [9.6243495e-01, 3.7565064e-02],\n",
       "       [9.6222651e-01, 3.7773456e-02],\n",
       "       [9.6209872e-01, 3.7901297e-02],\n",
       "       [9.6206200e-01, 3.7938032e-02],\n",
       "       [9.6371210e-01, 3.6287934e-02],\n",
       "       [9.6351576e-01, 3.6484279e-02],\n",
       "       [9.6330684e-01, 3.6693174e-02],\n",
       "       [9.6316463e-01, 3.6835425e-02],\n",
       "       [9.6319348e-01, 3.6806468e-02],\n",
       "       [9.6307671e-01, 3.6923360e-02],\n",
       "       [9.6291310e-01, 3.7086893e-02],\n",
       "       [9.6280545e-01, 3.7194595e-02],\n",
       "       [9.6277463e-01, 3.7225343e-02],\n",
       "       [9.6267575e-01, 3.7324321e-02],\n",
       "       [9.6253556e-01, 3.7464444e-02],\n",
       "       [9.6245515e-01, 3.7544865e-02],\n",
       "       [9.6250242e-01, 3.7497614e-02],\n",
       "       [9.6242690e-01, 3.7573136e-02],\n",
       "       [9.6233106e-01, 3.7668988e-02],\n",
       "       [9.6228510e-01, 3.7714925e-02],\n",
       "       [9.6225709e-01, 3.7742898e-02],\n",
       "       [9.6222973e-01, 3.7770268e-02],\n",
       "       [9.6220380e-01, 3.7796203e-02],\n",
       "       [9.6219641e-01, 3.7803624e-02],\n",
       "       [9.6220148e-01, 3.7798490e-02],\n",
       "       [9.6219569e-01, 3.7804250e-02],\n",
       "       [9.6218532e-01, 3.7814658e-02],\n",
       "       [9.6217024e-01, 3.7829719e-02],\n",
       "       [9.6216536e-01, 3.7834577e-02],\n",
       "       [9.6212178e-01, 3.7878208e-02],\n",
       "       [9.6194065e-01, 3.8059350e-02],\n",
       "       [9.6168447e-01, 3.8315613e-02],\n",
       "       [9.6186376e-01, 3.8136311e-02],\n",
       "       [9.6154249e-01, 3.8457550e-02],\n",
       "       [9.6051306e-01, 3.9486967e-02],\n",
       "       [9.5929402e-01, 4.0705945e-02],\n",
       "       [1.0000000e+00, 1.6187755e-12],\n",
       "       [1.0000000e+00, 1.4663381e-12],\n",
       "       [1.0000000e+00, 1.5938793e-12],\n",
       "       [1.0000000e+00, 1.4607413e-12],\n",
       "       [1.0000000e+00, 1.5397840e-12],\n",
       "       [1.0000000e+00, 1.4341583e-12],\n",
       "       [1.0000000e+00, 1.6513737e-12],\n",
       "       [1.0000000e+00, 1.5832231e-12],\n",
       "       [1.0000000e+00, 1.3759428e-12],\n",
       "       [1.0000000e+00, 1.3503079e-12],\n",
       "       [1.0000000e+00, 1.7240882e-12],\n",
       "       [1.0000000e+00, 1.7995514e-12],\n",
       "       [1.0000000e+00, 2.3695026e-12],\n",
       "       [1.0000000e+00, 2.7202155e-12],\n",
       "       [1.0000000e+00, 4.4691820e-12],\n",
       "       [1.0000000e+00, 6.0109712e-12],\n",
       "       [1.0000000e+00, 2.0696421e-10],\n",
       "       [1.0000000e+00, 5.0427573e-10],\n",
       "       [1.0000000e+00, 1.8526135e-09],\n",
       "       [1.0000000e+00, 6.7567418e-09],\n",
       "       [1.0000000e+00, 3.7236937e-08],\n",
       "       [9.9999976e-01, 1.8983489e-07],\n",
       "       [9.9999881e-01, 1.1507378e-06],\n",
       "       [9.9999440e-01, 5.5592786e-06],\n",
       "       [9.9998307e-01, 1.6870819e-05],\n",
       "       [9.9994552e-01, 5.4447188e-05],\n",
       "       [9.9986565e-01, 1.3428177e-04],\n",
       "       [9.9975544e-01, 2.4456112e-04],\n",
       "       [9.9962401e-01, 3.7591631e-04],\n",
       "       [9.9948299e-01, 5.1695551e-04],\n",
       "       [9.9927336e-01, 7.2670571e-04],\n",
       "       [9.9864680e-01, 1.3532038e-03],\n",
       "       [9.9984467e-01, 1.5527732e-04],\n",
       "       [9.9191147e-01, 8.0885952e-03],\n",
       "       [5.0988376e-01, 4.9011621e-01]], dtype=float32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_soft = np.zeros_like(predictions)\n",
    "predictions_soft[np.arange(len(predictions)), predictions.argmax(1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((predictions_soft.astype(int) == batch_labels).any(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(batch_labels[np.arange(len(batch_labels)), predictions_soft.argmax(1)])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
