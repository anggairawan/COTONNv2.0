{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "NN on vehicle\n",
    "\n",
    "Fully connected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plot param\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (28.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, manipulate the input and output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, always assumme that the state represent by single dimension vector\n",
    "in other word it only has one feature.\n",
    "\n",
    "Meanwhile the action size should be check if there is non-determinism in which action the controller should choose (it will influence how the score/loss function will be calculated). For instance, when the state has two ND actions (0 and 1), the loss function of that sample should be zero if the predicted action is action[0] OR action[1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this vehicle case the input size is one (there is no ND on the actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniqueness of the actions would determine the size of the output layer neuron's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the controller file\n",
    "f = open('vehicle.txt', \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrain_dataset = []\n",
    "ltrain_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in lines:\n",
    "    ltrain_dataset.append(x.split(' ')[0].strip('\\n'))\n",
    "    ltrain_label.append(x.split(' ')[1].strip('\\n'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.asarray(ltrain_dataset)\n",
    "train_label = np.asarray(ltrain_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.astype(np.float32)\n",
    "train_label = train_label.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48018,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48018,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_label = len(set(train_label))\n",
    "num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcoll_label = sorted(list(set(train_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 20.0,\n",
       " 21.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 34.0,\n",
       " 35.0,\n",
       " 41.0,\n",
       " 42.0,\n",
       " 43.0,\n",
       " 47.0,\n",
       " 48.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcoll_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcoll_label.index(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48018,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48018,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_dataset[:200], train_label[:200], 'r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('title')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcoll_label.index(train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 48018)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(train_label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_transformed = np.zeros_like(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_label.shape[0]):\n",
    "    train_label_transformed[i] = lcoll_label.index(train_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4., 13., 17., ..., 15., 15.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  147.,   150.,   151., ..., 90665., 90666., 90667.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lcoll_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_hot = (np.arange(len(lcoll_label)) == train_label_transformed[:, None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48018, 18)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_reform = train_dataset[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "# train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset_reform)\n",
    "  tf_train_labels = tf.constant(train_label_hot)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(tf.truncated_normal([1 , num_label]))\n",
    "  biases = tf.Variable(tf.zeros([num_label]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  # valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  # test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 89001.804688\n",
      "Training accuracy: 0.1%\n",
      "Loss at step 100: 111318064.000000\n",
      "Training accuracy: 7.5%\n",
      "Loss at step 200: 90963432.000000\n",
      "Training accuracy: 19.2%\n",
      "Loss at step 300: 76106392.000000\n",
      "Training accuracy: 3.7%\n",
      "Loss at step 400: 86101768.000000\n",
      "Training accuracy: 15.2%\n",
      "Loss at step 500: 124001056.000000\n",
      "Training accuracy: 15.5%\n",
      "Loss at step 600: 118513064.000000\n",
      "Training accuracy: 19.2%\n",
      "Loss at step 700: 81590240.000000\n",
      "Training accuracy: 7.9%\n",
      "Loss at step 800: 77480928.000000\n",
      "Training accuracy: 7.5%\n",
      "Loss at step 900: 106734560.000000\n",
      "Training accuracy: 13.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(predictions, train_label_hot))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Next step\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a n-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and k hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48018, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_reform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48018, 18)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = train_label_hot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48018"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2667.6666666666665"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples/num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(num_samples)\n",
    "hidden_size = 1024\n",
    "hidden_size2 = 512\n",
    "hidden_size3 = 18\n",
    "\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "# Input data. For the training data, we use a placeholder that will be fed\n",
    "# at run time with a training minibatch.\n",
    "tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_label))\n",
    "\n",
    "# Variables.\n",
    "weights1 = tf.Variable(tf.truncated_normal([1, hidden_size]))\n",
    "biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "weights2 = tf.Variable(tf.truncated_normal([hidden_size, hidden_size2]))\n",
    "# biases2 = tf.Variable(tf.zeros([num_label]))\n",
    "biases2 = tf.Variable(tf.zeros([hidden_size2]))\n",
    "\n",
    "weights3 = tf.Variable(tf.truncated_normal([hidden_size2, hidden_size3]))\n",
    "biases3 = tf.Variable(tf.zeros([hidden_size3]))\n",
    "\n",
    "weights4 = tf.Variable(tf.truncated_normal([hidden_size3, num_label]))\n",
    "biases4 = tf.Variable(tf.zeros([num_label]))\n",
    "\n",
    "# Training computation.\n",
    "logits1 = tf.matmul(tf_train_dataset, weights1) + biases1  \n",
    "relu_act_func = tf.nn.relu(logits1)\n",
    "logits2 = tf.matmul(relu_act_func, weights2) + biases2\n",
    "\n",
    "relu_act_func2 = tf.nn.relu(logits2)\n",
    "logits3 = tf.matmul(relu_act_func2, weights3) + biases3\n",
    "\n",
    "relu_act_func3 = tf.nn.relu(logits3)\n",
    "logits4 = tf.matmul(relu_act_func3, weights4) + biases4\n",
    "\n",
    "# loss = tf.reduce_mean(tf.losses.absolute_difference(labels=tf_train_labels, predictions=train_prediction))\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits4))\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "\n",
    "# Optimizer.\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(0.3).minimize(loss)\n",
    "\n",
    "# Predictions for the training, validation, and test data.\n",
    "train_prediction = tf.nn.softmax(logits4)\n",
    "# writer = tf.summary.FileWriter('./vehicle', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48018"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration_one_epoch = np.floor(num_samples / batch_size)\n",
    "iteration_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 39093880.000000\n",
      "Minibatch accuracy: 2.7%\n",
      "Minibatch loss at step 1: 366309216.000000\n",
      "Minibatch accuracy: 0.1%\n",
      "Minibatch loss at step 2: 841069248.000000\n",
      "Minibatch accuracy: 3.7%\n",
      "Minibatch loss at step 3: 556619712.000000\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 4: 297433504.000000\n",
      "Minibatch accuracy: 15.2%\n",
      "Minibatch loss at step 5: 369884000.000000\n",
      "Minibatch accuracy: 3.1%\n",
      "Minibatch loss at step 6: 210125392.000000\n",
      "Minibatch accuracy: 1.3%\n",
      "Minibatch loss at step 7: 56302388.000000\n",
      "Minibatch accuracy: 13.2%\n",
      "Minibatch loss at step 8: 3.215004\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 9: 980650.937500\n",
      "Minibatch accuracy: 3.7%\n",
      "Minibatch loss at step 10: 3.168487\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 11: 1012455.375000\n",
      "Minibatch accuracy: 0.4%\n",
      "Minibatch loss at step 12: 3.131607\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 13: 3.113926\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 14: 3.087184\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 15: 3.053509\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 16: 3.013611\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 17: 1664.578003\n",
      "Minibatch accuracy: 2.7%\n",
      "Minibatch loss at step 18: 2.942914\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 19: 2.906349\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 20: 2.857178\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 21: 2.798018\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 22: 2.733319\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 23: 2.668631\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 24: 2.609562\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 25: 2.561183\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 26: 2.526695\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 27: 2.507530\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 28: 2.502586\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 29: 2.508869\n",
      "Minibatch accuracy: 15.2%\n",
      "Minibatch loss at step 30: 2.522326\n",
      "Minibatch accuracy: 15.2%\n",
      "Minibatch loss at step 31: 2.539181\n",
      "Minibatch accuracy: 15.2%\n",
      "Minibatch loss at step 32: 2.555452\n",
      "Minibatch accuracy: 15.2%\n",
      "Minibatch loss at step 33: 2.568292\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 34: 2.575960\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 35: 2.577750\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 36: 2.573432\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 37: 2.563911\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 38: 2.550238\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 39: 2.533954\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 40: 2.516664\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 41: 2.500019\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 42: 2.485041\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 43: 2.472755\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 44: 2.463732\n",
      "Minibatch accuracy: 15.5%\n",
      "Minibatch loss at step 45: 2.457501\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 46: 2.453945\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 47: 2.452379\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 48: 2.451737\n",
      "Minibatch accuracy: 19.2%\n",
      "Minibatch loss at step 49: 2.451415\n",
      "Minibatch accuracy: 19.2%\n"
     ]
    }
   ],
   "source": [
    "# MINIBATCH GRADIENT DESCENT\n",
    "loss_history = []\n",
    "num_steps = int(iteration_one_epoch*50)\n",
    "\n",
    "with tf.Session() as session:\n",
    "#with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_label_hot.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    #batch_data = train_dataset_reform[offset:(offset + batch_size), :]\n",
    "    #batch_labels = train_label_hot[offset:(offset + batch_size), :]\n",
    "    batch_data = train_dataset_reform\n",
    "    batch_labels = train_label_hot\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions, added_summary = session.run(\n",
    "        [optimizer, loss, train_prediction, loss_summary], feed_dict=feed_dict)\n",
    "    \n",
    "    # writer.add_summary(added_summary)\n",
    "    loss_history.append(l)\n",
    "    \n",
    "    # if (step % iteration_one_epoch == 0):\n",
    "    print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "    print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "    # print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    # print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48018"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(predictions,1) == 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48018"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples/53/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "total_series_length = num_samples\n",
    "truncated_backprop_length = 53\n",
    "state_size = 1\n",
    "num_classes = num_label\n",
    "batch_size = 2\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computational graph input, output, initial state\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length, num_classes])\n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and biases\n",
    "# input(k) + hidden state(k-1) to state(k)\n",
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
    "# hidden state to output\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack columns/slice each column (batch_size x truncated_backprop_length) -> (batch_sizex1) x tbl  \n",
    "inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
    "labels_series = tf.unstack(batchY_placeholder, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack:0' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:1' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:2' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:3' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:4' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:5' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:6' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:7' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:8' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:9' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:10' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:11' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:12' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:13' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:14' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:15' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:16' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:17' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:18' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:19' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:20' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:21' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:22' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:23' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:24' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:25' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:26' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:27' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:28' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:29' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:30' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:31' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:32' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:33' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:34' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:35' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:36' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:37' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:38' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:39' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:40' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:41' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:42' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:43' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:44' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:45' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:46' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:47' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:48' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:49' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:50' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:51' shape=(2,) dtype=float32>,\n",
       " <tf.Tensor 'unstack:52' shape=(2,) dtype=float32>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack_1:0' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:1' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:2' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:3' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:4' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:5' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:6' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:7' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:8' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:9' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:10' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:11' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:12' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:13' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:14' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:15' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:16' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:17' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:18' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:19' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:20' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:21' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:22' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:23' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:24' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:25' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:26' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:27' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:28' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:29' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:30' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:31' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:32' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:33' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:34' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:35' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:36' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:37' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:38' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:39' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:40' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:41' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:42' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:43' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:44' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:45' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:46' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:47' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:48' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:49' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:50' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:51' shape=(2, 18) dtype=float32>,\n",
       " <tf.Tensor 'unstack_1:52' shape=(2, 18) dtype=float32>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "current_state = init_state # batch_s x state_s\n",
    "states_series = []\n",
    "for current_input in inputs_series:\n",
    "    # bsxss\n",
    "    current_input = tf.reshape(current_input, [batch_size, 1]) \n",
    "    # bsxis concat bsxss -> bsx(is+ss) \n",
    "    input_and_state_concatenated = tf.concat([current_input, current_state],1)  # Increasing number of columns\n",
    "    # 5x5, 5x4 -> 5x4 + 1x4 = 5x4\n",
    "    # next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
    "    next_state = tf.nn.relu(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
    "    \n",
    "    states_series.append(next_state)\n",
    "    current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss calculation\n",
    "# 5x4 * 4x2 + 1x2 = 5x2\n",
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "\n",
    "# the same size with the batch for the training\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "# predictions_series = [tf.round(tf.sigmoid(logits)) for logits in logits_series]\n",
    "\n",
    "# compute the loss with cross entropy (it will be probabilistic 0-1 compare it with label)\n",
    "losses = [tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits,labels = labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "# losses = [tf.nn.sigmoid_cross_entropy_with_logits(logits = logits,labels = labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "\n",
    "# make the loss scalar\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "# gradient step\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48018, 18)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48018, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_reform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_dataset_reform.reshape((batch_size,-1))\n",
    "y = train_label_hot.reshape((batch_size, -1, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 24009)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 24009, 18)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_list(predictions, labels):\n",
    "  return (100*np.mean(np.argmax(np.asarray(predictions),2) == np.argmax(np.asarray(labels),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data, epoch 0\n",
      "Step 0 Loss 8501.739\n",
      "Step 45 Loss 2.082132\n",
      "Step 90 Loss 2.2911139\n",
      "Step 135 Loss 2.2355294\n",
      "Step 180 Loss 2.3489084\n",
      "Step 225 Loss 2.357231\n",
      "Step 270 Loss 2.1763716\n",
      "Step 315 Loss 2.675916\n",
      "Step 360 Loss 1.9061393\n",
      "Step 405 Loss 2.6521223\n",
      "Step 450 Loss 2.3639457\n",
      "New data, epoch 1\n",
      "Step 0 Loss 2.8934162\n",
      "Step 45 Loss 1.9621732\n",
      "Step 90 Loss 2.2378416\n",
      "Step 135 Loss 2.3646743\n",
      "Step 180 Loss 2.3391216\n",
      "Step 225 Loss 2.3797293\n",
      "Step 270 Loss 2.2004259\n",
      "Step 315 Loss 2.7466142\n",
      "Step 360 Loss 1.9508241\n",
      "Step 405 Loss 2.5503995\n",
      "Step 450 Loss 2.4210515\n",
      "New data, epoch 2\n",
      "Step 0 Loss 2.860684\n",
      "Step 45 Loss 1.996052\n",
      "Step 90 Loss 2.2158673\n",
      "Step 135 Loss 2.3783388\n",
      "Step 180 Loss 2.324613\n",
      "Step 225 Loss 2.3834722\n",
      "Step 270 Loss 2.220913\n",
      "Step 315 Loss 2.771669\n",
      "Step 360 Loss 1.976799\n",
      "Step 405 Loss 2.4933639\n",
      "Step 450 Loss 2.452447\n",
      "New data, epoch 3\n",
      "Step 0 Loss 2.8341274\n",
      "Step 45 Loss 2.0200763\n",
      "Step 90 Loss 2.2112582\n",
      "Step 135 Loss 2.3829849\n",
      "Step 180 Loss 2.3111286\n",
      "Step 225 Loss 2.3837755\n",
      "Step 270 Loss 2.2343674\n",
      "Step 315 Loss 2.784411\n",
      "Step 360 Loss 1.9945229\n",
      "Step 405 Loss 2.4581027\n",
      "Step 450 Loss 2.470838\n",
      "New data, epoch 4\n",
      "Step 0 Loss 2.812704\n",
      "Step 45 Loss 2.0360472\n",
      "Step 90 Loss 2.2118526\n",
      "Step 135 Loss 2.385927\n",
      "Step 180 Loss 2.3001442\n",
      "Step 225 Loss 2.3831987\n",
      "Step 270 Loss 2.2435305\n",
      "Step 315 Loss 2.7921104\n",
      "Step 360 Loss 2.0073876\n",
      "Step 405 Loss 2.4345424\n",
      "Step 450 Loss 2.4828315\n",
      "New data, epoch 5\n",
      "Step 0 Loss 2.7954106\n",
      "Step 45 Loss 2.0471008\n",
      "Step 90 Loss 2.213985\n",
      "Step 135 Loss 2.3881779\n",
      "Step 180 Loss 2.291343\n",
      "Step 225 Loss 2.3823864\n",
      "Step 270 Loss 2.2500782\n",
      "Step 315 Loss 2.7972567\n",
      "Step 360 Loss 2.0171528\n",
      "Step 405 Loss 2.4177234\n",
      "Step 450 Loss 2.4913454\n",
      "New data, epoch 6\n",
      "Step 0 Loss 2.781278\n",
      "Step 45 Loss 2.0551276\n",
      "Step 90 Loss 2.2164829\n",
      "Step 135 Loss 2.389986\n",
      "Step 180 Loss 2.2842226\n",
      "Step 225 Loss 2.3815463\n",
      "Step 270 Loss 2.2549586\n",
      "Step 315 Loss 2.8009305\n",
      "Step 360 Loss 2.024827\n",
      "Step 405 Loss 2.4050844\n",
      "Step 450 Loss 2.4977748\n",
      "New data, epoch 7\n",
      "Step 0 Loss 2.7695432\n",
      "Step 45 Loss 2.0612028\n",
      "Step 90 Loss 2.2189503\n",
      "Step 135 Loss 2.3914652\n",
      "Step 180 Loss 2.2783718\n",
      "Step 225 Loss 2.3807476\n",
      "Step 270 Loss 2.2587268\n",
      "Step 315 Loss 2.8036773\n",
      "Step 360 Loss 2.0310256\n",
      "Step 405 Loss 2.395203\n",
      "Step 450 Loss 2.5028534\n",
      "New data, epoch 8\n",
      "Step 0 Loss 2.759645\n",
      "Step 45 Loss 2.0659611\n",
      "Step 90 Loss 2.221258\n",
      "Step 135 Loss 2.3926868\n",
      "Step 180 Loss 2.2734869\n",
      "Step 225 Loss 2.3800113\n",
      "Step 270 Loss 2.26172\n",
      "Step 315 Loss 2.805804\n",
      "Step 360 Loss 2.0361443\n",
      "Step 405 Loss 2.3872359\n",
      "Step 450 Loss 2.5069985\n",
      "New data, epoch 9\n",
      "Step 0 Loss 2.7511744\n",
      "Step 45 Loss 2.0697918\n",
      "Step 90 Loss 2.2233737\n",
      "Step 135 Loss 2.3937054\n",
      "Step 180 Loss 2.269348\n",
      "Step 225 Loss 2.37934\n",
      "Step 270 Loss 2.264155\n",
      "Step 315 Loss 2.8074954\n",
      "Step 360 Loss 2.0404482\n",
      "Step 405 Loss 2.3806539\n",
      "Step 450 Loss 2.510469\n",
      "New data, epoch 10\n",
      "Step 0 Loss 2.743833\n",
      "Step 45 Loss 2.072947\n",
      "Step 90 Loss 2.2252986\n",
      "Step 135 Loss 2.3945606\n",
      "Step 180 Loss 2.265795\n",
      "Step 225 Loss 2.3787308\n",
      "Step 270 Loss 2.2661753\n",
      "Step 315 Loss 2.8088715\n",
      "Step 360 Loss 2.0441225\n",
      "Step 405 Loss 2.3751078\n",
      "Step 450 Loss 2.5134323\n",
      "New data, epoch 11\n",
      "Step 0 Loss 2.7373996\n",
      "Step 45 Loss 2.0755947\n",
      "Step 90 Loss 2.2270484\n",
      "Step 135 Loss 2.395284\n",
      "Step 180 Loss 2.2627087\n",
      "Step 225 Loss 2.3781767\n",
      "Step 270 Loss 2.2678792\n",
      "Step 315 Loss 2.81001\n",
      "Step 360 Loss 2.047299\n",
      "Step 405 Loss 2.370357\n",
      "Step 450 Loss 2.5160024\n",
      "New data, epoch 12\n",
      "Step 0 Loss 2.7317073\n",
      "Step 45 Loss 2.077852\n",
      "Step 90 Loss 2.2286415\n",
      "Step 135 Loss 2.3959002\n",
      "Step 180 Loss 2.2600014\n",
      "Step 225 Loss 2.377672\n",
      "Step 270 Loss 2.2693367\n",
      "Step 315 Loss 2.8109672\n",
      "Step 360 Loss 2.0500758\n",
      "Step 405 Loss 2.3662338\n",
      "Step 450 Loss 2.5182598\n",
      "New data, epoch 13\n",
      "Step 0 Loss 2.7266264\n",
      "Step 45 Loss 2.079802\n",
      "Step 90 Loss 2.2300966\n",
      "Step 135 Loss 2.3964288\n",
      "Step 180 Loss 2.2576041\n",
      "Step 225 Loss 2.3772101\n",
      "Step 270 Loss 2.2705982\n",
      "Step 315 Loss 2.8117814\n",
      "Step 360 Loss 2.052526\n",
      "Step 405 Loss 2.3626127\n",
      "Step 450 Loss 2.5202637\n",
      "New data, epoch 14\n",
      "Step 0 Loss 2.7220585\n",
      "Step 45 Loss 2.0815067\n",
      "Step 90 Loss 2.2314289\n",
      "Step 135 Loss 2.3968847\n",
      "Step 180 Loss 2.2554653\n",
      "Step 225 Loss 2.376787\n",
      "Step 270 Loss 2.271702\n",
      "Step 315 Loss 2.8124828\n",
      "Step 360 Loss 2.054706\n",
      "Step 405 Loss 2.3594022\n",
      "Step 450 Loss 2.5220592\n",
      "New data, epoch 15\n",
      "Step 0 Loss 2.7179236\n",
      "Step 45 Loss 2.0830107\n",
      "Step 90 Loss 2.2326536\n",
      "Step 135 Loss 2.3972797\n",
      "Step 180 Loss 2.253543\n",
      "Step 225 Loss 2.376397\n",
      "Step 270 Loss 2.272677\n",
      "Step 315 Loss 2.813092\n",
      "Step 360 Loss 2.0566595\n",
      "Step 405 Loss 2.3565302\n",
      "Step 450 Loss 2.5236797\n",
      "New data, epoch 16\n",
      "Step 0 Loss 2.7141597\n",
      "Step 45 Loss 2.0843494\n",
      "Step 90 Loss 2.233784\n",
      "Step 135 Loss 2.3976238\n",
      "Step 180 Loss 2.2518046\n",
      "Step 225 Loss 2.3760366\n",
      "Step 270 Loss 2.2735445\n",
      "Step 315 Loss 2.8136265\n",
      "Step 360 Loss 2.0584216\n",
      "Step 405 Loss 2.3539433\n",
      "Step 450 Loss 2.5251524\n",
      "New data, epoch 17\n",
      "Step 0 Loss 2.7107139\n",
      "Step 45 Loss 2.0855498\n",
      "Step 90 Loss 2.2348297\n",
      "Step 135 Loss 2.3979244\n",
      "Step 180 Loss 2.2502239\n",
      "Step 225 Loss 2.3757029\n",
      "Step 270 Loss 2.274322\n",
      "Step 315 Loss 2.814098\n",
      "Step 360 Loss 2.0600202\n",
      "Step 405 Loss 2.3515968\n",
      "Step 450 Loss 2.5264986\n",
      "New data, epoch 18\n",
      "Step 0 Loss 2.707545\n",
      "Step 45 Loss 2.086634\n",
      "Step 90 Loss 2.2358012\n",
      "Step 135 Loss 2.3981888\n",
      "Step 180 Loss 2.248779\n",
      "Step 225 Loss 2.3753922\n",
      "Step 270 Loss 2.2750237\n",
      "Step 315 Loss 2.8145175\n",
      "Step 360 Loss 2.0614774\n",
      "Step 405 Loss 2.3494573\n",
      "Step 450 Loss 2.527734\n",
      "New data, epoch 19\n",
      "Step 0 Loss 2.7046192\n",
      "Step 45 Loss 2.0876186\n",
      "Step 90 Loss 2.236706\n",
      "Step 135 Loss 2.398422\n",
      "Step 180 Loss 2.2474527\n",
      "Step 225 Loss 2.3751016\n",
      "Step 270 Loss 2.27566\n",
      "Step 315 Loss 2.8148918\n",
      "Step 360 Loss 2.0628126\n",
      "Step 405 Loss 2.3474956\n",
      "Step 450 Loss 2.5288749\n",
      "New data, epoch 20\n",
      "Step 0 Loss 2.701906\n",
      "Step 45 Loss 2.0885177\n",
      "Step 90 Loss 2.237551\n",
      "Step 135 Loss 2.3986278\n",
      "Step 180 Loss 2.2462292\n",
      "Step 225 Loss 2.3748305\n",
      "Step 270 Loss 2.27624\n",
      "Step 315 Loss 2.815229\n",
      "Step 360 Loss 2.064041\n",
      "Step 405 Loss 2.3456888\n",
      "Step 450 Loss 2.5299313\n",
      "New data, epoch 21\n",
      "Step 0 Loss 2.699382\n",
      "Step 45 Loss 2.0893419\n",
      "Step 90 Loss 2.2383428\n",
      "Step 135 Loss 2.3988104\n",
      "Step 180 Loss 2.245097\n",
      "Step 225 Loss 2.3745756\n",
      "Step 270 Loss 2.2767718\n",
      "Step 315 Loss 2.815534\n",
      "Step 360 Loss 2.0651753\n",
      "Step 405 Loss 2.344018\n",
      "Step 450 Loss 2.5309145\n",
      "New data, epoch 22\n",
      "Step 0 Loss 2.6970265\n",
      "Step 45 Loss 2.0901022\n",
      "Step 90 Loss 2.2390862\n",
      "Step 135 Loss 2.3989725\n",
      "Step 180 Loss 2.2440455\n",
      "Step 225 Loss 2.374336\n",
      "Step 270 Loss 2.2772608\n",
      "Step 315 Loss 2.8158107\n",
      "Step 360 Loss 2.066226\n",
      "Step 405 Loss 2.3424673\n",
      "Step 450 Loss 2.5318313\n",
      "New data, epoch 23\n",
      "Step 0 Loss 2.6948204\n",
      "Step 45 Loss 2.0908048\n",
      "Step 90 Loss 2.2397857\n",
      "Step 135 Loss 2.3991168\n",
      "Step 180 Loss 2.2430658\n",
      "Step 225 Loss 2.3741097\n",
      "Step 270 Loss 2.2777126\n",
      "Step 315 Loss 2.8160627\n",
      "Step 360 Loss 2.0672028\n",
      "Step 405 Loss 2.341022\n",
      "Step 450 Loss 2.5326896\n",
      "New data, epoch 24\n",
      "Step 0 Loss 2.6927505\n",
      "Step 45 Loss 2.0914574\n",
      "Step 90 Loss 2.2404454\n",
      "Step 135 Loss 2.3992453\n",
      "Step 180 Loss 2.2421505\n",
      "Step 225 Loss 2.373896\n",
      "Step 270 Loss 2.2781308\n",
      "Step 315 Loss 2.816294\n",
      "Step 360 Loss 2.0681143\n",
      "Step 405 Loss 2.3396726\n",
      "Step 450 Loss 2.533495\n",
      "New data, epoch 25\n",
      "Step 0 Loss 2.6908028\n",
      "Step 45 Loss 2.092065\n",
      "Step 90 Loss 2.241069\n",
      "Step 135 Loss 2.3993602\n",
      "Step 180 Loss 2.2412927\n",
      "Step 225 Loss 2.3736944\n",
      "Step 270 Loss 2.2785199\n",
      "Step 315 Loss 2.8165057\n",
      "Step 360 Loss 2.0689664\n",
      "Step 405 Loss 2.3384075\n",
      "Step 450 Loss 2.534252\n",
      "New data, epoch 26\n",
      "Step 0 Loss 2.688966\n",
      "Step 45 Loss 2.0926328\n",
      "Step 90 Loss 2.2416599\n",
      "Step 135 Loss 2.399463\n",
      "Step 180 Loss 2.2404866\n",
      "Step 225 Loss 2.3735023\n",
      "Step 270 Loss 2.2788827\n",
      "Step 315 Loss 2.8167005\n",
      "Step 360 Loss 2.0697646\n",
      "Step 405 Loss 2.3372183\n",
      "Step 450 Loss 2.5349662\n",
      "New data, epoch 27\n",
      "Step 0 Loss 2.6872287\n",
      "Step 45 Loss 2.0931644\n",
      "Step 90 Loss 2.2422202\n",
      "Step 135 Loss 2.3995557\n",
      "Step 180 Loss 2.239728\n",
      "Step 225 Loss 2.37332\n",
      "Step 270 Loss 2.2792215\n",
      "Step 315 Loss 2.8168805\n",
      "Step 360 Loss 2.0705152\n",
      "Step 405 Loss 2.3360994\n",
      "Step 450 Loss 2.5356414\n",
      "New data, epoch 28\n",
      "Step 0 Loss 2.6855838\n",
      "Step 45 Loss 2.0936637\n",
      "Step 90 Loss 2.2427528\n",
      "Step 135 Loss 2.399638\n",
      "Step 180 Loss 2.239012\n",
      "Step 225 Loss 2.3731463\n",
      "Step 270 Loss 2.2795396\n",
      "Step 315 Loss 2.8170474\n",
      "Step 360 Loss 2.0712214\n",
      "Step 405 Loss 2.3350427\n",
      "Step 450 Loss 2.5362802\n",
      "New data, epoch 29\n",
      "Step 0 Loss 2.6840234\n",
      "Step 45 Loss 2.0941334\n",
      "Step 90 Loss 2.24326\n",
      "Step 135 Loss 2.3997123\n",
      "Step 180 Loss 2.2383351\n",
      "Step 225 Loss 2.3729806\n",
      "Step 270 Loss 2.2798383\n",
      "Step 315 Loss 2.8172023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 360 Loss 2.071888\n",
      "Step 405 Loss 2.334043\n",
      "Step 450 Loss 2.536887\n",
      "New data, epoch 30\n",
      "Step 0 Loss 2.6825404\n",
      "Step 45 Loss 2.0945768\n",
      "Step 90 Loss 2.243743\n",
      "Step 135 Loss 2.3997788\n",
      "Step 180 Loss 2.2376943\n",
      "Step 225 Loss 2.3728223\n",
      "Step 270 Loss 2.2801197\n",
      "Step 315 Loss 2.8173468\n",
      "Step 360 Loss 2.072518\n",
      "Step 405 Loss 2.3330956\n",
      "Step 450 Loss 2.537463\n",
      "New data, epoch 31\n",
      "Step 0 Loss 2.6811275\n",
      "Step 45 Loss 2.094996\n",
      "Step 90 Loss 2.244204\n",
      "Step 135 Loss 2.3998384\n",
      "Step 180 Loss 2.2370858\n",
      "Step 225 Loss 2.372671\n",
      "Step 270 Loss 2.2803853\n",
      "Step 315 Loss 2.8174808\n",
      "Step 360 Loss 2.073115\n",
      "Step 405 Loss 2.3321962\n",
      "Step 450 Loss 2.538011\n",
      "New data, epoch 32\n",
      "Step 0 Loss 2.6797807\n",
      "Step 45 Loss 2.0953932\n",
      "Step 90 Loss 2.244645\n",
      "Step 135 Loss 2.3998919\n",
      "Step 180 Loss 2.2365074\n",
      "Step 225 Loss 2.372526\n",
      "Step 270 Loss 2.280636\n",
      "Step 315 Loss 2.8176067\n",
      "Step 360 Loss 2.0736814\n",
      "Step 405 Loss 2.33134\n",
      "Step 450 Loss 2.5385342\n",
      "New data, epoch 33\n",
      "Step 0 Loss 2.678495\n",
      "Step 45 Loss 2.0957701\n",
      "Step 90 Loss 2.2450671\n",
      "Step 135 Loss 2.3999398\n",
      "Step 180 Loss 2.2359567\n",
      "Step 225 Loss 2.372387\n",
      "Step 270 Loss 2.2808735\n",
      "Step 315 Loss 2.8177245\n",
      "Step 360 Loss 2.0742197\n",
      "Step 405 Loss 2.330525\n",
      "Step 450 Loss 2.5390325\n",
      "New data, epoch 34\n",
      "Step 0 Loss 2.6772647\n",
      "Step 45 Loss 2.0961277\n",
      "Step 90 Loss 2.245472\n",
      "Step 135 Loss 2.399983\n",
      "Step 180 Loss 2.235432\n",
      "Step 225 Loss 2.3722537\n",
      "Step 270 Loss 2.2810988\n",
      "Step 315 Loss 2.8178353\n",
      "Step 360 Loss 2.074731\n",
      "Step 405 Loss 2.3297472\n",
      "Step 450 Loss 2.53951\n",
      "New data, epoch 35\n",
      "Step 0 Loss 2.6760876\n",
      "Step 45 Loss 2.0964687\n",
      "Step 90 Loss 2.2458599\n",
      "Step 135 Loss 2.4000216\n",
      "Step 180 Loss 2.234931\n",
      "Step 225 Loss 2.3721259\n",
      "Step 270 Loss 2.2813134\n",
      "Step 315 Loss 2.8179395\n",
      "Step 360 Loss 2.0752194\n",
      "Step 405 Loss 2.3290043\n",
      "Step 450 Loss 2.5399668\n",
      "New data, epoch 36\n",
      "Step 0 Loss 2.6749585\n",
      "Step 45 Loss 2.0967937\n",
      "Step 90 Loss 2.2462323\n",
      "Step 135 Loss 2.4000561\n",
      "Step 180 Loss 2.234452\n",
      "Step 225 Loss 2.3720021\n",
      "Step 270 Loss 2.2815175\n",
      "Step 315 Loss 2.818037\n",
      "Step 360 Loss 2.0756848\n",
      "Step 405 Loss 2.3282933\n",
      "Step 450 Loss 2.5404043\n",
      "New data, epoch 37\n",
      "Step 0 Loss 2.6738763\n",
      "Step 45 Loss 2.0971038\n",
      "Step 90 Loss 2.2465909\n",
      "Step 135 Loss 2.4000869\n",
      "Step 180 Loss 2.2339938\n",
      "Step 225 Loss 2.3718839\n",
      "Step 270 Loss 2.281712\n",
      "Step 315 Loss 2.8181295\n",
      "Step 360 Loss 2.0761297\n",
      "Step 405 Loss 2.3276122\n",
      "Step 450 Loss 2.5408242\n",
      "New data, epoch 38\n",
      "Step 0 Loss 2.6728354\n",
      "Step 45 Loss 2.097401\n",
      "Step 90 Loss 2.2469358\n",
      "Step 135 Loss 2.4001143\n",
      "Step 180 Loss 2.2335541\n",
      "Step 225 Loss 2.3717694\n",
      "Step 270 Loss 2.2818975\n",
      "Step 315 Loss 2.8182163\n",
      "Step 360 Loss 2.0765553\n",
      "Step 405 Loss 2.3269591\n",
      "Step 450 Loss 2.5412276\n",
      "New data, epoch 39\n",
      "Step 0 Loss 2.6718345\n",
      "Step 45 Loss 2.0976844\n",
      "Step 90 Loss 2.2472677\n",
      "Step 135 Loss 2.4001386\n",
      "Step 180 Loss 2.2331324\n",
      "Step 225 Loss 2.3716588\n",
      "Step 270 Loss 2.2820742\n",
      "Step 315 Loss 2.8182995\n",
      "Step 360 Loss 2.0769627\n",
      "Step 405 Loss 2.3263323\n",
      "Step 450 Loss 2.5416152\n",
      "New data, epoch 40\n",
      "Step 0 Loss 2.6708713\n",
      "Step 45 Loss 2.0979562\n",
      "Step 90 Loss 2.2475882\n",
      "Step 135 Loss 2.40016\n",
      "Step 180 Loss 2.2327275\n",
      "Step 225 Loss 2.3715522\n",
      "Step 270 Loss 2.2822442\n",
      "Step 315 Loss 2.8183777\n",
      "Step 360 Loss 2.077353\n",
      "Step 405 Loss 2.3257296\n",
      "Step 450 Loss 2.541988\n",
      "New data, epoch 41\n",
      "Step 0 Loss 2.6699421\n",
      "Step 45 Loss 2.098217\n",
      "Step 90 Loss 2.2478971\n",
      "Step 135 Loss 2.4001794\n",
      "Step 180 Loss 2.2323382\n",
      "Step 225 Loss 2.371449\n",
      "Step 270 Loss 2.2824066\n",
      "Step 315 Loss 2.818452\n",
      "Step 360 Loss 2.0777285\n",
      "Step 405 Loss 2.3251498\n",
      "Step 450 Loss 2.5423477\n",
      "New data, epoch 42\n",
      "Step 0 Loss 2.6690469\n",
      "Step 45 Loss 2.0984676\n",
      "Step 90 Loss 2.2481956\n",
      "Step 135 Loss 2.4001966\n",
      "Step 180 Loss 2.231964\n",
      "Step 225 Loss 2.3713496\n",
      "Step 270 Loss 2.2825625\n",
      "Step 315 Loss 2.8185217\n",
      "Step 360 Loss 2.0780888\n",
      "Step 405 Loss 2.3245914\n",
      "Step 450 Loss 2.542694\n",
      "New data, epoch 43\n",
      "Step 0 Loss 2.6681828\n",
      "Step 45 Loss 2.0987086\n",
      "Step 90 Loss 2.2484841\n",
      "Step 135 Loss 2.4002106\n",
      "Step 180 Loss 2.231603\n",
      "Step 225 Loss 2.3712535\n",
      "Step 270 Loss 2.282712\n",
      "Step 315 Loss 2.8185892\n",
      "Step 360 Loss 2.0784352\n",
      "Step 405 Loss 2.3240528\n",
      "Step 450 Loss 2.543028\n",
      "New data, epoch 44\n",
      "Step 0 Loss 2.6673477\n",
      "Step 45 Loss 2.0989406\n",
      "Step 90 Loss 2.2487633\n",
      "Step 135 Loss 2.4002225\n",
      "Step 180 Loss 2.2312555\n",
      "Step 225 Loss 2.3711598\n",
      "Step 270 Loss 2.2828557\n",
      "Step 315 Loss 2.8186533\n",
      "Step 360 Loss 2.0787687\n",
      "Step 405 Loss 2.3235338\n",
      "Step 450 Loss 2.5433507\n",
      "New data, epoch 45\n",
      "Step 0 Loss 2.6665409\n",
      "Step 45 Loss 2.0991638\n",
      "Step 90 Loss 2.2490332\n",
      "Step 135 Loss 2.4002335\n",
      "Step 180 Loss 2.2309206\n",
      "Step 225 Loss 2.371069\n",
      "Step 270 Loss 2.2829938\n",
      "Step 315 Loss 2.8187134\n",
      "Step 360 Loss 2.0790894\n",
      "Step 405 Loss 2.3230324\n",
      "Step 450 Loss 2.5436628\n",
      "New data, epoch 46\n",
      "Step 0 Loss 2.6657605\n",
      "Step 45 Loss 2.0993788\n",
      "Step 90 Loss 2.2492948\n",
      "Step 135 Loss 2.4002423\n",
      "Step 180 Loss 2.2305965\n",
      "Step 225 Loss 2.370981\n",
      "Step 270 Loss 2.2831266\n",
      "Step 315 Loss 2.8187714\n",
      "Step 360 Loss 2.0793989\n",
      "Step 405 Loss 2.322548\n",
      "Step 450 Loss 2.5439649\n",
      "New data, epoch 47\n",
      "Step 0 Loss 2.6650043\n",
      "Step 45 Loss 2.099587\n",
      "Step 90 Loss 2.2495482\n",
      "Step 135 Loss 2.4002507\n",
      "Step 180 Loss 2.2302837\n",
      "Step 225 Loss 2.370896\n",
      "Step 270 Loss 2.283255\n",
      "Step 315 Loss 2.818827\n",
      "Step 360 Loss 2.0796976\n",
      "Step 405 Loss 2.3220794\n",
      "Step 450 Loss 2.544257\n",
      "New data, epoch 48\n",
      "Step 0 Loss 2.6642718\n",
      "Step 45 Loss 2.0997875\n",
      "Step 90 Loss 2.2497942\n",
      "Step 135 Loss 2.4002564\n",
      "Step 180 Loss 2.2299807\n",
      "Step 225 Loss 2.370813\n",
      "Step 270 Loss 2.2833784\n",
      "Step 315 Loss 2.8188794\n",
      "Step 360 Loss 2.0799863\n",
      "Step 405 Loss 2.3216262\n",
      "Step 450 Loss 2.54454\n",
      "New data, epoch 49\n",
      "Step 0 Loss 2.663562\n",
      "Step 45 Loss 2.0999813\n",
      "Step 90 Loss 2.2500324\n",
      "Step 135 Loss 2.4002612\n",
      "Step 180 Loss 2.2296882\n",
      "Step 225 Loss 2.3707328\n",
      "Step 270 Loss 2.2834976\n",
      "Step 315 Loss 2.8189306\n",
      "Step 360 Loss 2.0802648\n",
      "Step 405 Loss 2.321187\n",
      "Step 450 Loss 2.5448139\n",
      "batch: 0 , accuracy: 6.60377358490566\n",
      "batch: 1 , accuracy: 8.49056603773585\n",
      "batch: 2 , accuracy: 6.60377358490566\n",
      "batch: 3 , accuracy: 7.547169811320755\n",
      "batch: 4 , accuracy: 6.60377358490566\n",
      "batch: 5 , accuracy: 9.433962264150944\n",
      "batch: 6 , accuracy: 7.547169811320755\n",
      "batch: 7 , accuracy: 9.433962264150944\n",
      "batch: 8 , accuracy: 9.433962264150944\n",
      "batch: 9 , accuracy: 10.377358490566039\n",
      "batch: 10 , accuracy: 8.49056603773585\n",
      "batch: 11 , accuracy: 13.20754716981132\n",
      "batch: 12 , accuracy: 11.320754716981133\n",
      "batch: 13 , accuracy: 16.037735849056602\n",
      "batch: 14 , accuracy: 13.20754716981132\n",
      "batch: 15 , accuracy: 15.09433962264151\n",
      "batch: 16 , accuracy: 14.150943396226415\n",
      "batch: 17 , accuracy: 10.377358490566039\n",
      "batch: 18 , accuracy: 9.433962264150944\n",
      "batch: 19 , accuracy: 10.377358490566039\n",
      "batch: 20 , accuracy: 13.20754716981132\n",
      "batch: 21 , accuracy: 13.20754716981132\n",
      "batch: 22 , accuracy: 1.8867924528301887\n",
      "batch: 23 , accuracy: 0.0\n",
      "batch: 24 , accuracy: 0.0\n",
      "batch: 25 , accuracy: 9.433962264150944\n",
      "batch: 26 , accuracy: 7.547169811320755\n",
      "batch: 27 , accuracy: 15.09433962264151\n",
      "batch: 28 , accuracy: 10.377358490566039\n",
      "batch: 29 , accuracy: 13.20754716981132\n",
      "batch: 30 , accuracy: 10.377358490566039\n",
      "batch: 31 , accuracy: 9.433962264150944\n",
      "batch: 32 , accuracy: 9.433962264150944\n",
      "batch: 33 , accuracy: 9.433962264150944\n",
      "batch: 34 , accuracy: 8.49056603773585\n",
      "batch: 35 , accuracy: 6.60377358490566\n",
      "batch: 36 , accuracy: 8.49056603773585\n",
      "batch: 37 , accuracy: 7.547169811320755\n",
      "batch: 38 , accuracy: 10.377358490566039\n",
      "batch: 39 , accuracy: 8.49056603773585\n",
      "batch: 40 , accuracy: 9.433962264150944\n",
      "batch: 41 , accuracy: 6.60377358490566\n",
      "batch: 42 , accuracy: 10.377358490566039\n",
      "batch: 43 , accuracy: 10.377358490566039\n",
      "batch: 44 , accuracy: 8.49056603773585\n",
      "batch: 45 , accuracy: 20.754716981132077\n",
      "batch: 46 , accuracy: 23.58490566037736\n",
      "batch: 47 , accuracy: 16.037735849056602\n",
      "batch: 48 , accuracy: 15.09433962264151\n",
      "batch: 49 , accuracy: 14.150943396226415\n",
      "batch: 50 , accuracy: 14.150943396226415\n",
      "batch: 51 , accuracy: 15.09433962264151\n",
      "batch: 52 , accuracy: 9.433962264150944\n",
      "batch: 53 , accuracy: 10.377358490566039\n",
      "batch: 54 , accuracy: 11.320754716981133\n",
      "batch: 55 , accuracy: 15.09433962264151\n",
      "batch: 56 , accuracy: 15.09433962264151\n",
      "batch: 57 , accuracy: 0.0\n",
      "batch: 58 , accuracy: 0.0\n",
      "batch: 59 , accuracy: 2.8301886792452833\n",
      "batch: 60 , accuracy: 4.716981132075472\n",
      "batch: 61 , accuracy: 9.433962264150944\n",
      "batch: 62 , accuracy: 7.547169811320755\n",
      "batch: 63 , accuracy: 8.49056603773585\n",
      "batch: 64 , accuracy: 11.320754716981133\n",
      "batch: 65 , accuracy: 5.660377358490567\n",
      "batch: 66 , accuracy: 2.8301886792452833\n",
      "batch: 67 , accuracy: 2.8301886792452833\n",
      "batch: 68 , accuracy: 4.716981132075472\n",
      "batch: 69 , accuracy: 2.8301886792452833\n",
      "batch: 70 , accuracy: 3.7735849056603774\n",
      "batch: 71 , accuracy: 1.8867924528301887\n",
      "batch: 72 , accuracy: 5.660377358490567\n",
      "batch: 73 , accuracy: 3.7735849056603774\n",
      "batch: 74 , accuracy: 2.8301886792452833\n",
      "batch: 75 , accuracy: 2.8301886792452833\n",
      "batch: 76 , accuracy: 4.716981132075472\n",
      "batch: 77 , accuracy: 4.716981132075472\n",
      "batch: 78 , accuracy: 3.7735849056603774\n",
      "batch: 79 , accuracy: 10.377358490566039\n",
      "batch: 80 , accuracy: 8.49056603773585\n",
      "batch: 81 , accuracy: 6.60377358490566\n",
      "batch: 82 , accuracy: 5.660377358490567\n",
      "batch: 83 , accuracy: 6.60377358490566\n",
      "batch: 84 , accuracy: 13.20754716981132\n",
      "batch: 85 , accuracy: 6.60377358490566\n",
      "batch: 86 , accuracy: 5.660377358490567\n",
      "batch: 87 , accuracy: 5.660377358490567\n",
      "batch: 88 , accuracy: 6.60377358490566\n",
      "batch: 89 , accuracy: 10.377358490566039\n",
      "batch: 90 , accuracy: 13.20754716981132\n",
      "batch: 91 , accuracy: 7.547169811320755\n",
      "batch: 92 , accuracy: 1.8867924528301887\n",
      "batch: 93 , accuracy: 0.0\n",
      "batch: 94 , accuracy: 2.8301886792452833\n",
      "batch: 95 , accuracy: 12.264150943396226\n",
      "batch: 96 , accuracy: 13.20754716981132\n",
      "batch: 97 , accuracy: 16.9811320754717\n",
      "batch: 98 , accuracy: 16.037735849056602\n",
      "batch: 99 , accuracy: 12.264150943396226\n",
      "batch: 100 , accuracy: 8.49056603773585\n",
      "batch: 101 , accuracy: 5.660377358490567\n",
      "batch: 102 , accuracy: 10.377358490566039\n",
      "batch: 103 , accuracy: 12.264150943396226\n",
      "batch: 104 , accuracy: 8.49056603773585\n",
      "batch: 105 , accuracy: 8.49056603773585\n",
      "batch: 106 , accuracy: 8.49056603773585\n",
      "batch: 107 , accuracy: 10.377358490566039\n",
      "batch: 108 , accuracy: 7.547169811320755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 109 , accuracy: 7.547169811320755\n",
      "batch: 110 , accuracy: 8.49056603773585\n",
      "batch: 111 , accuracy: 12.264150943396226\n",
      "batch: 112 , accuracy: 10.377358490566039\n",
      "batch: 113 , accuracy: 9.433962264150944\n",
      "batch: 114 , accuracy: 15.09433962264151\n",
      "batch: 115 , accuracy: 16.9811320754717\n",
      "batch: 116 , accuracy: 16.037735849056602\n",
      "batch: 117 , accuracy: 15.09433962264151\n",
      "batch: 118 , accuracy: 26.41509433962264\n",
      "batch: 119 , accuracy: 16.037735849056602\n",
      "batch: 120 , accuracy: 11.320754716981133\n",
      "batch: 121 , accuracy: 10.377358490566039\n",
      "batch: 122 , accuracy: 13.20754716981132\n",
      "batch: 123 , accuracy: 20.754716981132077\n",
      "batch: 124 , accuracy: 20.754716981132077\n",
      "batch: 125 , accuracy: 16.037735849056602\n",
      "batch: 126 , accuracy: 0.0\n",
      "batch: 127 , accuracy: 2.8301886792452833\n",
      "batch: 128 , accuracy: 8.49056603773585\n",
      "batch: 129 , accuracy: 9.433962264150944\n",
      "batch: 130 , accuracy: 9.433962264150944\n",
      "batch: 131 , accuracy: 13.20754716981132\n",
      "batch: 132 , accuracy: 5.660377358490567\n",
      "batch: 133 , accuracy: 9.433962264150944\n",
      "batch: 134 , accuracy: 7.547169811320755\n",
      "batch: 135 , accuracy: 7.547169811320755\n",
      "batch: 136 , accuracy: 11.320754716981133\n",
      "batch: 137 , accuracy: 7.547169811320755\n",
      "batch: 138 , accuracy: 5.660377358490567\n",
      "batch: 139 , accuracy: 7.547169811320755\n",
      "batch: 140 , accuracy: 12.264150943396226\n",
      "batch: 141 , accuracy: 9.433962264150944\n",
      "batch: 142 , accuracy: 6.60377358490566\n",
      "batch: 143 , accuracy: 8.49056603773585\n",
      "batch: 144 , accuracy: 10.377358490566039\n",
      "batch: 145 , accuracy: 16.9811320754717\n",
      "batch: 146 , accuracy: 9.433962264150944\n",
      "batch: 147 , accuracy: 10.377358490566039\n",
      "batch: 148 , accuracy: 26.41509433962264\n",
      "batch: 149 , accuracy: 10.377358490566039\n",
      "batch: 150 , accuracy: 16.037735849056602\n",
      "batch: 151 , accuracy: 15.09433962264151\n",
      "batch: 152 , accuracy: 13.20754716981132\n",
      "batch: 153 , accuracy: 14.150943396226415\n",
      "batch: 154 , accuracy: 13.20754716981132\n",
      "batch: 155 , accuracy: 14.150943396226415\n",
      "batch: 156 , accuracy: 16.9811320754717\n",
      "batch: 157 , accuracy: 24.528301886792452\n",
      "batch: 158 , accuracy: 36.79245283018868\n",
      "batch: 159 , accuracy: 12.264150943396226\n",
      "batch: 160 , accuracy: 0.0\n",
      "batch: 161 , accuracy: 6.60377358490566\n",
      "batch: 162 , accuracy: 14.150943396226415\n",
      "batch: 163 , accuracy: 10.377358490566039\n",
      "batch: 164 , accuracy: 16.9811320754717\n",
      "batch: 165 , accuracy: 16.9811320754717\n",
      "batch: 166 , accuracy: 15.09433962264151\n",
      "batch: 167 , accuracy: 11.320754716981133\n",
      "batch: 168 , accuracy: 15.09433962264151\n",
      "batch: 169 , accuracy: 10.377358490566039\n",
      "batch: 170 , accuracy: 17.92452830188679\n",
      "batch: 171 , accuracy: 15.09433962264151\n",
      "batch: 172 , accuracy: 11.320754716981133\n",
      "batch: 173 , accuracy: 5.660377358490567\n",
      "batch: 174 , accuracy: 17.92452830188679\n",
      "batch: 175 , accuracy: 10.377358490566039\n",
      "batch: 176 , accuracy: 12.264150943396226\n",
      "batch: 177 , accuracy: 20.754716981132077\n",
      "batch: 178 , accuracy: 15.09433962264151\n",
      "batch: 179 , accuracy: 14.150943396226415\n",
      "batch: 180 , accuracy: 16.9811320754717\n",
      "batch: 181 , accuracy: 18.867924528301888\n",
      "batch: 182 , accuracy: 23.58490566037736\n",
      "batch: 183 , accuracy: 23.58490566037736\n",
      "batch: 184 , accuracy: 22.641509433962266\n",
      "batch: 185 , accuracy: 25.471698113207548\n",
      "batch: 186 , accuracy: 18.867924528301888\n",
      "batch: 187 , accuracy: 17.92452830188679\n",
      "batch: 188 , accuracy: 16.9811320754717\n",
      "batch: 189 , accuracy: 19.81132075471698\n",
      "batch: 190 , accuracy: 27.358490566037734\n",
      "batch: 191 , accuracy: 41.509433962264154\n",
      "batch: 192 , accuracy: 24.528301886792452\n",
      "batch: 193 , accuracy: 0.0\n",
      "batch: 194 , accuracy: 2.8301886792452833\n",
      "batch: 195 , accuracy: 12.264150943396226\n",
      "batch: 196 , accuracy: 12.264150943396226\n",
      "batch: 197 , accuracy: 12.264150943396226\n",
      "batch: 198 , accuracy: 17.92452830188679\n",
      "batch: 199 , accuracy: 16.037735849056602\n",
      "batch: 200 , accuracy: 14.150943396226415\n",
      "batch: 201 , accuracy: 8.49056603773585\n",
      "batch: 202 , accuracy: 7.547169811320755\n",
      "batch: 203 , accuracy: 7.547169811320755\n",
      "batch: 204 , accuracy: 7.547169811320755\n",
      "batch: 205 , accuracy: 7.547169811320755\n",
      "batch: 206 , accuracy: 5.660377358490567\n",
      "batch: 207 , accuracy: 15.09433962264151\n",
      "batch: 208 , accuracy: 8.49056603773585\n",
      "batch: 209 , accuracy: 5.660377358490567\n",
      "batch: 210 , accuracy: 11.320754716981133\n",
      "batch: 211 , accuracy: 12.264150943396226\n",
      "batch: 212 , accuracy: 10.377358490566039\n",
      "batch: 213 , accuracy: 11.320754716981133\n",
      "batch: 214 , accuracy: 23.58490566037736\n",
      "batch: 215 , accuracy: 16.9811320754717\n",
      "batch: 216 , accuracy: 14.150943396226415\n",
      "batch: 217 , accuracy: 13.20754716981132\n",
      "batch: 218 , accuracy: 14.150943396226415\n",
      "batch: 219 , accuracy: 13.20754716981132\n",
      "batch: 220 , accuracy: 15.09433962264151\n",
      "batch: 221 , accuracy: 12.264150943396226\n",
      "batch: 222 , accuracy: 16.037735849056602\n",
      "batch: 223 , accuracy: 30.18867924528302\n",
      "batch: 224 , accuracy: 43.39622641509434\n",
      "batch: 225 , accuracy: 0.0\n",
      "batch: 226 , accuracy: 7.547169811320755\n",
      "batch: 227 , accuracy: 15.09433962264151\n",
      "batch: 228 , accuracy: 16.9811320754717\n",
      "batch: 229 , accuracy: 16.9811320754717\n",
      "batch: 230 , accuracy: 14.150943396226415\n",
      "batch: 231 , accuracy: 20.754716981132077\n",
      "batch: 232 , accuracy: 17.92452830188679\n",
      "batch: 233 , accuracy: 14.150943396226415\n",
      "batch: 234 , accuracy: 13.20754716981132\n",
      "batch: 235 , accuracy: 15.09433962264151\n",
      "batch: 236 , accuracy: 11.320754716981133\n",
      "batch: 237 , accuracy: 11.320754716981133\n",
      "batch: 238 , accuracy: 11.320754716981133\n",
      "batch: 239 , accuracy: 11.320754716981133\n",
      "batch: 240 , accuracy: 12.264150943396226\n",
      "batch: 241 , accuracy: 18.867924528301888\n",
      "batch: 242 , accuracy: 21.69811320754717\n",
      "batch: 243 , accuracy: 17.92452830188679\n",
      "batch: 244 , accuracy: 16.037735849056602\n",
      "batch: 245 , accuracy: 23.58490566037736\n",
      "batch: 246 , accuracy: 27.358490566037734\n",
      "batch: 247 , accuracy: 24.528301886792452\n",
      "batch: 248 , accuracy: 25.471698113207548\n",
      "batch: 249 , accuracy: 26.41509433962264\n",
      "batch: 250 , accuracy: 20.754716981132077\n",
      "batch: 251 , accuracy: 21.69811320754717\n",
      "batch: 252 , accuracy: 23.58490566037736\n",
      "batch: 253 , accuracy: 48.113207547169814\n",
      "batch: 254 , accuracy: 41.509433962264154\n",
      "batch: 255 , accuracy: 0.0\n",
      "batch: 256 , accuracy: 12.264150943396226\n",
      "batch: 257 , accuracy: 14.150943396226415\n",
      "batch: 258 , accuracy: 9.433962264150944\n",
      "batch: 259 , accuracy: 3.7735849056603774\n",
      "batch: 260 , accuracy: 1.8867924528301887\n",
      "batch: 261 , accuracy: 5.660377358490567\n",
      "batch: 262 , accuracy: 3.7735849056603774\n",
      "batch: 263 , accuracy: 3.7735849056603774\n",
      "batch: 264 , accuracy: 4.716981132075472\n",
      "batch: 265 , accuracy: 5.660377358490567\n",
      "batch: 266 , accuracy: 5.660377358490567\n",
      "batch: 267 , accuracy: 6.60377358490566\n",
      "batch: 268 , accuracy: 9.433962264150944\n",
      "batch: 269 , accuracy: 8.49056603773585\n",
      "batch: 270 , accuracy: 14.150943396226415\n",
      "batch: 271 , accuracy: 10.377358490566039\n",
      "batch: 272 , accuracy: 3.7735849056603774\n",
      "batch: 273 , accuracy: 12.264150943396226\n",
      "batch: 274 , accuracy: 16.9811320754717\n",
      "batch: 275 , accuracy: 15.09433962264151\n",
      "batch: 276 , accuracy: 11.320754716981133\n",
      "batch: 277 , accuracy: 11.320754716981133\n",
      "batch: 278 , accuracy: 7.547169811320755\n",
      "batch: 279 , accuracy: 8.49056603773585\n",
      "batch: 280 , accuracy: 42.45283018867924\n",
      "batch: 281 , accuracy: 43.39622641509434\n",
      "batch: 282 , accuracy: 0.0\n",
      "batch: 283 , accuracy: 14.150943396226415\n",
      "batch: 284 , accuracy: 16.9811320754717\n",
      "batch: 285 , accuracy: 16.9811320754717\n",
      "batch: 286 , accuracy: 5.660377358490567\n",
      "batch: 287 , accuracy: 0.9433962264150944\n",
      "batch: 288 , accuracy: 1.8867924528301887\n",
      "batch: 289 , accuracy: 1.8867924528301887\n",
      "batch: 290 , accuracy: 0.0\n",
      "batch: 291 , accuracy: 4.716981132075472\n",
      "batch: 292 , accuracy: 1.8867924528301887\n",
      "batch: 293 , accuracy: 0.0\n",
      "batch: 294 , accuracy: 0.0\n",
      "batch: 295 , accuracy: 0.0\n",
      "batch: 296 , accuracy: 6.60377358490566\n",
      "batch: 297 , accuracy: 9.433962264150944\n",
      "batch: 298 , accuracy: 2.8301886792452833\n",
      "batch: 299 , accuracy: 11.320754716981133\n",
      "batch: 300 , accuracy: 4.716981132075472\n",
      "batch: 301 , accuracy: 0.9433962264150944\n",
      "batch: 302 , accuracy: 0.9433962264150944\n",
      "batch: 303 , accuracy: 1.8867924528301887\n",
      "batch: 304 , accuracy: 28.30188679245283\n",
      "batch: 305 , accuracy: 44.339622641509436\n",
      "batch: 306 , accuracy: 11.320754716981133\n",
      "batch: 307 , accuracy: 10.377358490566039\n",
      "batch: 308 , accuracy: 23.58490566037736\n",
      "batch: 309 , accuracy: 18.867924528301888\n",
      "batch: 310 , accuracy: 9.433962264150944\n",
      "batch: 311 , accuracy: 8.49056603773585\n",
      "batch: 312 , accuracy: 8.49056603773585\n",
      "batch: 313 , accuracy: 4.716981132075472\n",
      "batch: 314 , accuracy: 4.716981132075472\n",
      "batch: 315 , accuracy: 2.8301886792452833\n",
      "batch: 316 , accuracy: 3.7735849056603774\n",
      "batch: 317 , accuracy: 3.7735849056603774\n",
      "batch: 318 , accuracy: 7.547169811320755\n",
      "batch: 319 , accuracy: 12.264150943396226\n",
      "batch: 320 , accuracy: 16.037735849056602\n",
      "batch: 321 , accuracy: 8.49056603773585\n",
      "batch: 322 , accuracy: 16.037735849056602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 323 , accuracy: 4.716981132075472\n",
      "batch: 324 , accuracy: 5.660377358490567\n",
      "batch: 325 , accuracy: 12.264150943396226\n",
      "batch: 326 , accuracy: 45.28301886792453\n",
      "batch: 327 , accuracy: 38.67924528301887\n",
      "batch: 328 , accuracy: 17.92452830188679\n",
      "batch: 329 , accuracy: 26.41509433962264\n",
      "batch: 330 , accuracy: 22.641509433962266\n",
      "batch: 331 , accuracy: 16.9811320754717\n",
      "batch: 332 , accuracy: 8.49056603773585\n",
      "batch: 333 , accuracy: 8.49056603773585\n",
      "batch: 334 , accuracy: 2.8301886792452833\n",
      "batch: 335 , accuracy: 0.0\n",
      "batch: 336 , accuracy: 1.8867924528301887\n",
      "batch: 337 , accuracy: 1.8867924528301887\n",
      "batch: 338 , accuracy: 0.9433962264150944\n",
      "batch: 339 , accuracy: 4.716981132075472\n",
      "batch: 340 , accuracy: 13.20754716981132\n",
      "batch: 341 , accuracy: 18.867924528301888\n",
      "batch: 342 , accuracy: 16.9811320754717\n",
      "batch: 343 , accuracy: 15.09433962264151\n",
      "batch: 344 , accuracy: 8.49056603773585\n",
      "batch: 345 , accuracy: 16.9811320754717\n",
      "batch: 346 , accuracy: 33.0188679245283\n",
      "batch: 347 , accuracy: 52.83018867924528\n",
      "batch: 348 , accuracy: 18.867924528301888\n",
      "batch: 349 , accuracy: 29.245283018867923\n",
      "batch: 350 , accuracy: 16.9811320754717\n",
      "batch: 351 , accuracy: 17.92452830188679\n",
      "batch: 352 , accuracy: 16.9811320754717\n",
      "batch: 353 , accuracy: 27.358490566037734\n",
      "batch: 354 , accuracy: 17.92452830188679\n",
      "batch: 355 , accuracy: 20.754716981132077\n",
      "batch: 356 , accuracy: 21.69811320754717\n",
      "batch: 357 , accuracy: 19.81132075471698\n",
      "batch: 358 , accuracy: 19.81132075471698\n",
      "batch: 359 , accuracy: 22.641509433962266\n",
      "batch: 360 , accuracy: 30.18867924528302\n",
      "batch: 361 , accuracy: 32.075471698113205\n",
      "batch: 362 , accuracy: 27.358490566037734\n",
      "batch: 363 , accuracy: 29.245283018867923\n",
      "batch: 364 , accuracy: 26.41509433962264\n",
      "batch: 365 , accuracy: 23.58490566037736\n",
      "batch: 366 , accuracy: 34.90566037735849\n",
      "batch: 367 , accuracy: 42.45283018867924\n",
      "batch: 368 , accuracy: 19.81132075471698\n",
      "batch: 369 , accuracy: 21.69811320754717\n",
      "batch: 370 , accuracy: 12.264150943396226\n",
      "batch: 371 , accuracy: 17.92452830188679\n",
      "batch: 372 , accuracy: 16.037735849056602\n",
      "batch: 373 , accuracy: 16.037735849056602\n",
      "batch: 374 , accuracy: 16.037735849056602\n",
      "batch: 375 , accuracy: 19.81132075471698\n",
      "batch: 376 , accuracy: 20.754716981132077\n",
      "batch: 377 , accuracy: 19.81132075471698\n",
      "batch: 378 , accuracy: 17.92452830188679\n",
      "batch: 379 , accuracy: 18.867924528301888\n",
      "batch: 380 , accuracy: 27.358490566037734\n",
      "batch: 381 , accuracy: 26.41509433962264\n",
      "batch: 382 , accuracy: 20.754716981132077\n",
      "batch: 383 , accuracy: 19.81132075471698\n",
      "batch: 384 , accuracy: 17.92452830188679\n",
      "batch: 385 , accuracy: 30.18867924528302\n",
      "batch: 386 , accuracy: 26.41509433962264\n",
      "batch: 387 , accuracy: 20.754716981132077\n",
      "batch: 388 , accuracy: 21.69811320754717\n",
      "batch: 389 , accuracy: 9.433962264150944\n",
      "batch: 390 , accuracy: 8.49056603773585\n",
      "batch: 391 , accuracy: 10.377358490566039\n",
      "batch: 392 , accuracy: 8.49056603773585\n",
      "batch: 393 , accuracy: 7.547169811320755\n",
      "batch: 394 , accuracy: 11.320754716981133\n",
      "batch: 395 , accuracy: 13.20754716981132\n",
      "batch: 396 , accuracy: 16.037735849056602\n",
      "batch: 397 , accuracy: 15.09433962264151\n",
      "batch: 398 , accuracy: 21.69811320754717\n",
      "batch: 399 , accuracy: 17.92452830188679\n",
      "batch: 400 , accuracy: 10.377358490566039\n",
      "batch: 401 , accuracy: 9.433962264150944\n",
      "batch: 402 , accuracy: 13.20754716981132\n",
      "batch: 403 , accuracy: 27.358490566037734\n",
      "batch: 404 , accuracy: 31.132075471698112\n",
      "batch: 405 , accuracy: 28.30188679245283\n",
      "batch: 406 , accuracy: 27.358490566037734\n",
      "batch: 407 , accuracy: 32.075471698113205\n",
      "batch: 408 , accuracy: 30.18867924528302\n",
      "batch: 409 , accuracy: 33.9622641509434\n",
      "batch: 410 , accuracy: 37.735849056603776\n",
      "batch: 411 , accuracy: 34.90566037735849\n",
      "batch: 412 , accuracy: 40.56603773584906\n",
      "batch: 413 , accuracy: 31.132075471698112\n",
      "batch: 414 , accuracy: 33.9622641509434\n",
      "batch: 415 , accuracy: 37.735849056603776\n",
      "batch: 416 , accuracy: 44.339622641509436\n",
      "batch: 417 , accuracy: 34.90566037735849\n",
      "batch: 418 , accuracy: 33.9622641509434\n",
      "batch: 419 , accuracy: 32.075471698113205\n",
      "batch: 420 , accuracy: 31.132075471698112\n",
      "batch: 421 , accuracy: 26.41509433962264\n",
      "batch: 422 , accuracy: 31.132075471698112\n",
      "batch: 423 , accuracy: 39.62264150943396\n",
      "batch: 424 , accuracy: 38.67924528301887\n",
      "batch: 425 , accuracy: 33.9622641509434\n",
      "batch: 426 , accuracy: 37.735849056603776\n",
      "batch: 427 , accuracy: 34.90566037735849\n",
      "batch: 428 , accuracy: 34.90566037735849\n",
      "batch: 429 , accuracy: 39.62264150943396\n",
      "batch: 430 , accuracy: 39.62264150943396\n",
      "batch: 431 , accuracy: 41.509433962264154\n",
      "batch: 432 , accuracy: 43.39622641509434\n",
      "batch: 433 , accuracy: 45.28301886792453\n",
      "batch: 434 , accuracy: 40.56603773584906\n",
      "batch: 435 , accuracy: 36.79245283018868\n",
      "batch: 436 , accuracy: 36.79245283018868\n",
      "batch: 437 , accuracy: 33.0188679245283\n",
      "batch: 438 , accuracy: 37.735849056603776\n",
      "batch: 439 , accuracy: 40.56603773584906\n",
      "batch: 440 , accuracy: 29.245283018867923\n",
      "batch: 441 , accuracy: 27.358490566037734\n",
      "batch: 442 , accuracy: 27.358490566037734\n",
      "batch: 443 , accuracy: 31.132075471698112\n",
      "batch: 444 , accuracy: 33.9622641509434\n",
      "batch: 445 , accuracy: 37.735849056603776\n",
      "batch: 446 , accuracy: 35.84905660377358\n",
      "batch: 447 , accuracy: 36.79245283018868\n",
      "batch: 448 , accuracy: 33.9622641509434\n",
      "batch: 449 , accuracy: 35.84905660377358\n",
      "batch: 450 , accuracy: 35.84905660377358\n",
      "batch: 451 , accuracy: 36.79245283018868\n",
      "batch: 452 , accuracy: 24.528301886792452\n",
      "15.508767545503767\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # sess.run(tf.initialize_all_variables())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # plt.ion()\n",
    "    # plt.figure()\n",
    "    # plt.show()\n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch_idx in range(num_epochs): # 100\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "        for batch_idx in range(num_batches): # 666\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "            # all 5 rows, certain numbers of column which the total = 15 columns\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "            \n",
    "            # calculate loss, gradient step, \n",
    "            _total_loss, _train_step, _current_state, _predictions_series, _labels_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series, labels_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder:batchX,\n",
    "                    batchY_placeholder:batchY,\n",
    "                    # it was part of computational graph, where the init_state is the placeholder\n",
    "                    init_state:_current_state\n",
    "                })\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "            # print(\"prediction\", _predictions_series)\n",
    "            # print(\"label\",_labels_series)\n",
    "           \n",
    "            if (batch_idx%(num_batches//10) == 0):\n",
    "                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "                # plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "    # accuracy\n",
    "    _current_state = np.zeros((batch_size, state_size))\n",
    "    acc = 0\n",
    "    total_acc = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * truncated_backprop_length\n",
    "        end_idx = start_idx + truncated_backprop_length\n",
    "        \n",
    "        batchX = x[:,start_idx:end_idx]\n",
    "        batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "        # calculate loss, gradient step, \n",
    "        _predictions_series, _labels_series = sess.run([predictions_series, labels_series],\n",
    "            feed_dict={\n",
    "                batchX_placeholder:batchX,\n",
    "                batchY_placeholder:batchY,\n",
    "                # it was part of computational graph, where the init_state is the placeholder\n",
    "                init_state:_current_state\n",
    "            })\n",
    "        acc = accuracy_list( _predictions_series, _labels_series)\n",
    "        print(\"batch:\",batch_idx,\", accuracy:\",acc)\n",
    "        total_acc += acc\n",
    "    \n",
    "    print(total_acc/num_batches)\n",
    "# plt.ioff()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13241306, 0.02757386, 0.04472994, 0.15169942, 0.01510084,\n",
       "        0.08247881, 0.00120155, 0.02419116, 0.00051674, 0.00379654,\n",
       "        0.01344477, 0.07153476, 0.00606989, 0.01386202, 0.18017827,\n",
       "        0.03059891, 0.03409372, 0.16651577],\n",
       "       [0.13241306, 0.02757386, 0.04472994, 0.15169942, 0.01510084,\n",
       "        0.08247881, 0.00120155, 0.02419116, 0.00051674, 0.00379654,\n",
       "        0.01344477, 0.07153476, 0.00606989, 0.01386202, 0.18017827,\n",
       "        0.03059891, 0.03409372, 0.16651577]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_predictions_series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_labels_series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 14], dtype=int64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(_predictions_series[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(_labels_series[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 2, 18)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(_predictions_series).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.asarray(_labels_series),2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 2)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.asarray(_predictions_series),2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 15],\n",
       "       [ 0,  2],\n",
       "       [16, 14],\n",
       "       [16, 14],\n",
       "       [ 0, 15],\n",
       "       [ 0,  2],\n",
       "       [16, 14],\n",
       "       [16, 14],\n",
       "       [ 0, 15],\n",
       "       [ 0,  2],\n",
       "       [ 2, 14],\n",
       "       [ 2, 14],\n",
       "       [14, 14],\n",
       "       [14, 15],\n",
       "       [16,  2],\n",
       "       [16, 14],\n",
       "       [ 0, 14],\n",
       "       [ 0, 15],\n",
       "       [ 2,  2],\n",
       "       [ 2, 14],\n",
       "       [14, 14],\n",
       "       [14, 15],\n",
       "       [16,  2],\n",
       "       [16, 14],\n",
       "       [ 0, 14],\n",
       "       [ 0, 15],\n",
       "       [ 2, 15],\n",
       "       [ 2,  2],\n",
       "       [14, 15],\n",
       "       [16, 15],\n",
       "       [16, 15],\n",
       "       [ 0,  2],\n",
       "       [ 0, 14],\n",
       "       [ 0, 14],\n",
       "       [16, 15],\n",
       "       [16,  2],\n",
       "       [ 0, 14],\n",
       "       [ 0, 15],\n",
       "       [16, 15],\n",
       "       [16,  2],\n",
       "       [ 0,  3],\n",
       "       [ 0, 14],\n",
       "       [ 2, 14],\n",
       "       [ 2, 15],\n",
       "       [14,  2],\n",
       "       [14, 15],\n",
       "       [16, 15],\n",
       "       [16,  2],\n",
       "       [ 0,  3],\n",
       "       [ 0, 15],\n",
       "       [ 2, 15],\n",
       "       [ 2, 15],\n",
       "       [14,  2]], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.asarray(_labels_series),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14],\n",
       "       [14, 14]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.asarray(_predictions_series),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.528301886792452"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*np.mean(np.argmax(np.asarray(_predictions_series),2) == np.argmax(np.asarray(_labels_series),2))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
